\section{Deterministic eigenvalue processes for matrix-valued diffusions}

Now that we have shown the construction of a matrix-valued process whose eigenvalue is the deterministic Dyson Brownian motion, we generalize the result to get processes with a deterministic spectrum that can follow the dynamics of any eigenvalue process with the form \eqref{eq:gen_dyson}.

% \begin{theorem}

%     Let $Z$ be a process with covariation $\d Z_{ij}\d Z_{kl} = (\delta_{ik}\delta_{jl} + \delta_{il}\delta_{jk} - 2\delta_{ij}\delta_{kl}\delta_{ik} )\d t$ and no finite variation part, which means $Z$ is a symmetric matrix with independent Brownian motions in its entries, except for the diagonal, where $Z_{ii} = 0$. Let $X$ be a matrix valued process such that  $X = \trans H \Lambda H$ and it satisfies the stochastic differential equation
    
    
%     \begin{equation}
%         \trans{H}\d X(t)H = g(X(t)) \d Z(t) h(X(t)) + h(X(t)) \d \trans{Z(t)} g(X(t)) + b(X(t))\d t, \label{eq:deterministic_matrix_diffusion}
%     \end{equation}

%     Then the eigenvalue process $\Lambda$ satisfies

%     \begin{equation}
%         \d \lambda_i = \biggl( b(\lambda_i) + \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t.
%     \end{equation}

% \end{theorem}

\begin{theorem} \label{thm:deterministic_diffusion}
    Let $B = (B(t), t\ge 0)$ be a Brownian motion in $\M_{n,n}(\R)$ and $Y(t) = QM\trans{Q}$ be a symmetric $n\times n$ matrix-valued stochastic process satisfying the stochastic differential equation

    \begin{equation}
        \d Y(t) = g(Y(t)) \d B(t) h(Y(t)) + h(Y(t)) \d \trans{B(t)} g(Y(t)) + b(Y(t))\d t, \label{eq:matrix_diffusion}
    \end{equation}

    where $g,h,b$ are real functions acting spectrally, and $Y(0)$ is a symmetric $n\times n$ matrix with $n$ different eigenvalues.

    Let $G(x,y) = g^2(x)h^2(y) + g^2(y)h^2(x)$, $\tau$ be defined as in \eqref{eq:collision_time}, and take a process $X = (X(t), t\ge 0)$ with diagonalization $X = H \Lambda \trans{H}$ satisfying:
    \begin{itemize}
        \item $(\trans H (\d X) H)_{ij} = (\trans Q ( \d Y) Q)_{ij}$ for $i\neq j$.
        \item $(\trans H (\d X) H)_{ii}=0$ for every $i \in [n]$.
    \end{itemize}
    
    Then, for $t < \tau$ the eigenvalue process $\Lambda(t)$ verifies the following system of stochastic differential equations:

    \begin{equation}
        \d \lambda_i = \biggl( b(\lambda_i) + \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t. \label{eq:deterministic_diffusion}
    \end{equation}
\end{theorem}


\begin{proof}
    We define again $L$ to be the stochastic logarithm of $H$, $\d L \coloneqq \trans H \circ \d H$ and by the same techniques as in the proof of Theorem \ref{thm:diffusion_real} we have that,

    \begin{equation*}
        \d \Lambda = \trans H \circ(\partial X) \circ H - (\d L)\circ \Lambda + \Lambda \circ \d L.
    \end{equation*}

    Define $\d N \coloneqq \trans H \circ(\partial X) \circ H$. Using that $\Lambda \circ \partial L -  (\partial L)\circ \Lambda $ has zero diagonal, we get that $\d \Lambda_i = \d N_{ii}$. The martingale part of $\d N_{ii}$ is the martingale part of $(\trans H (\d X) H)_{ii}$, and by hypothesis, this is zero.

    From the above equation, for $i\neq j$ we have that 

    \begin{align*}
        0 &= \d N_{ij} - (\d L \circ \Lambda)_{ij} + (\Lambda \circ \d L)_{ij} = \d N_{ij} - \d L_{ij} (\lambda_j - \lambda_i).
    \end{align*}
    
    This implies that $\d L_{ij} = \d N_{ij}/(\lambda_j - \lambda_i)$. Now we compute the finite variation part $\d F$ of $\d N$,

    \begin{align*}
        \d F &= \trans H b(X) H \d t + \frac12 (\d \trans H \d X H + \trans H \d X H), \\ 
        &= b(\Lambda) \d t + \frac12( \trans{(\d N\d L)} + \d N \d L).
    \end{align*}

    For $\d N \d A$ we find

    \begin{align*}
        (\d N \d L)_{ij} &= \sum_{k\neq j} \d N_{ik}\d L_{kj} = \sum_{k\neq j} \frac{\d N_{ik}\d N_{kj}}{\lambda_j - \lambda_k}.
    \end{align*}

    Now we use that the martingale part of  $\d N$ has the same entries as $\trans{Q} (\d Y) Q$ and by the results in Theorem \ref{thm:diffusion_real} we know that 

    \begin{equation*}
        (\trans{Q} M Q)_{ik}(\trans{Q} M Q)_{kj} = \delta_{ij}G(\lambda_i,\lambda_k)\d t,
    \end{equation*}

    \noindent so substituting the last equality we get

    \begin{equation*}
        \d \lambda_i = \d F_{ii} = b(\lambda_i)\d t + \sum_{k\neq j} \frac{G(\lambda_i,\lambda_k)}{\lambda_j - \lambda_k}\d t,
    \end{equation*}

    \noindent which is the desired result.
\end{proof}


These results can be particularized for any matrix-valued diffusions. Especially, we are interested in the Wishart and Jacobi processes. We state the result in these two cases as a corollary to the last theorem.

\subsection{Wishart process}

\begin{corollary}
    Let $Y = (Y(t), t \ge 0)$ be an $n\times n$ Wishart process with parameter $m$ and diagonalization $Y = QM\trans{Q}$. Let $X$ be an $n\times n$ self-adjoint matrix process with diagonalization $X = H \Lambda \trans{H}$, such that the off-diagonal part of $\trans{H} \d X H $ and $\trans{Q} \d Y Q$ coincide, and $(\trans{H} \d X H)_{ii} = 0$ for every $i \in [n]$. Then the eigenvalues of $X$, $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$ satisfy the following system of stochastic differential equations

    \begin{equation} \label{eq:deterministic_wishart}
        \d \lambda_i = \left( m +\sum_{k\neq i} \frac{ \abs{\lambda_i} + \abs{\lambda_k} }{\lambda_i - \lambda_k}\right)\d t.
    \end{equation}
\end{corollary}

\begin{proof}
    Apply Theorem \ref{thm:deterministic_diffusion} with $b(x)\equiv m, h(x)\equiv 1, g(x)=\sqrt{x} $. 
\end{proof}


\subsection{Jacobi process}

\begin{corollary}
    Let $Y = (Y(t), t \ge 0)$ be an $n\times n$ matrix Jacobi process with parameters $n_1,n_2$ and diagonalization $Y = QM\trans{Q}$. Let $X$ be an $n\times n$ self-adjoint matrix process with diagonalization $X = H \Lambda \trans{H}$, such that the off-diagonal part of $\trans{H} \d X H $ and $\trans{Q} \d Y Q$ coincide, and $(\trans{H} \d X H)_{ii} = 0$ for every $i \in [n]$. Then the eigenvalues of $X$, $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$ satisfy the following system of stochastic differential equations

    \begin{equation} \label{eq:deterministic_jacobi}
        \d \lambda_i = \left( n_2 - (n_1 + n_2)\lambda_i + \sum_{k\neq i} \frac{\lambda_k(1-\lambda_i) + \lambda_i(1-\lambda_k)}{\lambda_i - \lambda_k} \right)\d t.
    \end{equation}
\end{corollary}

\begin{proof}
    Apply Theorem \ref{thm:deterministic_diffusion} with $b(x) = n_2 - (n_1+n_2)x, h(x) = \sqrt{\abs{1-x}}, g(x)=\sqrt{\abs{x}}$. 
\end{proof}

The systems of eigenvalues for the processes in this chapter follow a deterministic behavior. In the next section, we study some properties derived from the dynamics for the deterministic versions of the Dyson Brownian motion and the Wishart process.

\section{Dynamical behavior of the deterministic eigenvalue processes}

This section aims to derive some results of the dynamical behavior for the deterministic Dyson Brownian motion and the deterministic Wishart process. In particular, we are interested in studying the possibility of collision. Since the Jacobi process is stationary, the behavior is a bit different. We do not include results about it here.

An easy result for a system of functions satisfying \eqref{eq:deterministic_diffusion}, is that the distance between the biggest and the smallest function always grows.


\begin{proposition} \label{proposition:total_distance_grows}
     Let $\lambda_1, \dots, \lambda_n$ be a system of $n$ functions satisfying \eqref{eq:deterministic_diffusion} with $b$ monotone increasing and initial condition $\lambda_1(0) < \dots < \lambda_n(0)$. Then the distance between $\lambda_n$ and $\lambda_1$ grows for every time before the first collision $t\le \tau$.

     \begin{equation*}
         \frac{\d}{\d t}\left( \lambda_n(t) - \lambda_1(t) \right) > 0, \qquad t \ge 0.
     \end{equation*}
\end{proposition}

\begin{proof}
    Compute the derivative of the difference using \eqref{eq:deterministic_diffusion}:

    \begin{align*}
        \frac{\d}{\d t}\left( \lambda_n(t) - \lambda_1(t) \right) &= \frac{\d}{\d t} \lambda_n(t) - \frac{\d}{\d t} \lambda_1 (t),\\
        &= b(\lambda_n) + \sum_{k\neq n} \frac{G(\lambda_n,\lambda_k)}{\lambda_n - \lambda_k} - b(\lambda_1) - \sum_{k\neq 1} \frac{G(\lambda_1,\lambda_k)}{\lambda_1 - \lambda_k},\\
        &= b(\lambda_n) - b(\lambda_1) + \sum_{k\neq n} \frac{G(\lambda_n,\lambda_k)}{\lambda_n - \lambda_k} + \sum_{k\neq 1} \frac{G(\lambda_1,\lambda_k)}{\lambda_1 - \lambda_k}.
    \end{align*}

    Due to the monotonicity of $b$, the difference $b(\lambda_n) - b(\lambda_1)$ is non-negative. The two last terms are positive because $\lambda_n-\lambda_k$ and $\lambda_k-\lambda_1$ are positive for every $k\in [n]\setminus \{1,n\}$.
\end{proof}

In particular, in a system of functions following the equations of the deterministic Dyson Brownian motion or the deterministic Wishart process, the distance between the biggest and the smallest functions always grows.

\subsection{Dyson Brownian motion}

%  Next, we prove that in a system of functions that satisfy the system of equations of the deterministic Dyson Brownian motion, the sum of the distances between functions grows as $t$ grows.

% \begin{theorem}
%     Let $\lambda_1,\dots,\lambda_n$ be a system of $n$ functions satisfying \eqref{eq:deterministic_dyson}. Then the sum of the distances 

%     \begin{equation*}
%         \sum_{i,j,i\neq j} \abs{\lambda_i - \lambda_j},
%     \end{equation*}

%     \noindent grows as $t$ grows.
% \end{theorem}

% \begin{proof}
%     Using the monotonicity of the logarithm function, the growth of the sum of distances is equivalent to the growth of the sum of the logarithms. Define the function $H$ as 
    
%     \begin{equation*}
%         H \coloneqq \sum_{i,j, i\neq j} \log \abs{ \lambda_i - \lambda_j }.
%     \end{equation*}

%     The derivative of $H$ with respect to $\lambda_i$ is 

%     \begin{align*}
%         \frac{\partial H}{\partial \lambda_i} = \sum_{j \neq i} \frac{\partial}{\partial \lambda_i} \left[ \log \abs{\lambda_i - \lambda_j } \right] = \sum_{j\neq i} \frac{1}{\lambda_i - \lambda_j} = \partial_t [\lambda_i].
%     \end{align*}

%     With this, the derivative of $H$ with respect to $t$ is 

%     \begin{align*}
%         \partial_t[H] &= \sum_{i=1}^n \left(\frac{\partial}{\partial t}\lambda_i\right)\left(\frac{\partial}{\partial \lambda_i} H\right) = \sum_{i=1}^n \left( \sum_{j \neq i} \frac{1}{\lambda_i - \lambda_j} \right)^2, \\
%         &= \sum_{i=1}^n \sum_{j\neq i} \frac{1}{(\lambda_i - \lambda_j)^2} + 2 \sum_{i=1}^n\sum_{j\neq i, k \neq i} \frac{1}{(\lambda_i - \lambda_k)(\lambda_i-\lambda_j)}.
%     \end{align*}

%     In the second term, we have the sum of all the possible terms of the form 

%     \begin{equation*}
%         \frac{1}{(\lambda_i - \lambda_k)(\lambda_i-\lambda_j)},
%     \end{equation*}

%     \noindent for $i,j,k$ different. Notice that for every set of three different indexes $i,j,k$ we have

%     \begin{align*}
%         \frac1{(\lambda_i-\lambda_j)(\lambda_i-\lambda_k)} &+ \frac1{(\lambda_j-\lambda_i)(\lambda_j-\lambda_k)} + \frac1{(\lambda_k-\lambda_j)(\lambda_k-\lambda_i)} \\
%         &= \frac{\lambda_i - \lambda_j}{\lambda_j - \lambda_k} - \frac{\lambda_i-\lambda_k}{\lambda_j - \lambda_k} + 1 = 0.
%     \end{align*}

%     Then $\partial_t[H] = \sum_{i,j,i\neq j} 1/(\lambda_i - \lambda_j)^2$, and since this derivative is positive, $H$ grows with $t$. This implies that the sum of the distances grows as well.
% \end{proof}

If we have a system of functions $\lambda_1, \lambda_2, \dots, \lambda_n$ that satisfy a system like \eqref{eq:deterministic_dyson} or \eqref{eq:deterministic_wishart}, we will say that the functions $\lambda_i$ and $\lambda_j$ repel each other at time $t_0$ if the derivative $\frac{\d}{\d t} \abs{\lambda_i - \lambda_j}$ exists and is positive at time $t_0$. The next Lemma gives us a necessary and sufficient condition for this to happen in the case of a deterministic Dyson Brownian motion.

\begin{lemma} \label{lemma:separation}
    Let $\lambda_1,\dots,\lambda_n$ be a system of $n$ functions satisfying the system of differential equations \eqref{eq:deterministic_dyson}. Take $\lambda_i(t_0) > \lambda_j(t_0)$ and such that there is no $\lambda_k$ that satisfies $\lambda_i(t_0) > \lambda_k(t_0) > \lambda_j(t_0)$. Then, $\lambda_i$ and $\lambda_k$ repel each other at time $t=t_0$ if and only if 

    \begin{equation} \label{eq:separation_condition}
        \frac{2}{(\lambda_i(t_0) - \lambda_j(t_0))^2} > \sum_{k\neq i,j} \frac{1}{(\lambda_i(t_0) - \lambda_k(t_0))(\lambda_j(t_0) - \lambda_k(t_0))}.
    \end{equation}
\end{lemma}

\begin{proof}

    Since there is no confusion, let us write $\lambda_i(t_0)$ as $\lambda_i$ for every $i$. The distance between two functions grows if and only if its derivative its positive, so 

    \begin{align*}
        \frac{\d }{\d t} (\lambda_i - \lambda_j) &= \sum_{k\neq i} \frac{1}{\lambda_i - \lambda_k} - \sum_{k\neq j} \frac{1}{\lambda_j - \lambda_k} = \sum_{k\neq i,j} \left( \frac{1}{\lambda_i - \lambda_k} - \frac{1}{\lambda_j - \lambda_k} \right) + \frac{2}{\lambda_i - \lambda_j},\\
        &= \sum_{k \neq i, j} \frac{\lambda_j - \lambda_k - \lambda_i + \lambda_k}{(\lambda_i- \lambda_k)(\lambda_j - \lambda_k)} + \frac{2}{\lambda_i - \lambda_j},= \frac{2}{\lambda_i - \lambda_j} - \sum_{k\neq i,j} \frac{\lambda_i - \lambda_j}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)}.
    \end{align*}
    
    So this derivative is positive if and only if
    
    \begin{align*}
        \frac{2}{\lambda_i - \lambda_j} &> \sum_{k\neq i,j} \frac{\lambda_i - \lambda_j}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)}.
    \intertext{Finally, we divide both sides of the equality by $\lambda_i - \lambda_j$ to find the equivalent condition:}
        \frac{2}{(\lambda_I - \lambda_j)^2} &> \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)}.
    \end{align*}
\end{proof}



\begin{theorem} \label{thm:hermite_minimal_grows}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of functions moving according to \eqref{eq:deterministic_dyson} and let $\lambda_i,\lambda_j$ be such that for a given $t_0$ it is satisfied $\lambda_i(t_0) > \lambda_j(t_0)$ and 
    
    \begin{equation*}
        \lambda_i(t_0) - \lambda_j(t_0) = \min_{k,l \in [n]} \abs{ \lambda_k(t_0) - \lambda_l(t_0)}.
    \end{equation*}

    Then, $\lambda_i$ and $\lambda_k$ repel. 
\end{theorem}

\begin{proof}
    Define the quotients $c_{ik}, c_{jk}$ as 

    \begin{equation*}
        c_{ik} = \frac{\lambda_i(t_0) - \lambda_k(t_0)}{\lambda_i(t_0) - \lambda_j(t_0)}, \qquad c_{jk} = \frac{\lambda_j(t_0) - \lambda_k(t_0)}{\lambda_i(t_0) - \lambda_j(t_0)}.
    \end{equation*}

    Notice that the condition that the distance between $\lambda_i$ and $\lambda_j$ at time $t_0$ is minimal means that there is no $\lambda_k$ between them. Therefore, for a fixed $k_0$, $c_{ik_0}$ and $c_{jk_0}$ have the same sign and $c_{ik_0}c_{jk_0}>0$. 
    
    Again, let us write $\lambda_i(t_0)$ as $\lambda_i$ for every $i$. We can write the right-hand side of \eqref{eq:separation_condition} as

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)} &= \sum_{k\neq i,j} \frac{1}{ c_{ik}(\lambda_i - \lambda_j) c_{jk}(\lambda_i - \lambda_j)} = \sum_{k\neq i,j} \frac{ (c_{ik}c_{jk})^{-1} }{(\lambda_i-\lambda_j)^2}.
    \end{align*}

    With this, condition \eqref{eq:separation_condition} can be written as

    \begin{align}
        \frac{2}{(\lambda_i - \lambda_j)^2} &> \sum_{k\neq i,j} \frac{ (c_{ik}c_{jk})^{-1} }{(\lambda_i-\lambda_j)^2},\notag\\
        \intertext{this condition is equivalent to}
        2 &> \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}}. \label{eq:simplified_condition}
    \end{align}

    We know that the distance between $\lambda_i$ and $\lambda_j$ is minimal, in the worst-case scenario, all of the other distances are the same as $\lambda_i - \lambda_j$, so $\abs{c_{ik}},\abs{c_{ij}}\ge1$. Notice that if $\lambda_{i-1}$ is the function located immediately below $\lambda_i$, this would mean that in the worst case scenario $c_{i,i-1}=-1$ and $c_{j,i-1} = -2$. Similarly, for $\lambda_{i-2}$ we would have $c_{i,i-1} = -2, c_{j,i-2} =-3$. In general, $c_{i,i-l} = -l, c_{j,i-l} = -(l+1)$. Analogously, if $\lambda_{j+l}$ is the function located $l$ positions below $j$, then $c_{j,j+l} = l, c_{i,j+l} = l+1$.

    Now, for the arrangement of the functions, we have two extreme cases. If $\lambda_i$ and $\lambda_j$ are the functions in one of the extremes (i.e. the two biggest or two smallest ones), this would mean that the right-hand side of \eqref{eq:simplified_condition} can be written as

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} &= \sum_{k=1}^{n-2} \frac{1}{k(k+1)} = \sum_{k=1}^{n-2} \frac{k+1 - k}{k(k+1)} = \sum_{k=1}^{n-2} \left( \frac{1}{k} - \frac{1}{k+1} \right),\\
        &= 1 - \frac{1}{n-1} = \frac{n-2}{n-1} < 2.
    \end{align*}

    So in this case, applying Lemma \ref{lemma:separation}, $\lambda_i,\lambda_j$ would repel each other. The other extreme case is when exactly half of the functions are located at each side of $\lambda_i$ and $\lambda_j$. Let us suppose first that $n$ is even, in this case we would have,

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} &= \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{c_{i,i-k}c_{j,i-k}} + \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{c_{i,j+k}c_{j,j+k}} = \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{k(k+1)} + \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{k(k+1)}, \\
        &= 2 \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{k(k+1)} = 2\left(1 - \frac{2}{n}\right) = 2 - \frac4n < 2.
    \end{align*}

    There is also repulsion in this case. Finally, if $n$ is odd, $n-1$ is even and 

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} &= \sum_{k=1}^{\frac{n+1-2}{2}} \frac{1}{c_{i,i-k}c_{j,i-k}} + \sum_{k=1}^{\frac{n+1-2}{2}} \frac{1}{c_{i,j+k}c_{j,j+k}} + \frac{1}{\left(\frac{n+1}{2}\right)\left(\frac{n+3}{2}\right)},\\
        &= 2\left( 1 - \frac{2}{n+1} \right) + \frac{4}{(n+1)(n+3)} < 2.
    \end{align*}
    
    % we consider the previous sum for $\lfloor n/2 \rfloor$ and sum the term corresponding to the remaining function, i.e. the term 

    % \begin{equation*}
    %     \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} = 2 - \frac{4}{n} + \frac1{(\lfloor n/2\rfloor+1)(\lfloor n/2\rfloor +2)} < 2.
    % \end{equation*}

    So, even in the worst-case scenario, criterion \eqref{eq:separation_condition} is satisfied, and we conclude that the minimal distance grows for every initial condition of the system.
\end{proof}


If at some time $t_0$, it happens that $\lambda_i(t_0) = \lambda_j(t_0)$, we say that the functions $\lambda_i$ and $\lambda_j$ collide at time $t_0$. An easy consequence of Theorem \ref{thm:hermite_minimal_grows} is that there are no collisions for a deterministic Dyson Brownian motion.

\begin{corollary}
    If a system of functions satisfies \eqref{eq:deterministic_dyson} with initial condition $\lambda_1(0) < \lambda_2(0) <\dots <\lambda_n(0)$, then there are no collisions.
\end{corollary}

\begin{proof}
    Let $\delta$ be the minimal distance between two functions at time $0$, i.e.,

    \begin{equation*}
        \delta = \min_{k,l\in [n]} \abs{ \lambda_k(0) - \lambda_l(0)}.
    \end{equation*}
    By Theorem \ref{thm:hermite_minimal_grows}, the distance between any two functions is bigger than $\delta$ for every $t >0$.
\end{proof}


Although the minimal distance between two functions always grows in a system that satisfies \eqref{eq:deterministic_dyson}, this is not the case for all the distances in every configuration. This result is stated in the following remark.


\begin{remark} \label{remark:hermite_not_all_repel}
    In a system that satisfies~\eqref{eq:deterministic_dyson}, for $n\ge3$, not all functions repel for every given initial condition. For $n =2$, the functions always repel each other.
\end{remark}

\begin{proof}
    We prove the first part by providing a counterexample. Suppose that $\lambda_i,\lambda_j$ are the two biggest or two smallest functions at time $t_0$, and their separation is 1, while all the other separations are $0.1$. The same computations that the proof of Theorem \ref{thm:hermite_minimal_grows}, but with a re-scaling of the $c_{ik}c_{jk}$ lead to

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} = \frac{1}{0.01}\left( 1 - \frac1{n-1}\right) = 100 - \frac{100}{n-1}.
    \end{align*} Clearly, for $n\ge 3$, condition~\eqref{eq:separation_condition} is not satisfied. The second part is a direct consequence of Theorem \ref{thm:hermite_minimal_grows}.
\end{proof}

\subsection{Wishart process}

\begin{lemma} \label{lemma:separating_condition_wishart}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of $n$ functions satisfying \eqref{eq:deterministic_wishart}. Let $\lambda_i, \lambda_j$ be such that $\lambda_i(t_0) > \lambda_j(t_0)$ and there is no $\lambda_k$ such that $\lambda_i(t_0) > \lambda_k(t_0) > \lambda_j(t_0)$. Then $\lambda_i$ and $\lambda_j$ repel each other at time $t_0$ if and only if 

    \begin{equation} \label{eq:separation_condition_wishart}
        \frac{\lambda_i(t_0) + \lambda_j(t_0)}{(\lambda_i(t_0) - \lambda_j(t_0))^2} < \sum_{k\neq i,j} \frac{\lambda_k(t_0)}{(\lambda_i(t_0)-\lambda_k(t_0))(\lambda_j(t_0)-\lambda_k(t_0))}.
    \end{equation}
\end{lemma}

\begin{proof}

    Take the derivative of the separation and use linearity with \eqref{eq:deterministic_wishart} to get

    \begin{align*}
        \frac{\d}{\d t}(\lambda_i - \lambda_j) &= \sum_{k\neq i} \frac{\lambda_i + \lambda_k}{\lambda_i - \lambda_k} - \sum_{k\neq j} \frac{\lambda_j + \lambda_k}{\lambda_j - \lambda_k}, \\
        &= \sum_{k \neq i,j} \frac{ (\lambda_i + \lambda_k)(\lambda_j - \lambda_k) - (\lambda_j + \lambda_k)(\lambda_i - \lambda_k) }{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)} + 2\frac{\lambda_i + \lambda_j}{\lambda_i - \lambda_j},\\
        &= \sum_{k\neq i,j} \frac{2\lambda_k(\lambda_j - \lambda_i)}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)} + 2 \frac{\lambda_i + \lambda_j}{\lambda_i - \lambda_j}.
    \end{align*}

    Thus, the derivative is positive if and only if 

    \begin{align*}
        \sum_{k\neq i,j} \frac{2\lambda_k(\lambda_j - \lambda_i)}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)} > 2 \frac{\lambda_i + \lambda_j}{\lambda_j - \lambda_i}.\\
        \intertext{Dividing both sides by $2(\lambda_j - \lambda_i)$ we get}
        \sum_{k\neq i,j} \frac{\lambda_k}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)} >  \frac{\lambda_i + \lambda_j}{(\lambda_j - \lambda_i)^2}.
    \end{align*}
\end{proof}

\begin{lemma}
    Let $(\lambda_1,\lambda_2, \dots, \lambda_n)$ be a system of $n$ functions satisfying \eqref{eq:deterministic_wishart} with initial condition $0<\lambda_1(0) < \lambda_2(0) < \cdots \lambda_n(0)$. Define $\tau$ as the first collision time:

    \[ \tau = \inf\{ t \ge 0 \colon \lambda_i(t) = \lambda_j(t) \textrm{ for some } i \neq j \}. \]
    Then, if $m> n-1$, the function $\lambda_1(t)$ remains greater than zero for every $0 \le t < \tau$.
\end{lemma}

\begin{proof}

Define the function $f(t)$ as

    \begin{equation*}
        f(t) \coloneqq \sup_{k\ge 2}\left\{ \frac{\lambda_k(t) + \lambda_1(t)}{\lambda_k(t) - \lambda_1(t)} \right\}.
    \end{equation*}
    For every $2 \le k\le n$, the quotient $(\lambda_k(t) + \lambda_1(t))/( \lambda_k(t) - \lambda_1(t) )$ converges to 1 as $\lambda_1(t)$ converges to 0. There are only finitely many eigenvalues, so they converge uniformly and $f(t)$ also converges to 1 as $\lambda_1(t)$ converges to 0.

    The derivative of $\lambda_1$ is 
    
    \begin{align*}
        \frac{\d \lambda_1}{\d t} &= m + \sum_{2 \le k\le n} \frac{\lambda_1 + \lambda_k}{\lambda_1 - \lambda_k} = m - \sum_{2\le k \le n} \frac{\lambda_1+\lambda_k}{\lambda_k - \lambda_1} \ge m - (n-1)f(t).
    \end{align*}
    Suppose that $\lambda_1(t)$ reaches 0 at time $t_1$. Since $f(t)$ is continuous before the first collision time and converges to 1 as $\lambda_1(t) \to 0$, we use that $m > n-1$, and conclude that exists a $t_0$ such that, for $t_0 \le t\le t_1$, $(n-1)f(t)$ is smaller than $m$, i.e.

    \begin{equation*}
        \frac{\lambda_1+\lambda_k}{\lambda_k - \lambda_1} \ge m - (n-1)f(t) \ge 0, \qquad t\ge t_0.
    \end{equation*}
    So the derivative of $\lambda_1(t)$ is positive for every $t_0 \le t\le t_1$, but this contradicts the fact that the function decreases in this interval. The contradiction comes form supposing that $\lambda_1(t)$ reaches zero.
\end{proof}

\begin{corollary}
    In a system of $n$ functions satisfying \eqref{eq:deterministic_wishart} there is no collision of the functions in finite time.
\end{corollary}

\begin{proof}
    Proposition \ref{proposition:total_distance_grows} guarantees that the $n$ functions do not collide together simultaneously. Suppose that the first collision of the system happens between $k < n$ functions at time $t_1$ i the point $c$. Let $K = \{ n_1, n_2, \dots, n_k\}$ denote the collection of indices of the functions colliding at time $t_1$ with $n_1 < n_2 <\cdots < n_k$. The set $K$ consists of only consecutive numbers because $t_1$ is the first collision time.The derivative of the distance between the $\lambda_{n_k}$ and $\lambda_{n_1}$ is

    \begin{align*}
        \frac{\d }{\d t} \left[ \lambda_{n_k}(t) - \lambda_{n_1}(t) \right] &= m + \sum_{j\neq n_k} \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j} - m - \sum_{j\neq n_1} \frac{\lambda_{n_1} + \lambda_j}{\lambda_{n_1} - \lambda_j},\\
        &= \sum_{j\neq n_k} \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j} - \sum_{j\neq n_1} \frac{\lambda_{n_1} + \lambda_j}{\lambda_{n_1} - \lambda_j},\\
        &= \sum_{j \in K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j } + \frac{\lambda_{n_1} + \lambda_j}{\lambda_j - \lambda_{n_1} } \right\} + \sum_{j \notin K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j } + \frac{\lambda_{n_1} + \lambda_j}{\lambda_j - \lambda_{n_1} } \right\}.
    \end{align*}
    In the last expression, the first sum is positive because for $j\in K'$, the function $\lambda_j(t)$ is greater than $\lambda_{n_1}(t)$ and smaller than $\lambda_{n_k}(t)$ before the first collision. 
    
    Denote by $d(t)$ the distance between $\lambda_{n_k}$ and $\lambda_{n_1}$:

    \[ d(t) = \lambda_{n_k}(t) - \lambda_{n_1}(t). \]
    Since we are supposing the collision at time $t_1$, the limit of $d$ as $t$ tends to $t_1$ is 0. Then for every $\epsilon >0$, there exists a $t_\epsilon<t_1$ such that $d(t) < \epsilon$ for every $t_\epsilon \le t\le t_1$. The distance between any other functions $\lambda_j(t)$ with index $j\in K$ is bounded by $d(t)$ because $d(t)$ is the distance of the extremes. So, using the last equality we find:

    \begin{align*}
        \frac{\d }{\d t} \left[ \lambda_{n_k}(t) - \lambda_{n_1}(t) \right] &\ge \sum_{j \in K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{d(t)} + \frac{\lambda_{n_1} + \lambda_j}{d(t)} \right\} + \sum_{j \notin K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j } + \frac{\lambda_{n_1} + \lambda_j}{\lambda_j - \lambda_{n_1} } \right\},
        \\ &\ge\frac{4\lambda_1(t)(k-1)}{d(t)} + \sum_{j \notin K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{\lambda_{n_k} - \lambda_j } + \frac{\lambda_{n_1} + \lambda_j}{\lambda_j - \lambda_{n_1} } \right\}.
    \end{align*}

    Now, take $b(t)$ to be the minimal distance between a function outside the collision and a function in the collision, i.e.,

    \begin{equation*}
        b(t) \coloneqq \inf_{j\in K, i\notin K} \{ \abs{\lambda_j(t) - \lambda_i(t)} \}.
    \end{equation*}

    Using the continuity and the fact that the functions $\lambda_j$ for $j \notin K$ do not collide until time $t_1$, we have that there exists a $\delta >0$ such that $b(t) > \delta$ for every $t \in [0,t_1]$. By the same continuity argument, all the functions $\lambda_i(t)$ for $i \in [k]$ are bounded until time $t_1$. Take $M>0$ to be a universal bound for all of them. With this, we can conclude

    \begin{align*}
        \frac{\d }{\d t} \left[ \lambda_{n_k}(t) - \lambda_{n_1}(t) \right] &\ge \frac{4\lambda_1(t)(k-1)}{d(t)} - \sum_{j\notin K} \left\{ \frac{\lambda_{n_k} + \lambda_j}{b(t)} + \frac{\lambda_{n_1} + \lambda_j}{b(t)} \right\},\\
        &\ge \frac{4\lambda_1(t)(k-1)}{d(t)} - \frac{4M(n-k)}{\delta}.
    \end{align*}

    Taking an appropriate $\epsilon$, we can make the last quantity bigger than zero for $t$ between $t_\epsilon$ and $t_1$. This means that the derivative of the difference between $\lambda_{n_k}$ and $\lambda_{n_1}$ is positive in $(t_\epsilon, t_1)$ and this contradicts the fact that the distance decreases to zero.
\end{proof}

% \begin{proof}
%    If $n=2$, Proposition \ref{proposition:total_distance_grows} guarantees the result. For $n\ge 3$, take an initial condition with $\lambda_i(0) < \lambda_j(0)$ and there is no $k$ such that $\lambda_i(0) < \lambda_k(0) < \lambda_j(0)$. Now suppose that $\lambda_i$ $\lambda_j$ are the first two functions to collide and they do it at the time of the first collision between $\lambda_i$ and $\lambda_j$  $\tau_{ij} \in \R^+$, defined as 

%    \begin{equation*}
%        \tau_{ij} \coloneqq \inf\left\{ t > 0 \colon \lambda_i(t) \ge \lambda_j(t) \right\}.
%    \end{equation*}

%    The function $\lambda_j(t) - \lambda_i(t)$ is differentiable, so it has finite variation. Because it is positive in $(0, \tau)$ and vanishes at time $\tau$, it must be monotone decreasing $(\tau-\epsilon, \tau)$ for $\epsilon >0$ sufficiently small. Then in this interval, its derivative must be negative. %Choose an arbitrary $\epsilon >0$.% and take any $t_0 \in (\tau-\epsilon, \tau)$.
%    Define the time dependent quotients $c_{ik}(t),c_{jk}(t)$ as 

%     \begin{equation*}
%         c_{ik}(t) = \frac{\lambda_i(t) - \lambda_k(t)}{\lambda_i(t) - \lambda_j(t)}, \qquad c_{jk}(t) = \frac{\lambda_j(t) - \lambda_k(t)}{\lambda_i(t) - \lambda_j(t)}.
%     \end{equation*}
%     At every $t \in (0,\tau)$ the derivative of $\lambda_j - \lambda_i$ is negative if and only if

%     \begin{align*}
%         \sum_{k \neq i,j} \frac{\lambda_k(t)}{c_{ik}(t)c_{jk}(t)} &> \lambda_i(t) + \lambda_j(t). 
%     \end{align*}
%     Take $\lambda_i(\tau) = \lambda_j(\tau) = L$. Due to the continuity of the functions, it follows that every $\lambda_k$ is bounded on $[0,\tau]$ for $k \in [n]$, let $K$ be a common bound for them. Suppose that for every $k \neq i,j$, $\lambda_k(\tau) \neq L$ . Using that $c_{ik}(t)c_{jk}(t) > 0$, this implies that

%     \begin{align*}
%         \lim_{t\to \tau} c_{ik}(t)c_{jk}(t) = \lim_{t\to\tau} \left(\frac{\lambda_i(t) - \lambda_k(t)}{\lambda_i(t) - \lambda_j(t)}\right)\left(\frac{\lambda_j(t) - \lambda_k(t)}{\lambda_i(t) - \lambda_j(t)}\right) = \infty.
%     \end{align*}
%     This means that   

%     \begin{equation*}
%         \lim_{t\to\tau} \sum_{k\neq i,j} \frac{\lambda_k(t)}{c_{ik}(t)c_{jk}(t)} = 0.
%     \end{equation*}
%     But $\lim_{t\to\tau} \lambda_i(t) + \lambda_j(t) = 2L >0$. So, for every $\epsilon$ small enough the condition \eqref{eq:separation_condition_wishart} is satisfied in $(\tau-\epsilon,\tau)$ which contradicts the fact that the derivative is negative in these intervals. We conclude that there is no collision in finite time.
%     \end{proof}

\begin{proposition} \label{proposition:laguerre_does_not_grow}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of $n$ functions moving according to \eqref{eq:deterministic_wishart} and let $\lambda_i,\lambda_j$ be such that for a given $t_0$ it is satisfied $\lambda_i(t_0) > \lambda_j(t_0)$ and 
    
    \begin{equation*}
        \lambda_i(t_0) - \lambda_j(t_0) = \min_{k,l \in [n]} \abs{ \lambda_k(t_0) - \lambda_l(t_0)}.
    \end{equation*}
    Then, $\lambda_i$ and $\lambda_j$ do not necessarily repel. 
\end{proposition}

\begin{proof}
    We will prove the result by providing an initial condition for which the minimal distance will have a negative derivative. Let $\lambda_j(t_0) = \min_{k \le n} \lambda_k(t_0)$ and so $\lambda_i(t_0) = \min_{k\neq j} \lambda_k(t_0)$. Similarly to the proof of Theorem \ref{thm:hermite_minimal_grows}, let us denote $\lambda_i(t_0)$ as $\lambda_i$ along the proof and define the quotients $c_{ik},c_{jk}$ as 

    \begin{equation*}
        c_{ik} = \frac{\lambda_i - \lambda_k}{\lambda_i - \lambda_j}, \qquad c_{jk} = \frac{\lambda_j - \lambda_k}{\lambda_i - \lambda_j}.
    \end{equation*}

    For every fixed $k$, the quotients $c_{ik}$ and $c_{jk}$ have the same sign and given the minimality of $\lambda_i - \lambda_j$ we have that $c_{ik}c_{jk}>1$. Using these quantities, the separation condition \eqref{eq:separation_condition_wishart} is reduced to 

    \begin{equation*}
        \lambda_i + \lambda_j > \sum_{k \neq i,j} \frac{\lambda_k}{c_{ik}c_{jk}}.
    \end{equation*}

    Take $\lambda_i, \lambda_j$ such that $\lambda_i - \lambda_j =1$ at $t=t_0$, and that $\lambda_k > \lambda_i$ for all $k \notin \{i,j\}$. Take the separation between all the $(\lambda_k)_{k\neq i,j}$ to be $1+\epsilon$ for some $\epsilon >0$. Then the coefficients $c_{i,i+1}, c_{j,i+1}$ are equal to

    \[ c_{i,i+1} = \frac{\lambda_i - \lambda_{i+1}}{\lambda_i - \lambda_j} = -\frac{1+\epsilon}{1}, \qquad c_{j,i+1} = \frac{\lambda_j - \lambda_{i+1}}{\lambda_i - \lambda_j} = -\frac{2+\epsilon}{1}. \]
    In general, $\abs{c_{i,i+l}}=l(1+\epsilon), \abs{c_{j,i+l}} = 1 + l(1+\epsilon)$. Furthermore, we have that $\lambda_{i+l}$ can be expressed as

    \begin{equation*}
         \lambda_{i+l} = \lambda_i + l(1+\epsilon) = \lambda_j + 1 + l(1+\epsilon).
    \end{equation*}

    With this, the separating condition for $\lambda_i$ and $\lambda_j$ can be written as

    \begin{equation*}
        2\lambda_j + 1 > \sum_{k=2}^{n} \frac{\lambda_j + 1 + k(1+\epsilon)}{k(1+\epsilon)(1+k(1+\epsilon))}. 
    \end{equation*}
    The left-hand side is fixed for fixed $\lambda_j(t_0)$. For the left-hand side, we have

    \begin{align*}
        \sum_{k=2}^{n} \frac{\lambda_j + 1 + k(1+\epsilon)}{k(1+\epsilon)(1+k(1+\epsilon))} = \sum_{k=2}^n \frac{\lambda_j + 1}{k(1+\epsilon)(1+k(1+\epsilon))} + \sum_{k=2}^n \frac{1}{1+k(1+\epsilon)}.
    \end{align*}

    For the second element in the sum and $\epsilon$ sufficiently small we have 

    \begin{align*}
        \sum_{k=2}^n \frac{1}{1+k(1+\epsilon)} > \sum_{k=2}^n \frac{1}{1+2k} > \frac12\sum_{k=2}^n \frac{1}{1+k}.
    \end{align*}

    The last expression can be made arbitrarily big for $n$ big enough. We conclude that under these conditions, for a system with enough functions, the minimal distance can be made smaller with a specific initial condition.
    
\end{proof}


\section{Path simulations}

With these simulations, we exemplify the dynamical behavior of the system of functions explained in the last section. In Figure \ref{fig:four_det_dyson} we have the analogous to Figure \ref{fig:four_dyson}. The initial conditions are uniformly spaced points around zero for the last three examples. For the first example, the distance between the two points in the middle is two, while the top and bottom points are separated by 0.1 of the two in the middle. We can notice that in concordance with Remark \ref{remark:hermite_not_all_repel}, the bigger value decreases for some time until the particle spacing is more uniform and then it starts growing. For the functions with a small separation, this distance grows very quickly at the beginning, as expected by Theorem \ref{thm:hermite_minimal_grows}.


\begin{figure}[h!] \centering 
    \input{img/four_det_dysons.pgf}
    \caption{Simulation of four different deterministic Dyson Brownian motions with different dimensions. The initial conditions from left to right are $(-1.1,-1,2,2.1)$, $(-1, -0.5, 0, 0.5, 1)$, $(-2.5,-1.67,-0.83,0,0.83,1.67,2.5)$ and $(-2.5,-1.875,-1.25,-0.625,0,0.625,1.25,1.875,2.5)$. \label{fig:four_det_dyson}}
\end{figure}

In the remaining three panels of Figure \ref{fig:four_det_dyson}, we also see a behavior according to \ref{thm:hermite_minimal_grows}. Also notice that \eqref{eq:deterministic_dyson} implies that if we start with a configuration that is evenly spaced, then the points will roughly stay evenly spaced through time, which is precisely what we can see. 

In Figure \ref{fig:det_wishart}, we have the simulation analogous to \ref{fig:wishart_comparison} with a nine-dimensional deterministic Wishart process. Apart from observing that the total distance of the particles grows, we check that this example confirms Proposition \ref{proposition:laguerre_does_not_grow}, as the distance between the two paths in the bottom is minimal at some point and it continues to decrease. However, the biggest is the value of the function and the surrounding ones, the faster their spacing grows, as we can notice for the two functions at the top.

\begin{figure}[h!] \centering 
    \input{img/deterministic_wishart.pgf}
    \caption{Simulation of the path of a deterministic Wishart process. The initial conditions are $(0.1,2.6,5.1,7.6,10.1,12.6,15.1,17.6,20.1)$.\label{fig:det_wishart}}
\end{figure}

For the path simulations of the deterministic Jacobi process, we observe that for $t=0.05$ they are almost evenly spaced in $[0,1]$. We would expect this deterministic process to converge to the equilibrium which would be the roots of the ninth Jacobi polynomial with parameters $n_1 = 1, n_2 =2$. 

\begin{figure}[h!] \centering 
    \input{img/deterministic_jacobi.pgf}
    \caption{Simulation of the path of a deterministic Jacobi process. The initial conditions are $(0.3,0.325,0.35,0.375,0.4,0.425,0.45,0.475,0.5)$.\label{fig:det_jacobi}}
\end{figure}

%\newpage