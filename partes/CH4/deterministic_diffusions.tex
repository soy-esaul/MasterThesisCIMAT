\section{Deterministic eigenvalue processes for matrix-valued diffusions}

Now that we have shown the construction of a matrix-valued process whose eigenvalue is the deterministic Dyson Brownian motion, we generaliza the result to get processes with a deterministic spectrum that can follow the dynamics of any eigenvalue process with the form \eqref{eq:gen_dyson}.

% \begin{theorem}

%     Let $Z$ be a process with covariation $\d Z_{ij}\d Z_{kl} = (\delta_{ik}\delta_{jl} + \delta_{il}\delta_{jk} - 2\delta_{ij}\delta_{kl}\delta_{ik} )\d t$ and no finite variation part, which means $Z$ is a symmetric matrix with independent Brownian motions in its entries, except for the diagonal, where $Z_{ii} = 0$. Let $X$ be a matrix valued process such that  $X = \trans H \Lambda H$ and it satisfies the stochastic differential equation
    
    
%     \begin{equation}
%         \trans{H}\d X(t)H = g(X(t)) \d Z(t) h(X(t)) + h(X(t)) \d \trans{Z(t)} g(X(t)) + b(X(t))\d t, \label{eq:deterministic_matrix_diffusion}
%     \end{equation}

%     Then the eigenvalue process $\Lambda$ satisfies

%     \begin{equation}
%         \d \lambda_i = \biggl( b(\lambda_i) + \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t.
%     \end{equation}

% \end{theorem}

\begin{theorem} \label{thm:deterministic_diffusion}
    Let $B = (B(t), t\ge 0)$ be a Brownian motion in $\M_{p,p}(\R)$ and $Y(t) = QM\trans{Q}$ be a symmetric $p\times p$ matrix-valued stochastic process satisfying the stochastic differential equation

    \begin{equation}
        \d Y(t) = g(Y(t)) \d B(t) h(Y(t)) + h(Y(t)) \d \trans{B(t)} g(Y(t)) + b(Y(t))\d t, \label{eq:matrix_diffusion}
    \end{equation}

    where $g,h,b$ are real functions acting spectrally, and $Y(0)$ is a symmetric $p\times p$ matrix with $p$ different eigenvalues.

    Let $G(x,y) = g^2(x)h^2(y) + g^2(y)h^2(x)$, $\tau$ be defined as in \eqref{eq:collision_time}, and take a process $X = (X(t), t\ge 0)$ with diagonalization $X = H \Lambda \trans{H}$ such that $\trans H (\d \Lambda) H$ has the same off-diagonal entries as $\trans Q ( \d M) \circ Q$ and has diagonal entries equal to zero.
    
    Then, for $t < \tau$ the eigenvalue process $\Lambda(t)$ verifies the following system of stochastic differential equations:

    \begin{equation}
        \d \lambda_i = \biggl( b(\lambda_i) + \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t.
    \end{equation}
\end{theorem}


\begin{proof}
    We define again $L$ to be the stochastic logarithm of $H$, $L \coloneqq \trans H \circ \d H$ and using the same techniques as in Theorem \ref{thm:diffusion_real} we have that,

    \begin{equation*}
        \d \Lambda = \trans H \circ(\partial X) \circ H - (\partial L)\circ \Lambda + \Lambda \circ \partial L.
    \end{equation*}

    Using that $\Lambda \circ \partial L -  (\partial L)\circ \Lambda $ has zero diagonal, we get that $\d lambda_i = (\trans H \circ(\partial X) \circ H)_{ii}$ and by hypothesis, we know that the martingale part of this diagonal is zero. 

    Define $\d N \coloneqq \trans H \circ(\partial X) \circ H$. For $i\neq j$ we have that $\d L_{ij} = \d N_{ij}/(\lambda_j - \lambda_i)$.

    Finally, we compute the finite variation $\d F$ part of $\d N$,

    \begin{align*}
        \d F &= \trans H b(X) H \d t + \frac12 (\d \trans H \d X H + \trans H \d X H), \\ 
        &= b(\Lambda) \d t + \frac12( \trans{(\d N\d A)} + \d N \d A ).
    \end{align*}

    For $\d N \d A$ we find

    \begin{align*}
        (\d N \d A)_{ij} &= \sum_{k\neq j} \d N_{ik}\d A_{kj} = \sum_{k\neq j} \frac{\d N_{ik}\d N_{kj}}{\lambda_j - \lambda_k}.
    \end{align*}

    Now we use that the martingale part of  $\d N$ has the same entries as $\trans{Q} M Q$ and by the results in Theorem \ref{thm:diffusion_real} we know that 

    \begin{equation*}
        (\trans{Q} M Q)_{ik}(\trans{Q} M Q)_{kj} = \delta_{ij}G(\lambda_i,\lambda_k)\d t,
    \end{equation*}

    \noindent so substituting the last result we get

    \begin{equation*}
        \d \lambda_i = \d F_{ii} = b(\lambda_i)\d t + \sum_{k\neq j} \frac{G(\lambda_i,\lambda_k)\d t}{\lambda_j - \lambda_k},
    \end{equation*}

    \noindent which is the desired result.



\end{proof}


These results can be particularized for any matrix-valued diffusions. Especially, we are interested in the Wishart and Jacobi processes. We give the proofs for these as a corollary to last Theorem.

\subsection{Wishart process}

\begin{corollary}
    Let $Y = (Y(t), t \ge 0)$ be an $n\times n$ Wishart process with parameter $m$ and diagonalization $Y = QM\trans{Q}$. Let $X$ be an $n\times n$ self adjoint matrix process with diagonalization $X = H \Lambda \trans{H}$, such that the off-diagonal part of $\trans{H} \d X H $ and $\overset{d}{=} \trans{Q} \d Y Q$ coincide, and $(\trans{H} \d X H)_{ii} = 0$ for every $i \in [n]$. Then the eigenvalues of $X$, $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$ satisfy the following system of stochastic differential equations

    \begin{equation} \label{eq:deterministic_wishart}
        \d \lambda_i = \left( m +\sum_{k\neq i} \frac{ \abs{\lambda_i} + \abs{\lambda_k} }{\lambda_i - \lambda_k}\right)\d t.
    \end{equation}
\end{corollary}

\begin{proof}
    
\end{proof}


\subsection{Jacobi process}

\begin{corollary}
    Let $Y = (Y(t), t \ge 0)$ be an $n\times n$ matrix Jacobi process with parameters $n_1,n_2$ and diagonalization $Y = QM\trans{Q}$. Let $X$ be an $n\times n$ self adjoint matrix process with diagonalization $X = H \Lambda \trans{H}$, such that the off-diagonal part of $\trans{H} \d X H $ and $\overset{d}{=} \trans{Q} \d Y Q$ coincide, and $(\trans{H} \d X H)_{ii} = 0$ for every $i \in [n]$. Then the eigenvalues of $X$, $\lambda_1 \ge \lambda_2 \ge \cdots \ge \lambda_n$ satisfy the following system of stochastic differential equations

    \begin{equation} \label{eq:deterministic_jacobi}
        \d \lambda_i = \left( n_2 - (n_1 + n_2)\lambda_i + \sum_{k\neq i} \frac{\lambda_k(1-\lambda_i) + \lambda_i(1-\lambda_k)}{\lambda_i - \lambda_k} \right)\d t.
    \end{equation}

\end{corollary}

\section{Dynamical behavior of the deterministic eigenvalue processes}

\begin{lemma} \label{lemma:separation}
    Let $\lambda_1,\dots,\lambda_n$ be a system of $n$ functions moving according to \eqref{eq:deterministic_dyson} and $\lambda_i(0) > \lambda_j(0)$ and there is no $\lambda_k$ such that $\lambda_i(0) > \lambda_k(0) > \lambda_j(0)$. Then, $\lambda_i$ and $\lambda_k$ repel each other if and only if 

    \begin{equation} \label{eq:separation_condition}
        \frac{2}{(\lambda_i - \lambda_j)^2} > \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)}
    \end{equation}
\end{lemma}

\begin{proof}

    The distance between the particles grows if and only if its derivative its positive, so 

    \begin{align*}
        \frac{\d }{\d t} (\lambda_i - \lambda_j) &= \sum_{k\neq i} \frac{1}{\lambda_i - \lambda_k} - \sum_{k\neq j} \frac{1}{\lambda_j - \lambda_k} = \sum_{k\neq i,j} \left( \frac{1}{\lambda_i - \lambda_k} - \frac{1}{\lambda_j - \lambda_k} \right) + \frac{2}{\lambda_i - \lambda_j},\\
        &= \sum_{k \neq i, j} \frac{\lambda_j - \lambda_k - \lambda_i + \lambda_k}{(\lambda_i- \lambda_k)(\lambda_j - \lambda_k)} + \frac{2}{\lambda_i - \lambda_j},= \frac{2}{(\lambda_i - \lambda_j)^2} - \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)}.
    \end{align*}

    Comparing to zero, we get the desired result.
\end{proof}



\begin{theorem}
    If a system of functions satisfies \eqref{eq:deterministic_dyson}, then there ar no collisions.
\end{theorem}

\begin{proof}
    Write condition \eqref{eq:separation_condition} as

    \begin{equation*}
        \frac{2}{(\lambda_i - \lambda_j)^2} > \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_j + \lambda_j -\lambda_k)(\lambda_j - \lambda_k)},
    \end{equation*}

    \noindent and take $\lambda_i \to \lambda_j$. As $\lambda_i$ approaches $\lambda_k$, the left-hand side grows to $\infty$, while the right-hand side converges to the finite quantity

    \begin{equation*}
        \sum_{k\neq i,j} \frac{1}{(\lambda_j - \lambda_k)^2}.
    \end{equation*}

    Thus, before the collision occurs, the distance between $\lambda_i$ and $\lambda_j$ starts to grow.
\end{proof}

\begin{theorem} \label{thm:hermite_minimal_grows}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of functions moving according to \eqref{eq:deterministic_dyson} and let $\lambda_i,\lambda_j$ be such that for a given $t_0$ it is satisfied $\lambda_i(t_0) > \lambda_j(t_0)$ and 
    
    \begin{equation*}
        \lambda_i(t_0) - \lambda_j(t_0) = \min_{k,l \in [n]} \abs{ \lambda_k(t_0) - \lambda_l(t_0)}.
    \end{equation*}

    Then, $\lambda_i$ and $\lambda_k$ repel. 
\end{theorem}

\begin{proof}
    Define the quotients $c_{ik}, c_{jk}$ as 

    \begin{equation*}
        c_{ik} = \frac{\lambda_i - \lambda_k}{\lambda_i - \lambda_j}, \qquad c_{jk} = \frac{\lambda_j - \lambda_k}{\lambda_i - \lambda_j}.
    \end{equation*}

    Notice that the condition that the distance between $\lambda_i$ and $\lambda_j$ is minimal means that there is no $\lambda_k$ between them and therefore for a fixed $k_0$, $c_{ik_0}$ and $c_{jk_0}$ have the same sign. We can write the right-hand side of \eqref{eq:separation_condition} as

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)} &= \sum_{k\neq i,j} \frac{1}{ c_{ik}(\lambda_i - \lambda_j) c_{jk}(\lambda_i - \lambda_j)} = \sum_{k\neq i,j} \frac{ (c_{ik}c_{jk})^{-1} }{(\lambda_i-\lambda_j)^2}.
    \end{align*}

    With this, condition \eqref{eq:separation_condition} can be written as

    \begin{align}
        \frac{2}{(\lambda_i - \lambda_j)^2} &> \sum_{k\neq i,j} \frac{ (c_{ik}c_{jk})^{-1} }{(\lambda_i-\lambda_j)^2},\\
        \Leftrightarrow 2 &> \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}}. \label{eq:simplified_condition}
    \end{align}

    We know that the distance between $\lambda_i$ and $\lambda_j$ is minimal, in the worst-case scenario, all of the other distances are the same as $\lambda_i - \lambda_j$, so $\abs{c_{ik}},\abs{c_{ij}}>1$. Notice that if $\lambda_{i-1}$ is the function located immediately bellow $\lambda_i$, this would mean that in the worst case scenario $c_{i,i-1}=-1$ and $c_{j,i-1} = -2$. Similarly, for $\lambda_{i-2}$ we would have $c_{i,i-1} = -2, c_{j,i-2} =-3$. In general, $c_{i,i-l} = -l, c_{j,i-l} = -(l+1)$. Analogously, if $\lambda_{j+l}$ is the function located $l$ positions below $j$, then $c_{j,j+l} = l, c_{i,j+l} = l+1$.

    Now, for the arrangement of the functions, we have two extreme cases. If $\lambda_i$ and $\lambda_j$ are the functions in one of the extremes (i.e. the two biggest or two smallest ones), this would mean that the right-hand side of \eqref{eq:simplified_condition} can be written as

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} &= \sum_{k=1}^{n-2} \frac{1}{k(k+1)} = \sum_{k=1}^{n-2} \frac{k+1 - k}{k(k+1)} = \sum_{k=1}^{n-2} \left( \frac{1}{k} - \frac{1}{k+1} \right),\\
        &= 1 - \frac{1}{n-1} = \frac{n-2}{n-1} < 2.
    \end{align*}

    So in this case, applying Lemma \ref{lemma:separation}, $\lambda_i,\lambda_j$ would repel each other. The other extreme case is when exactly half of the functions are located at each side of $\lambda_i$ and $\lambda_j$. Let us suppose first that $n$ is even, in this case we would have,

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} &= 2 \sum_{k=1}^{\frac{n-2}{2}} \frac{1}{k(k+1)} = 2\left(1 - \frac{2}{n}\right) = 2 - \frac4n < 2.
    \end{align*}

    There is also separation in this case. Finally, if $n$ is even, we considr the previos sum for $\lfloor n/2 \rfloor$ and sum the term corresponding to the remaining function.

    \begin{equation*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} = 2 - \frac{4}{n} + \frac1{(\lfloor n/2\rfloor+1)(\lfloor n/2\rfloor +2)} < 2.
    \end{equation*}

    So, even in the worst-case scenario, criterion \eqref{eq:separation_condition} is satisfied, and we conclude that the minimal distance grows for every initial condition of the system.
\end{proof}

\begin{corollary}
    In a system governed by~\eqref{eq:deterministic_dyson}, for $n\ge3$ not all of the functions separate for every given initial condition. For $n =2$, the functions always repel each other.
\end{corollary}

\begin{proof}
    We prove the first part by providing a counterexample. Suppose that the separation between $\lambda_i$ and $\lambda_j$ is 1, while all of the other separations are $0.1$. The same computations that the proof of in Theorem \ref{thm:hermite_minimal_grows}, but with a re-scaling of the $c_{ik}c_{jk}$ lead to

    \begin{align*}
        \sum_{k\neq i,j} \frac{1}{c_{ik}c_{jk}} = \frac{1}{0.01}\left( 1 - \frac1{n-1}\right) = 100 - \frac{100}{n-1}.
    \end{align*}

    Clearly, for $n\ge 3$, condition~\eqref{eq:separation_condition} is satisfied.

    For the second part, we simply take that if $\lambda_i$ and $\lambda_j$ are the unique functions in the system, then 

    \begin{equation*}
        \sum_{k\neq i,j} \frac{1}{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)} = 0 < \frac{2}{(\lambda_i - \lambda_j)^2},
    \end{equation*}

    \noindent for every position of $\lambda_i, \lambda_j$.
\end{proof}

\begin{lemma} \label{lemma:separating_condition_wishart}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of $n$ functions moving according to \eqref{eq:deterministic_wishart}, and let $\lambda_i, \lambda_j$ be such that $\lambda_i(t_0) > \lambda_j(t_0)$ and there is no $\lambda_k$ such that $\lambda_i(t_0) > \lambda_k(t_0) > \lambda_j(t_0)$. Then $\lambda_i$ and $\lambda_j$ repel each other if and only if 

    \begin{equation} \label{eq:separation_condition_wishart}
        \frac{\lambda_i + \lambda_j}{(\lambda_i - \lambda_j)^2} > \sum_{k\neq i,j} \frac{\lambda_k}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)}.
    \end{equation}
\end{lemma}

\begin{proof}

    Take the derivative of the separation and use linearity with \eqref{eq:deterministic_wishart} to get

    \begin{align*}
        \frac{\d}{\d t}(\lambda_i - \lambda_j) &= \sum_{k\neq i} \frac{\lambda_i + \lambda_k}{\lambda_i - \lambda_k} - \sum_{k\neq j} \frac{\lambda_j + \lambda_k}{\lambda_j - \lambda_k}, \\
        &= \sum_{k \neq i,j} \frac{ (\lambda_i + \lambda_k)(\lambda_j - \lambda_k) - (\lambda_j + \lambda_k)(\lambda_i - \lambda_k) }{(\lambda_i - \lambda_k)(\lambda_j - \lambda_k)} + 2\frac{\lambda_i + \lambda_j}{\lambda_i - \lambda_j},\\
        &= \sum_{k\neq i,j} \frac{2\lambda_k(\lambda_j - \lambda_i)}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)} + 2 \frac{\lambda_i + \lambda_j}{\lambda_i - \lambda_j}.
    \end{align*}

    Comparing to zero yields the result.
\end{proof}

\begin{corollary}
    In a system of $n$ functions satisfying \eqref{eq:deterministic_wishart} there is no collision of the functions.
\end{corollary}

\begin{proof}
    Use condition \eqref{eq:separation_condition_wishart} and let $\lambda_i \to \lambda_j$, then

    \begin{equation*}
        \frac{\lambda_i + \lambda_j}{(\lambda_i-\lambda_j)^2} \to \infty,
    \end{equation*}

    \noindent but

    \begin{align*}
        \sum_{k\neq i,j} \frac{\lambda_k}{(\lambda_i-\lambda_k)(\lambda_j-\lambda_k)} = \sum_{k\neq i,j} \frac{\lambda_k}{(\lambda_i-\lambda_j + \lambda_j -\lambda_k)(\lambda_j-\lambda_k)} \to \sum_{k\neq i,j} \frac{\lambda_k}{(\lambda_j-\lambda_k)^2} < \infty.
    \end{align*}

    So before the functions collide, the derivative of the separation is positive and they repel each other.
\end{proof}

\begin{theorem} \label{thm:laguerre_does_not_grow}
    Let $(\lambda_1, \lambda_2, \dots, \lambda_n)$ be a system of $n$ functions moving according to \eqref{eq:deterministic_wishart} and let $\lambda_i,\lambda_j$ be such that for a given $t_0$ it is satisfied $\lambda_i(t_0) > \lambda_j(t_0)$ and 
    
    \begin{equation*}
        \lambda_i(t_0) - \lambda_j(t_0) = \min_{k,l \in [n]} \abs{ \lambda_k(t_0) - \lambda_l(t_0)}.
    \end{equation*}

    Then, $\lambda_i$ and $\lambda_k$ do not necessarily repel. 
\end{theorem}

\begin{proof}
    We will prove providing an initial condition for which the minimal distance will have a negative derivative. Let $\lambda_j(t_0) = \min_{k \le n} \lambda_k(t_0)$ and so $\lambda_i(t_0) = \min_{k\neq j} \lambda_k(t_0)$. Similarly to the proof of Theorem \ref*{thm:hermite_minimal_grows}, let us define the quotients $c_{ik},c_{jk}$ as 

    \begin{equation*}
        c_{ik} = \frac{\lambda_i - \lambda_k}{\lambda_i - \lambda_j}, \qquad c_{jk} = \frac{\lambda_j - \lambda_k}{\lambda_i - \lambda_j}.
    \end{equation*}

    For every fixed $k$, the quotients $c_{ik}$ and $c_{jk}$ have the same sign and given the minimality of $\lambda_i - \lambda_j$ we have that $c_{ik}c_{jk}>1$. Using these quantities, the separation condition \eqref{eq:separation_condition_wishart} is reduced to 

    \begin{equation*}
        \lambda_i + \lambda_j > \sum_{k \neq i,j} \frac{\lambda_k}{c_{ik}c_{jk}}.
    \end{equation*}

    Suppose that $\lambda_i - \lambda_j =1$, and that $\lambda_k > \lambda_i$ for all $k \notin \{i,j\}$. Assume further that the separation between all the $(\lambda_k)_{k\neq i,j}$ is $1+\epsilon$ for some $\epsilon >0$. Then $c_{i,i-1}= 1+\epsilon, \abs{c_{j,i+1}}= 2 + \epsilon$ and in general $\abs{c_{i,i+l}}=l(1+\epsilon), \abs{c_{j,i+l}} = 1 + l(1+\epsilon)$. Furthermore, we have that $\lambda_{i+l}$ can be expressed as

    \begin{equation*}
         \lambda_{i+l} = \lambda_i + l(1+\epsilon) = \lambda_j + 1 + l(1+\epsilon).
    \end{equation*}

    With this, the separating condition for $\lambda_i$ and $\lambda_j$ can be written as

    \begin{equation*}
        2\lambda_j + 1 > \sum_{k=2}^{n} \frac{\lambda_j + 1 + k(1+\epsilon)}{k(1+\epsilon)(1+k(1+\epsilon))}. 
    \end{equation*}

    The left-hand side is fixed for fixed $\lambda_j(t_0)$. For the left-hand side we have

    \begin{align*}
        \sum_{k=2}^{n} \frac{\lambda_j + 1 + k(1+\epsilon)}{k(1+\epsilon)(1+k(1+\epsilon))} = \sum_{k=2}^n \frac{\lambda_j + 1}{k(1+\epsilon)(1+k(1+\epsilon))} + \sum_{k=2}^n \frac{1}{1+k(1+\epsilon)}.
    \end{align*}

    For the second element in the sum and $\epsilon$ sufficiently small we have 

    \begin{align*}
        \sum_{k=2}^n \frac{1}{1+k(1+\epsilon)} > \sum_{k=2}^n \frac{1}{1+2k} > \frac12\sum_{k=2}^n \frac{1}{1+k}.
    \end{align*}

    The last expression can be made arbitrarily big for $n$ big enough. We conclude that under these conditions, for a system with enough functions, the minimal distance can be made smaller with a specific initial condition.
    
\end{proof}

\section{Simulations}


\begin{figure}[h!]
    \input{img/four_det_dysons.pgf}
\end{figure}

\begin{figure}[h!]
    \input{img/deterministic_wishart.pgf}
\end{figure}

\begin{figure}[h!]
    \input{img/deterministic_jacobi.pgf}
\end{figure}