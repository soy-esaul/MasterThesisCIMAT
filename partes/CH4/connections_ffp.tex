\section{Connections with finite free probability}

In this section we relate finite free probability to eigenvalue processes. To do so, we first find the expected characteristic polynomial of the random matrices. We show that in some cases they are well-known polynomials whose convolutions satisfy good properties. We start with the case of a self-adjoint Brownian matrix and then we replicate the results for the Wishart and Jacobi processes.

\begin{lemma} \label{lemma:tridiag}
    Let $A_{\beta}$ denote an $n\times n$ matrix from the GOE, GUE or GSE, for $\beta = 1,2,4$, respectively, then the eigenvalues of the tridiagonal matrix $H_\beta$ have the same joint law as the eigenvalues of $A_\beta$, with $H_\beta$ defined as

    \begin{align} \label{eq:tridiag_hermite}
        H_\beta &= \frac1{\sqrt2}\begin{bmatrix}
            N_1   & \xi_2 & 0     & \cdots & 0 \\ 
            \xi_2 & N_2   & \xi_3 & \cdots & 0 \\
            0     & \xi_3 & N_3  & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\ 
            0      & \cdots & 0      & \xi_{n} & N_n 
        \end{bmatrix}.
    \end{align}

    The diagonal entries $N_i, 0 \le i \le n$ are independent normal random variables with mean 0 and variance 2, while the subdiagonal entries $\xi_i$ are independent random variables distributed as

    \[ \xi_i \sim \chi_{\beta(n+1-i)}. \]

    Note that in this case $\chi_\nu$ denotes the chi distribution which is the squared root of a chi-squared random variable or the absolute value of a normal random variable in the case $\nu$ is integer.

    We call $H_\beta$ the tridiagonal $\beta$-Hermite ensemble.

\end{lemma}

\begin{proof}
    Write $A_{\beta}$ as 

    \begin{align*}
        A_{\beta} &= \begin{bmatrix}
            N_1 & \trans{\vec x} \\ 
            \vec x & B_{\beta}
        \end{bmatrix},
    \end{align*}

    \noindent with $N_1$ a normal random variable, $\vec x$ an $n-1$-dimensional gaussian vector with independent entries in $\R, \C$ or $\H$, depending on $\beta$, and $B_{\beta}$ an $(n-1)\times(n-1)$ matrix from the GOE, GUE or GSE, respectively. All of the elements are independent from each other.

    Now we take $H$ to be any $(n-1)\times(n-1)$ orthogonal (or unitary, symplectic, according to $\beta$) matrix such that $H \trans{\vec x} = \norm{\vec x}_2 e_1$, where $e_1 = (1,0,\cdots,0)$ is the first element in the canonical basis of $\R^{n-1}$. Then we have

    \begin{align*}
        \begin{bmatrix}
            1 & \trans{\vec0} \\ 
            \vec0 & H
        \end{bmatrix} 
        \begin{bmatrix}
            N_1 & \trans{\vec x} \\ 
            \vec x & B_{\beta}
        \end{bmatrix}  
        \begin{bmatrix}
            1 & \trans{\vec 0} \\ 
            \vec 0 & \trans H
        \end{bmatrix} &= \begin{bmatrix}
            N_1 & \trans{\vec x} \\ 
            \norm{\vec x}_2e_1 & H B_{\beta}
        \end{bmatrix}  
        \begin{bmatrix}
            1 & \trans{\vec0} \\ 
            \vec0 & \trans H
        \end{bmatrix} 
        =
        \begin{bmatrix}
            N_1 & \norm{x}_2\trans{e_1} \\ 
            \norm{x}_2 e_1 & H B_\beta \trans{H}
        \end{bmatrix}.
    \end{align*}

    Now we can find the distribution of each of the blocks of the new matrix. The variable $N_1$ has not changed, it is a standard normal variable. The term $\norm{\vec x}_2$ is the norm of a Gaussian vector of length $n-1$ with real (complex or quaternionic) entries, non-correlated and with variance $1/2$, so it is distributed like a $\frac1{\sqrt{2}}\chi_{\beta(n-1)}$ random variable, where $\beta$ indicates the number of normal variables in each entry of the matrix. Since $B_\beta$ is a GOE (GUE, GSE), it is invariant under orthogonal (unitary, symplectic) transformations and thus $H B_{\beta} \trans{H}$ is a GOE (GUE, GSE).

    The matrix $ \begin{bmatrix}
        1 & \trans{\vec0} \\ 
        \vec0 & H
    \end{bmatrix}$ is orthogonal (unitary, symplectic), so the eigenvalue distribution of $A_\beta$ remains unchanged under this transformation. By repeating the procedure with $B_\beta$, we find the tridiagonal matrix \eqref{eq:tridiag_hermite}, which finishes the proof.
\end{proof}


\begin{lemma}
    Let $W_\beta$ be an $n\times n$ matrix from the  $\beta$-Wishart ensemble. The eigenvalues of $W_\beta$ has the same joint law as those of the tridiagonal matrix $L_\beta = B_\beta \trans B_\beta$ with $B_\beta$ is an $m\times n$ bidiagonal matrix defined as 

    \begin{align} \label{eq:tridiag_laguerre}
        B_\beta &= \begin{bmatrix}
            \xi_{n\beta}   & 0 & 0     & \cdots & 0 \\ 
            \xi_{\beta(m-1)} & \xi_{n\beta - \beta}   & 0 & \cdots & 0 \\
            % 0     & \ddots & \ddots  & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\ 
            0      & \cdots & 0      & \xi_{\beta} & \xi_{n\beta - \beta(m-1)} 
        \end{bmatrix}.
    \end{align}

    The variables $\xi_j$ being independent random variables with distribution $\xi_j \sim \chi_j$.

    We call $L_\beta$ the tridiagonal $\beta$-Laguerre ensemble.
\end{lemma}

\begin{proof}
    Take $G$ to be an $m\times n$ matrix with independent standard $\beta$-Gaussian random variables as entries (real if $\beta=1$, complex if $\beta=2$ and quaternions if $\beta=4$). Write $G$ as 

    \begin{equation*}
        \begin{bmatrix}
            \trans{\vec x}\\
            G_1
        \end{bmatrix}.
    \end{equation*}

    We have that $\vec x$ is a vector distributed as a multivariate $\beta$-normal variable with mean $\vec 0$ and covariance matrix $\Sigma = I$, while $G_1$ is an $(m-1)\times n$ matrix of independent standard $\beta$-Gaussian random variables. 

    Take $R$ to be a ``right reflector'' corresponding to $\vec x$ independent of $G_1$, which means $\trans{\vec x}R = \norm{\vec x}_2 e_1$. Due to being a reflector, $R$ is orthogonal (unitary, simplectic) and this means that $G_1 R$ is an $(m-1)\times n$ matrix with independent standard $\beta$-Gaussian matrices as entries.

    Now take $G_1R = [\vec y, G_2]$ with $\vec y$ being a $\beta$-Gaussian vector with mean $\vec 0$ and covariance matrix $\Sigma = I$. Then $G_2$ is an $(m-1)\times (n_1)$ matrix of independent standard $\beta$-Gaussian random variables. Let $L$ be a left reflector corresponding to $\vec y$ ($Ly = \norm{\vec y}_2 e_1$) independent of $G_2$. Again by the orthogonality (unitarity, simplecticity) of $L$, $LG_2$ is still an $(m-1)\times(n-1)$ matrix of independent standard $\beta$-Gaussian random variables. This means that

    \begin{equation*}
        \begin{bmatrix}
            1 & 0 \\
            0 & L
        \end{bmatrix} G R = \begin{bmatrix}
            \norm{\vec x}_2 & 0 \\
            \norm{\vec y}_2 e_1 & LG_2
        \end{bmatrix}.
    \end{equation*}

    We proceed with this procedure now for $LG_2$. The product by an orthogonal (unitary, simplectic) matrix does not affect the singular values of a matrix, so the singular values of the bidiagonal matrix $B_\beta$ and the original matrix $G$ are the same. The eigenvalues of $W = G\trans{G}$ are the squares of the singular values of $G$ and the same happens with the eigenvalues of $L_\beta = B_\beta\trans{B_\beta}$ which is a tridiagonal matrix.

    The distribution of the entries follows from the definition of the complex and simplectic normal distributions as sums of real normal random variables.
\end{proof}


\begin{lemma} \label{lemma:chpol_tridiag}
    Let $A$ be an $n\times n$ tridiagonal symmetric matrix with diagonal elements $\{a_i\}_{1\le i \le n}$ and subdiagonal elements $\{ b_j \}_{2 \le j \le n}$, such that

    \begin{equation*}
        A = \begin{bmatrix}
            a_n   & b_{n-1} & 0     & \cdots & 0 \\ 
            b_{n-1} & a_{n-1}   & b_{n-2} & \cdots & 0 \\
            0     & b_{n-2} & a_{n-2}  & \cdots & 0 \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\ 
            0      & \cdots & 0      & b_1 & a_1 
        \end{bmatrix}.
    \end{equation*}
    
    
    Then its characteristic polynomial $Q_n(x) = \det(xI_n - A)$ satisfies the following recursion

    \begin{equation*}
        Q_n(x) = (x-a_n)Q_{n-1}(x) - b_{n-1}^2Q_{n-2}(x). \label{eq:recursion}
    \end{equation*}
\end{lemma}

\begin{proof}
    Let us write

    \begin{equation*}
        A = \begin{bmatrix}
            a_n   & b_{n-1} \trans{e_1} \\
            b_{n-1}e_1 & B 
        \end{bmatrix},
    \end{equation*}

    \noindent with $e_1$ the first element in the canonical base of $\R^{n-1}$, and write $B$ as

    \begin{equation*}
        B = \begin{bmatrix}
            a_{n-1}   & b_{n-2} \trans{{e_1^{(n-2)}}} \\
            b_{n-2} e_1^{(n-2)} & C 
        \end{bmatrix}.
    \end{equation*}

    The determinant of $xI_n - A$ is 

    \begin{align*}
        \det \left( xI_n - A \right) &= (xI_n - A)_{11}\det(xI_{n-1}-B) - (xI_n - A)_{12}(xI_n - A)_{21}\det(xI_n - C)\\ 
        &= (x - a_n)Q_{n-1}(x) - b_{n-1}^2Q_{n-2}(x).
    \end{align*}
\end{proof}

\begin{theorem}
    Let $X$ be an $n\times n$ GUE, then its expected characteristic polynomial, $P_n(x)$ is the $n$th Hermite polynomial.
\end{theorem}

\begin{proof}
    Let $A_2$ be a tridiagonalization of $X$, due to Lemma \ref{lemma:tridiag} we know that the eigenvalues of $A_2$ have the same joint distribution of those of $X$, so the expected characteristic polynomial must coincide. We can use Lemma \ref{lemma:chpol_tridiag} to find the expected characteristic polynomial of $A$. Let $Q_n$ denote its characteristic polynomial, then

    \begin{align*}
        P_n(x) &= \mathbb E\left( \left(x - \frac{N_1}{\sqrt2}\right)Q_{n-1}(x) - \frac{\xi_{2(n-1)}^2}{2}Q_{n-2}(x) \right),\\ 
               &= xP_{n-1}(x) - (n-1)P_{n-2}(x).
    \end{align*}

    So $P_n(x)$ satisfies the recursion that determines the Hermite polynomials. We need to check the initial conditions $P_1(x)$ and $P_2(x)$. For $P_1(x)$ the condition is trivial

    \begin{equation*}
        P_1(x) = \mathbb E \left( x - \frac{N_1}{\sqrt{2}} \right) = x.
    \end{equation*}

    For $P_2(x)$, we have

    \begin{align*}
        P_2(x) &= \mathbb E \left( \det \begin{bmatrix}
        x - N_1 & \xi_{2} \\ 
        \xi_2 & x - N_2
        \end{bmatrix} \right) = \mathbb E \left[ \left(x - \frac{N_1}{\sqrt{2}}\right)^2 - \frac{\xi_2^2}2 \right],\\ 
        &= x^2 - 1.
    \end{align*}

    So $P_1(x) = H_1(x)$ and $P_2(x) = H_2(x)$. Using the recursion, we can conclude that $P_n(x) = H_n(x)$.
\end{proof}

\begin{corollary} \label{corollary:brownian_expected}
    Let $B(t)$ be an $n\times n$ self-adjoint complex Brownian matrix, then its expected characteristic polynomial $P_n(x,t)$ is the $n$th generalized Hermite polynomial with variance $t$, $H^{[t]_n}$, i.e. the Hermite polynomials which are orthogonal with respect to a Gaussian random variable of variance $t$.

\end{corollary}

\begin{proof}

    We will prove the result by showing that $P_n(x,t)$ satisfies the following recursion
    
    \begin{align*}
        P_n(x,t) &= x P_{n-1}(x,t) - t(n-1)P_{n-2}(x,t),
    \end{align*}

    \noindent with initial conditions $P_1(x,t) = x$ and $P_2(x,t) = x^2-t$.

    For any given $t$, $B(t)$ has the same law as $\sqrt{t}A_2$ with $A_2$ a GUE. By Lemma \ref{lemma:tridiag}, the expected characteristic polynomial is the same as that of $\sqrt{t}H_2$ with $H_2$ a $2$-Hermite polynomial. Let $Q_n(x,t)$ be the characteristic polyomial of $\sqrt{t}H_2$, applying Lemma \ref{lemma:chpol_tridiag} we have

    \begin{align*}
        P_n(x,t) &= \mathbb E \left(  \left( x - \sqrt{\frac t2}N_1 \right) Q_{n-1}(x,t) - \frac t2 \xi_{2(n-1)}^2 Q_{n-2}(x,t)\right), \\ 
        &= x P_{n-1}(x,t) - t(n-1)P_{n-2}(x,t).
    \end{align*}

    Now we find the first two polynomials,

    \begin{align*}
        P_1(x,t) &= \mathbb E \left( x - \sqrt{\frac t2}N_1 \right) = x,\\
        P_2(x,t) &= \mathbb E \left( \det\begin{bmatrix}
            x - \sqrt{\frac t2}N_1 & \sqrt{\frac t2} \xi_2 \\ 
            \sqrt{\frac t2} \xi_2  & x - \sqrt{\frac t2}N_2                         
        \end{bmatrix} \right),\\ 
        &= \mathbb E \left( \left(x - \sqrt{\frac t2}N_1\right)\left( x - \sqrt{\frac t2}N_2 \right) - \frac t2 \xi_2^2 \right),\\ 
        &= x^2 - t.
    \end{align*}
\end{proof}

    With the last result, we have found a recursion and initial conditions that are enough to uniquely determine the expected characteristic polynomial of a self-adjoint complex Brownian matrix.

\begin{theorem} \label{thm:heat_burgers}
    The Hermite polynomials $H_n(z,t)$ solve the differential equation

    \begin{equation} \label{eq:calor}
        \frac{\partial}{\partial t} H_n(z,t) + \frac12 \frac{\partial^2}{\partial z^2} H_n(z,t) = 0,
    \end{equation}

    \noindent and the Cauchy transform $G_{H_n}(y)$ of the empirical measure associated to its roots $\left\{ z_j(t) \right\}_{j\in [n]}$, 
    
    \begin{equation*}
        G_{H_n}(z,t) \coloneqq \frac1n \sum_{j=1}^n \frac{1}{z_j(t) - z},
    \end{equation*}

    \noindent satisfies the viscous Burgers equation with diffusion coefficient $-1/2$,

    \begin{equation*} \label{eq:burgers_12}
        \frac{\partial G_{H_n}(z,t)}{\partial t} + nG_{H_n}(z,t)\frac{\partial G_{H_n}(z,t)}{\partial z} = -\frac12 \frac{\partial^2 G_{H_n}(z,t)}{\partial z^2}.
    \end{equation*}
    
\end{theorem}

\begin{proof}

    Lets us write $H_n(z,t)= H_n$, $G_{H_n}(z,t) = G$ and $\frac{\partial f}{\partial z} = \partial_z f$ to simplify the notation.

    First, we prove that $H$ satisfies \eqref{eq:calor}. Write $H_n(z,t) = \exp\left\{-\frac{t \partial_z^2}{2}\right\}(z^n)$, and

    \begin{align*}
        \partial_t H_n &= \partial_t \exp\left\{-\frac{t \partial_z^2}{2}\right\}(z^n) 
        = \partial_t \sum_{j=0}^{\lfloor n/2 \rfloor} (-1)^j \frac{t^j \partial_z^{2j}(z^n) }{2^j j!},\\ 
        &= \sum_{j=0}^{\lfloor n/2 \rfloor} (-1)^j \frac{\partial_t (t^j) \partial_z^{2j}(z^n) }{2^j j!} = \sum_{j=0}^{\lfloor n/2 \rfloor} (-1)^j \frac{j(t^{j-1}) \partial_z^{2j}(z^n) }{2^j j!},\\ 
        &= \sum_{j=1}^{\lfloor n/2 \rfloor} (-1)^j \frac{j(t^{j-1}) \partial_z^{2j}(z^n) }{2^j j!}= -\frac12\sum_{j=1}^{\lfloor n/2 \rfloor} (-1)^{j-1} \frac{(t^{j-1}) \partial_z^{2(j-1)}\partial_{zz}(z^n) }{2^{j-1} (j-1)!}, \\ 
        &= -\frac12\partial_{zz} \sum_{k=0}^{\lfloor n/2 - 1\rfloor} (-1)^{k} \frac{(t^{k}) \partial_z^{2k}(z^n) }{2^{k} k!} = - \frac12 \partial_{zz} H_n.
    \end{align*}

    
    
    For the Cauchy transform part, we recall from Lemma \ref{lemma:cauchy_empirical_polynomial} that since $H_n$ is monic on $z$, then  $G= \frac{\partial_z H_n}{nH_n}$.

    Now, let us show that $G$ satisfies \eqref{eq:burgers_12}, for this we compute $\partial_t G$, $G\partial_z G$ and $\partial_{zz}G$,

    \begin{align*}
        \partial_t G &= \frac1n\partial_t \left( \frac{\partial_z H_n}{H_n} \right) = \frac1n\frac{ H_n \partial_t \partial_z H_n - \partial_z H_n \partial_t H_n }{H_n^2},\\ 
        \partial_z G &= \frac1n\partial_z \left( \frac{\partial_z H_n}{H_n} \right) = \frac1n\frac{H_n \partial_{zz}H_n - (\partial_z H_n)^2}{H_n^2},\\ 
        \partial_{zz} G &= \frac1n\partial_z \left( \frac{H_n \partial_{zz}H_n - (\partial_z H_n)^2}{H_n^2} \right), \\ 
        &= \frac1n \frac{ H_n^2\partial_z H_n \partial_{zz}H_n + H_n^3\partial_{zzz}H_n - 2H_n^2\partial_z H_n \partial_{zz}H_n }{H_n^4} \\
        & \phantom{separadoasídem}-\frac1n\frac{2H_n^2\partial_z H_n \partial_{zz}H_n - 2 H_n (\partial_z H_n)^3}{H_n^4},\\
        &= \frac1n\frac{-3H_n\partial_z H_n \partial_{zz}H_n + H_n^2 \partial_{zzz}H_n + 2(\partial_z H_n)^3}{H_n^3} \\
        G\partial_z G &= \frac{\partial_z H_n}{nH_n}\left( \frac{H_n \partial_{zz}H_n - (\partial_z H_n)^2}{H_n^2}  \right) = \frac{1}{n^2}\frac{H_n\partial_z H_n \partial_{zz}H_n - (\partial_z H_n)^3}{H_n^3}.
    \end{align*}

    Finally, we can use the above results to find

    \begin{align*}
        \partial_t G + n G\partial_z G &= \frac1n\left(\frac{ H_n \partial_t \partial_z H_n - \partial_z H_n \partial_t H_n }{H_n^2} + \frac{H_n\partial_z H_n \partial_{zz}H_n - (\partial_z H_n)^3}{H_n^3} \right),\\ 
        &= \frac1n\left( \frac{ -\frac12H_n^2 \partial_{zzz} H_n + \frac12H_n\partial_z H_n \partial_{zz} H_n + H_n\partial_z H_n \partial_{zz}H_n - (\partial_z H_n)^3}{H_n^3} \right), \\ 
        &= \frac1n\left( \frac{\frac32 H_n^2 \partial_{z}H_n\partial_{zz}H_n - \frac12H_n^2\partial_{zzz}H_n - (\partial_z H_n)^3}{H_n^3} \right) = -\frac12 \partial_{zz}G.
    \end{align*}
    % \begin{align*}
    %     \partial_t s &= \partial_t \left( \frac{-\partial_z P}{P} \right) = \frac{ \partial_z P \partial_t P - P \partial_t \partial_z P }{P^2},\\ 
    %     \partial_z s &= \partial_z \left( \frac{-\partial_z P}{P} \right) = \frac{(\partial_z P)^2 - P \partial_{zz}P}{P^2},\\ 
    %     \partial_{zz} s &= \partial_z \left( \frac{(\partial_z P)^2 - P \partial_{zz}P}{P^2} \right), \\ 
    %     &= \frac{\partial_z(\partial_z P)^2 P^2 - 2P(\partial_z P)^2\partial_{z}P}{P^4} - \frac{ P^2\partial_z P \partial_{zz}P + P^3\partial_{zzz}P - 2P^2(\partial_z P)\partial_{zz}P }{P^4}, \\ 
    %     &= \frac{ 2P^2\partial_z P \partial_{zz}P - 2P(\partial_z P)^2\partial_{z}P}{P^4} + \frac{ P^2\partial_z P \partial_{zz}P - P^3\partial_{zzz}P}{P^4}, \\
    %     &= \frac{ 3P\partial_z P \partial_{zz}P - 2(\partial_z P)^3 - P^2\partial_{zzz}P}{P^3}, \\ 
    %     %&=\frac{ \partial_{zz}P\partial_z P + \partial_z (\partial_z P)^2 - P \partial_{zzz}P }{P^2} - 2\frac{(\partial_z P)^3}{P^2},
    %     s\partial_z s &= -\frac{\partial_z P}{P} \left( \frac{(\partial_z P)^2 - P \partial_{zz}P}{P^2} \right) = \frac{P \partial_z P \partial_{zz}P - (\partial_z P)^3}{P^3}
    % \end{align*}
\end{proof}



\begin{corollary}
    The roots $\left\{z_i(t)\right\}_{i\le n}$ of the Hermite polynomials $H_n(z,t)$ satisfy the deterministic Dyson's equation,

    \begin{equation*}
        \d z_i = \sum_{k\neq i} \frac{\d t}{z_{i} - z_k}.
    \end{equation*}
\end{corollary}

\begin{proof}
    Let $z_i(t)$ be the roots of $H_n(z,t)$, this means

    \begin{align*}
        H_n(z_i(t),t) &= 0,\\ 
        \partial_t H_n(z_i(t),t) &= 0.
    \end{align*}

    By the chain rule and the heat kind equation \eqref{eq:calor} in Theorem \ref{thm:heat_burgers} we have

    \begin{align*}
        0 &= \partial_t H_n(z_i(t),t) = (\partial_t (z_i))\partial_z H_n(z_i(t),t) - \frac12 \partial_{zz} H_n(z_i(t),t),
        \intertext{by the Leibniz rule}
        \frac{\d}{\d t} z_i(t) &= \frac{\partial_{zz} H_n(z_i(t),t)}{2\partial_z H_n(z_i(t),t)} = \frac{2 \sum_{k\neq i} \prod_{j\neq i, j\neq k} (z_i - z_j)}{2 \prod_{j\neq i} (z_i - z_j)} = \sum_{k\neq i} \frac{1}{z_i - z_k}.
    \end{align*}
\end{proof}



\begin{theorem}
    Let $A$ be a $d\times d$ fixed matrix and $W$ a $d\times d$ self-adjoint Brownian matrix, then $q_A(z,t)$ defined as
    
    \begin{equation*}
        q_A(z,t) \coloneqq \E{ \chi_z( A + W) },
    \end{equation*}
    
    \noindent satisfies the following differential equation
    
    \begin{equation*}
        \partial_t q_A(z,t) + \frac12 \partial_{zz} q_A(z,t) = 0.
    \end{equation*}
    \end{theorem}
    
    \begin{proof}
        
        Let $p(z) = \E{ \chi_z(A) }$ and $r(z,t) = \E{ \chi_z(W) }$. Corollary \ref{corollary:brownian_expected} tells us that $r(z,t)$ is the $d$th Hermite polynomial $H_d(z,t)$. First suppose that e write these polynomials as

        \begin{align*}
            p(z)     &= \sum_{j=0}^d a_j z^j,\\ 
            H_d(z,t) &= \sum_{j=0}^d b_j t^{j/2} z^{d-j}.
        \end{align*}
        
        Notice that if $d$ is $odd$ all of the $b_j$ are zero for even $j$  and if $d$ is even, all of the $b_j$ are zero for odd $j$. Further, using the explicit expression for the coefficients we have the following recursion for $b_j$

        \begin{align}
            b_{j} &= \frac{d! (-1)^{j/2}}{2^{j/2} (j/2)! (d-j)!},\\ 
            b_{j-2} &= \frac{d! (-1)^{\frac{j-2}{2}}}{2^{\frac{j-2}{2}} \left(\frac{j-2}{2}\right)! (d-j+2)!},\\ 
            \Rightarrow b_{j} &= \frac{d! (-1)^{\frac{j-2}{2}}}{2^{\frac{j-2}{2}} \left(\frac{j-2}{2}\right)! (d-j+2)!} \frac{(-1)(d-j+2)(d-j+1)}{j},\\  
            &= b_{j-2} \frac{(-1)(d-j+2)(d-j+1)}{j}. \label{eq:hermite_coef_recurr}
        \end{align}

        Using the invariance of $W$ under unitary transforms and Theorem \ref{thm:symmad} we have that $q_A(z,t) = H_d(z,t) \boxplus_d p(z)$. By definition, 

        \begin{equation*}
            q_A(z,t) = H_d(z,t) \boxplus_d p(z) \coloneqq \sum_{k=0}^d z^{d-k}(-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2},
        \end{equation*} 

        \noindent with $c_{k,i,d} = \frac{(d-i)!(d-k+i)!}{d!(d-k)!}$.

        % In order to prove the differential equation, we compute the derivatives of each power. The coefficient of $z^{d-k}$ is $e_k = (-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2}$, then

        % \begin{align*}
        %     \partial_{zz} e_k z^{d-k} &= (d-k)(d-k-1)e_k z^{d-k-2}.
        % \end{align*}

        % So $\partial_{zz} q_A(z,t)$ is a polynomial of order $d-k-2$ with 
        
        % \begin{equation*}
        %     \partial_{zz} q_A(z,t) = \sum_{k=0}^{d-2} z^{d-2-k}(d-k)(d-k-1)e_k = \sum_{k=0}^{d-2} z^{d-2-k}(d-k)(d-k-1) (-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2}.
        % \end{equation*}

        % Now for the derivative with respect to $t$,

        % \begin{align*}
        %     \partial_t e_k z^{d-k} &= \partial_t \left((-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2} z^{d-k}\right) = (-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} \frac{it^{(i-2)/2}}{2}z^{d-k}.
        % \end{align*}

        We compute first $\frac{\partial^2}{\partial z^2}q_A(z,t)$,

        \begin{align*}
            \partial_{zz} q_A(z,t) %&= \partial_{zz} \left[ \sum_{k=0}^d z^{d-k}(-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2} \right],\\ 
            &= \partial_{zz} \left[ \sum_{k=0}^d z^{d-k}(-1)^k \sum_{i=0}^k \frac{(d-i)!(d-k+i)!}{d!(d-k)!} b_i a_{k-i} t^{i/2} \right],\\
            &= \sum_{k=0}^{d-2} (d-k)(d-k-1)z^{d-k-2}(-1)^k \sum_{i=0}^k \frac{(d-i)!(d-k+i)!}{d!(d-k)!} b_i a_{k-i} t^{i/2}, \\
            &= \sum_{k=0}^{d-2} z^{d-k-2}(-1)^k \sum_{i=0}^k \frac{(d-i)!(d-k+i)!}{d!(d-k-2)!} b_i a_{k-i} t^{i/2}, \\
            &= \sum_{k=2}^{d} z^{d-k}(-1)^k \sum_{i=0}^{k-2} \frac{(d-i)!(d-(k-2)+i)!}{d!(d-(k-2)-2)!} b_i a_{k-2-i}t^{i/2},\\ 
            &= \sum_{k=2}^{d} z^{d-k}(-1)^k \sum_{i=0}^{k-2} \frac{(d-i)!(d-k+i+2)!}{d!(d-k)!} b_i a_{k-2-i}t^{i/2}.
        \end{align*}

        Now the derivative with respect to $t$ is

        \begin{align*}
            \partial_t q_A(z,t) &= \partial_t \left[ \sum_{k=0}^d z^{d-k}(-1)^k \sum_{i=0}^k c_{k,i,d} b_i a_{k-i} t^{i/2} \right],\\ 
            &= \sum_{k=0}^d z^{d-k}(-1)^k \sum_{i=0}^k \frac{(d-i)!(d-k+i)!}{d!(d-k)!} b_i a_{k-i} \frac{i}{2}t^{\frac{i-2}{2}},\\ 
            &= \sum_{k=0}^d z^{d-k}(-1)^k \sum_{j=0}^{k-2} \frac{(d-j-2)!(d-k+j+2)!}{d!(d-k)!} b_{j+2}a_{k-j-2}\left( \frac{j+2}{2} \right)t^{j/2},\\ 
            &= \sum_{k=0}^d z^{d-k}(-1)^k \sum_{j=0}^{k-2} \frac{(d-j)!(d-k+j+2)!}{d!(d-k)!} b_ja_{k-j-2}\frac{(j+2)t^{j/2}}2,\\
            \intertext{using \eqref{eq:hermite_coef_recurr} for $b_{j+2}$} 
            &= \frac12 \sum_{k=0}^d z^{d-k}(-1)^{k+1} \sum_{j=0}^{k-2} \frac{(d-j)!(d-k+j+2)!}{d!(d-k)!} b_ja_{k-j-2}t^{j/2}, \\ 
            &= -\frac12 \sum_{k=2}^d z^{d-k}(-1)^{k} \sum_{j=0}^{k-2} \frac{(d-j)!(d-k+j+2)!}{d!(d-k)!} b_ja_{k-j-2}t^{j/2} = -\frac12 \partial_{zz} q_A(z,t).
        \end{align*}





    \end{proof}
    







\begin{theorem}
    Let $R$ be an $n\times n$ matrix and $R_{ij}$ be independent Gaussian random variables with mean 0 and variance 1, then

    \begin{equation*}
        E\left[ \chi_x\left(\trans{R}R\right) \right] = L_n(z),
    \end{equation*}

    \noindent where $L_n(x) = \left( 1 - \frac{\d}{\d z} \right)^n z^n$ is the $n$th Laguerre polynomial.
\end{theorem}

\begin{proof}
    We use the tridiagonal model defined before together with Lemma \ref{lemma:chpol_tridiag}. Let $P_n(z)$ be the characteristic polynomial of $L_\beta$ with $L_\beta$ being a tridiagonal $\beta$-Laguerre ensemble, then

    \begin{equation*}
        P_n(z) = (z - (L_\beta)_{11})P_{n-1}(z) - (L_\beta)_{12}^2 P_{n-2}(z).
    \end{equation*}

    The corresponding entries are

    \begin{equation*}
        (L_\beta)_{11} = \xi^2_{n\beta} + \xi^2_{\beta(m-1)}, \qquad (L_\beta)_{12} = \xi_{\beta(n-1)}\xi_{\beta(m-1)},
    \end{equation*}

    \noindent with this the expected characteristic polynomial satisfies the recursion

    \begin{align} \label{eq:laguerre_recursion}
        \E{P_n(z)} &= \E{ (z - \xi^2_{n\beta} - \xi^2_{\beta(m-1)})P_{n-1}(z) - \xi^2_{\beta(n-1)}\xi_{\beta(m-1)}^2 P_{n-2}(z) },\\ 
        &= (z - \beta(n - m + 1))\E{P_{n-1}(z)} - \beta^2(n-1)(m-1)P_{n-2}(z).
    \end{align}

    Which is satisfied by the Laguerre polynomials.\todo{Mostrar que los polinomios de Laguerre satisfacen ests recursión cuando los introduzca} By finding the cases $n=2$ and $n=3$, then using a recursive argument, we finish the proof.

    The case $n=1$ is trivial because in this case $G$ is an univariate Gaussian random variable, so $\trans{G}G = G^2$ is a chi-squared random variable and $P_1(z) = z - 1$. For the case $n=2$ we have

    \begin{align*}
        P_2(z) &= \det[zI - \trans{G}G] = \det\begin{bmatrix}
            z- g_1^2 - g_2^2 & -g_1g_3 - g_2 g_4 \\
            - g_1g_3 - g_2 g_4 & z - g_3^2 - g_4^2
        \end{bmatrix}, \\
        &= z^2 - z(g_1^2 + g_2^2 + g_3^2 + g_4^2) +g_1^2 g_3^2 + g_2^2g_4^2 -2g_1g_2g_3g_4.
    \end{align*}

    Taking expectation leads to $P_z(z) = z^2 - 4z + 2$. The rest of the polynomials are found using recursion \eqref{eq:laguerre_recursion}.
\end{proof}

\begin{theorem}
    Let $B$ be an $n\times n$ matrix and $R_{ij}$ be independent standard Brownian motions, then 

    \begin{equation*}
        E\left[ \chi_z\left( \trans{B}B\right) \right] = L_n(z,t),
    \end{equation*}

    \noindent where $L_n(z,t) = \left( 1 - t\frac{\d}{\d z} \right)^n z^n$ is the $n$th Laguerre polynomial with variance $t$.
\end{theorem}

\begin{proof}
    Let $G$ be an $n\times n$ matrix of independent standard Gaussian random variables, so $B \overset{d}{=} \sqrt{t}G$ and this means $\trans{B}B \overset{d}{=} t \trans G G$, it follows that

    \begin{align*}
        \E{\det[zI - \trans B B]} &= \E{\det[zI - t \trans G G]} = t^n \E{\frac zt I - \trans GG]}, \\
        &= t^n \left(1 - \frac{\d }{\d (z/t)}\right)^n\left[ \frac{z}{t}^n\right] = t^n\sum_{k=0}^n (-1)^k \binom nk \frac{n!}{(n-k)!} \left(\frac zt\right)^{n-k},\\
        &= \sum_{k=0}^n (-t)^k \binom nk \frac{n!}{(n-k)!} z^{n-k} = \left( 1 - t\frac{\d}{\d z} \right)^n z^n.
    \end{align*}
\end{proof}


\begin{theorem}
    Let $c>0$ and $P(t,z) \coloneqq \left( 1 - ct\partial_z \right)^n [z^n]$, with $t\ge 0, z \in \C$, then $P(t,z)$ satisfies the following differential equation

    \begin{equation*} 
        c\partial_z P(t,z) + cz  \partial_{zz}P(t,z) + \partial_t P(t,z) = 0.
     \end{equation*}

\end{theorem}

 \begin{proof}
    By definition of $P(t,z)$,

    \begin{align*}
        P(t,z) &= \left( 1 - ct\partial_z \right)^n [z^n] = \sum_{k=0}^n \binom{n}{k}\left( -ct \right)^k \partial_z^k [z^n ] = \left( 1 - ct\partial_z \right)^n [z^n] = \sum_{k=0}^n \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k)!}z^{n-k}.
    \end{align*}

    Now we find the derivatives

    \begin{align*}
        \partial_z P(t,z) &= \partial_z\left[ \sum_{k=0}^n \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k)!}z^{n-k} \right] = \sum_{k=0}^{n-1} \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k-1)!}z^{n-k-1},\\ 
        \partial_{zz} P(t,z) &= \partial_z\left[\sum_{k=0}^{n-1} \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k-1)!}z^{n-k-1}\right] = \sum_{k=0}^{n-2} \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k-2)!}z^{n-k-2},\\ 
        \partial_t P(t,z) &= \partial_t \left[ \sum_{k=0}^n \binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k)!}z^{n-k} \right] = \sum_{k=1}^n \binom{n}{k}k(-c)^kt^{k-1} \frac{n!}{(n-k)!}z^{n-k},\\ 
        &= \sum_{k=0}^{n-1} \binom{n}{k+1}(k+1)(-c)^{k+1}t^{k} \frac{n!}{(n-k-1)!}z^{n-k-1}.
    \end{align*}

    For the sum we have

    \begin{align*}
        c\partial_z P(t,z) + cz\partial_{zz} + \partial_t P(t,z) &= \sum_{k=0}^{n-2} \left[ c\binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k-1)!}z^{n-k-1} + cz\binom{n}{k}\left( -ct \right)^k \frac{n!}{(n-k-2)!}z^{n-k-2}\right.\\ &\phantom{espac} + \left.\binom{n}{k+1}(k+1)(-c)^{k+1}t^{k} \frac{n!}{(n-k-1)!}z^{n-k-1}\right] + cn(-ct)^{n-1}n!+n(-c)^nt^{n-1}n!,\\ 
        &= \sum_{k=0}^{n-2} \binom{n}{k} (-ct)^k n!\left( \frac{cz^{n-k-1}+(n-k-1)cz^{n-k-1}-c(n-k)z^{n-k-1}}{(n-k-1)!} \right), \\ 
        &=\sum_{k=0}^{n-2} \binom{n}{k} \frac{ c^{k+1}(-t)^kn!z^{n-k-1}}{(n-k-1)!}\left( 1 + n-k-1 -n +k \right) = 0.
    \end{align*}
 \end{proof}

 \begin{theorem}
    Let $P(t,z)$ be a monic polynomial with degree $n$ satisfying the equation

     \begin{equation*} 
        c\partial_z P(t,z) + cz  \partial_{zz}P(t,z) + \partial_t P(t,z) = 0.
     \end{equation*}

    Then its roots $\left(z_i(t)\right)_{i=1}^n$ satisfy the equation of motion

    \begin{equation*}
        \frac{\d z_i}{\d t} = c \left( \sum_{k\neq i} \frac{z_i + z_k}{z_i - z_k}  + n\right).
    \end{equation*}
\end{theorem}

\begin{proof}
    Let $z_i(t)$ be such that $P(t,z_i(t)) = 0$ for every $t$, this means in particular that $\partial_z P(t,z_i(t)) = 0$, so

    \begin{align*}
        0 &= \partial_t P(t,z_i(t)) = \partial_t z_i(t) \partial_z P(t,z)|_{z=z_i} + \partial_t P(t,z)|_{z=z_i}, \\ 
        &= \partial_t z_i(t) \partial_z P(t,z)|_{z=z_i} - \left[c\partial_z P(t,z)- cz \partial_{zz}P(t,z)\right]_{z=z_i},
        \intertext{now we use that $P(t,z)$ is monic and the Leibnitz rule to get}
        \partial_t z_i(t) &= c\left[\frac{\partial_z P(t,z) + z  \partial_{zz}P(t,z)}{\partial_z P(t,z)}\right]_{z=z_i} = c\left[ 1 +  \frac{2z_i\sum_{k\neq i}\prod_{j\neq i,j\neq k}(z_i - z_j)}{\prod_{j\neq i}(z_i - z_j)} \right] = c\left[ 1 + \sum_{k\neq i}\frac{2z_i}{z_i - z_k}\right],\\
        &= c\left[ 1 + \sum_{k\neq i}\frac{2z_i}{z_i - z_k} - \sum_{k\neq i}\frac{z_i-z_k}{z_i - z_k} + \sum_{k\neq i}\frac{z_i-z_k}{z_i - z_k} \right],\\ 
        &= c\left[ 1 + \sum_{k\neq i}\frac{2z_i-z_i + z_k}{z_i - z_k} + n-1 \right] = c\left[\sum_{k\neq i}\frac{z_i + z_k}{z_i - z_k} + n\right].
    \end{align*}
\end{proof}

\begin{theorem}
    Let $w_n(t_1,z) = (1-ct_1\partial_z)^n[z^n]$ and $w_n(t_2,z) = (1-ct_2\partial_z)^n[z^n]$ be two Laguerre polynomials of degree $n$, then their asymmetric additive convolution $w_n(t_1,z) \aac w_n(t_2,z)$ is a Laguerre polynomial of degree $n$ with variance $t_1+t_2$, $w_n(t_1+t_2,z)$. 
\end{theorem}

\begin{proof}
    \begin{align*}
        w_n(t_1,z) \aac w_n(t_2,z) &= \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2\binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}\binom{n}{k-j}\frac{(ct_1)^{k-j} n!}{(n-k+j)!}\\ 
        &= \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=0}^k \frac{((n-j)!)^2((n-k+j)!)^2(n!)^4(ct_1)^j(ct_2)^{k-j}}{(n!)^2((n-k)!)^2j!(k-j)!((n-j)!)^2((n-k+j)!)^2},\\ 
        &= \sum_{k=0}^n \binom{n}{k} \frac{(-1)^k z^{n-k}n!}{(n-k)!}\sum_{j=0}^k \binom{k}{j}(ct_1)^j(ct_2)^{k-j}, \\  
        &=  \sum_{k=0}^n \binom{n}{k} \frac{(-1)^k z^{n-k}n!}{(n-k)!}[c(t_1 + t_2)]^k = w_n(t_1 + t_2, z).
    \end{align*}
\end{proof}

\begin{theorem}
    Let $w_n(t,z)$ be a Laguerre polynomial of order $n$ with variance $t$ and let $p(z)$ be any monic polynomial. Then the asymmetric additive convolution of $w_n(t,z)$ and $p(z)$, $w_n(t,z) \aac p(z)$ satisfies the following differential equation

    \begin{equation*}
        c\partial_z [w_n(t,z) \aac p(z)] + cz \partial_{zz}[w_n(t,z) \aac p(z)] + \partial_t [w_n(t,z) \aac p(z)] = 0.
    \end{equation*}
\end{theorem}

\begin{proof}
    We find the derivatives first

    \begin{align*}
        \partial_z[w_n(t,z) \aac p(z)] &= \partial_z \left[ \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}b_{k-j}\right],\\ 
        &= \sum_{k=0}^{n-1} (n-k)z^{n-k-1}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}b_{k-j},\\
        \partial_{zz}[w_n(t,z) \aac p(z)] &= \partial_{zz} \left[ \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}b_{k-j}\right],\\ 
        &= \sum_{k=0}^{n-2} (n-k)(n-k-1)z^{n-k-2}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}b_{k-j},\\
        \partial_t[w_n(t,z) \aac p(z)] &= \partial_t \left[ \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=0}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{(ct_1)^jn!}{(n-j)!}b_{k-j}\right],\\
        &= \sum_{k=0}^n z^{n-k}(-1)^k \sum_{j=1}^k \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{c^j jt_1^{j-1}n!}{(n-j)!}b_{k-j},\\ 
        &= \sum_{k=1}^n z^{n-k}(-1)^k \sum_{j=0}^{k-1} \left(\frac{(n-j-1)!(n-k+j+1)!}{n!(n-k)!}\right)^2 \binom{n}{j+1}\frac{c^{j+1} (j+1)t_1^{j}n!}{(n-j-1)!}b_{k-j-1},\\ 
        &= \sum_{k=0}^{n-1} z^{n-k-1}(-1)^{k+1} \sum_{j=0}^{k} \left(\frac{(n-j-1)!(n-k+j)!}{n!(n-k-1)!}\right)^2 \binom{n}{j+1}\frac{c^{j+1} (j+1)t_1^{j}n!}{(n-j-1)!}b_{k-j},\\ 
        &= \sum_{k=0}^{n-1} (n-k)^2 z^{n-k-1}(-1)^{k+1} \sum_{j=0}^{k} \left(\frac{(n-j)!(n-k+j)!}{n!(n-k)!}\right)^2 \binom{n}{j}\frac{c^{j+1}t_1^{j}n!}{(n-j)!}b_{k-j},\\ 
    \end{align*}

    For the sum we have

    \begin{align*}
        (c&\partial_z + cz\partial_{zz} + \partial_t )[w_n(t,z)\aac p(z)] \\  &= \sum_{k=0}^{n-2} z^{n-k-1}(-1)^k \sum_{j=0}^{k} \left(\frac{(n-j)!(n-k+j)!}{n!(n-k-1)!}\right)^2 \binom{n}{j}\frac{c^{j+1}t_1^{j}n!}{(n-j)!}b_{k-j}\Bigl(n-k + (n-k)(n-k-1) - (n-k)^2\Bigr)\\ 
        &\phantom{espacio}+ (-1)^{n-1}\sum_{j=0}^{n-1} \left( \frac{(j+1)!}{n!} \right)^2\binom{n}{j}\frac{c^{j+1}t_1^{j}n!}{(n-j)!}b_{n-j+1} + (-1)^{n}\sum_{j=0}^{n-1} \left( \frac{(j+1)!}{n!} \right)^2\binom{n}{j}\frac{c^{j+1}t_1^{j}n!}{(n-j)!}b_{n-j+1} = 0.
    \end{align*}
\end{proof}


As defined in \cite{doumerc2005matrices} the generalized singular values problem can be expressed by the following matrix

\begin{equation*}
    W = (\trans{M_1}M_1 + \trans{M_2}M_2)^{-1}\trans{M_1}M_1(\trans{M_1}M_1 + \trans{M_2}M_2)^{-1},
\end{equation*}

\noindent where $M_1, M_2$ are independent Gaussian matrices with dimensions $n_1\times k$ and $n_2 \times k$, respectively.  We can write $W_i = \trans{M_i}M_i$ for a shorter notation. Now to compute the characteristic polynomial we have,

\begin{align*}
    p_W(z) &= \det\left[ zI - (W_1+W_2)^{-\frac12}W_1(W_1+W_2)^{-\frac12} \right], \\ 
    &= \det\left[(W_1+W_2)^{-1}\right]\det\left[ z(W_1+W_2) - (W_1+W_2)^{\frac12}W_1(W_1+W_2)^{-\frac12} \right],\\ 
    &= \det\left[(W_1+W_2)^{-1}\right]\det\left[ z(W_1+W_2)(W_1+W_2)^{\frac12}(W_1+W_2)^{-\frac12} - (W_1+W_2)^{\frac12}W_1(W_1+W_2)^{-\frac12} \right], \\
    &= \det\left[(W_1+W_2)^{-1}\right]\det\left[(W_1+W_2)^{\frac12}\right]\det\left[ z(W_1 + W_2) - W_1\right]\det\left[(W_1+W_2)^{-\frac12}\right],\\ 
    &= \det\left[(W_1+W_2)^{-1}\right]\det\left[ (z-1)W_1 + zW_2 \right].
\end{align*}

Thus finding the expected characteristic polynomial (up to a normalization constant) can be done without the need to use the inverse of $W_1 + W_2$.

In order to find the associated process, we will first generalize the results of the static Wishart and Jacobi matrices with an operator that generalized characteristic polynomial, then we will show that the Wishart process can be built in the base of this operator by adding a time parameter. Finally, we will tray to extend this same procedure to the Jacobi process which, as we will see, is less straightforward. In the process of building this generalizations we will make extensive use of finite free probability.

\begin{definition}[Generalized characteristic polynomial]
    Let $A, B$ be two random independent Gaussian matrices in $\mathcal M_{n_1,k}(\mathbb C)$ and $\mathcal M_{n_2,k}(\mathbb C)$ respectively. We define the generalized characteristic polynomial $p_{A,B}(x,y,z)$ as

    \begin{equation*}
        p_{A,B}(x,y,z) \coloneqq \det\left[ xI + y \trans{A}A + z \trans{B}B \right].
    \end{equation*}

    Similarly, we define the reciprocal generalized reciprocal polynomial as $q_{A,B}(x,y,z) \coloneqq y^{n_1}z^{n_2}P_{A,B}(x,y,z)$.
\end{definition}

    Notice that this generalizes the characteristic polynomial of the (static and dynamical) Wishart and the static Jacobi cases. 

    \begin{theorem} \label{thm:multivariate_operators}
        Let $F$ and $G$ be two-variable polynomials and $A,B \in \mathcal M_{k,n_1}, C,D \in \mathcal M_{k,n_2}$ such that $C$ and $D$ are invariant under product by signed permutation matrices. Suppose that

         \begin{align*}
            \E{q_{A,B}(x,y,z)} &= F(\partial_x \partial_y,\partial_x\partial_z)[x^ky^{n_1}z^{n_2}],\\ 
            \E{q_{C,D}(x,y,z)} &= G(\partial_x \partial_y,\partial_x\partial_z)[x^ky^{n_1}z^{n_2}].
         \end{align*}

         Then $\E{q_{A+C, B+D}(x,y,z)} = F(\partial_x \partial_y,\partial_x\partial_z)G(\partial_x \partial_y,\partial_x\partial_z)[x^ky^{n_1}z^{n_2}]$
    \end{theorem}

    \begin{proof}
        If $F',G'$ are the linear differential operators (multivariate polynomials on $\partial_x\partial_y, \partial_x\partial_z$) generating $p_{A,B}$ and $p_{C,D}$ and they satisfy the former multiplicative property, then it can be extended to $F,G$ by linearity. Thus we prove for $F', G'$. 


        By definition, we have that 
        
        \begin{align*}
            \E{ p_{A, B}(x,y,z) } &= \E{ \det[ x I + y \trans A A + z \trans BB ] } = \E{\chi_{-y\trans AA -z\trans BB}(x)},\\
            \E{ p_{C, D}(x,y,z) } &= \E{ \det[ x I + y \trans CC + z \trans DD ] } = \E{\chi_{-y\trans CC -z\trans DD}(x)}.
        \end{align*}

        Notice that by the results in \todo{citar convolución como polinomiso característicos esperados}

        \begin{align*}
            \E{p_{A+C, B+D}(x,y,z)} &= \E{\chi_{-y\trans AA -z\trans BB}(x)} \boxplus_n \E{\chi_{-y\trans CC -z\trans DD}(x)}.
        \end{align*}

        Applying Theorem \ref{thm:multiplicative_operators} we get to the desired result by considering the derivatives in $y,z$ as constant coefficients when evaluating in $x$.

    \end{proof}

    \begin{theorem} \label{thm:exp_operator}
        Let $\{A_i\}_{i=0}^\infty$ and $\{B_i\}_{i=0}^\infty$ be sequences of matrices invariant under transformation by signed permutation matrices such that

        \begin{align*}
            \E{Tr(\trans A_iA_i)} &= \sigma_1 n_1 k,\\
            \E{Tr(\trans B_iB_i)} &= \sigma_2 n_2 k.
        \end{align*}

        If we define $C_m, D_m$ as

        \begin{align*}
            C_m &\coloneqq \sum_{i=0}^m \frac{A_i}{\sqrt m}, \\
            D_m &\coloneqq \sum_{i=0}^m \frac{D_i}{\sqrt m}.
        \end{align*}

        Then the matrices $C_m, D_m$ satisfy

        \begin{equation*}
            \lim_{m\to\infty} \E{q_{C,D}(x,y,z)} = e^{\sigma_1 \partial_x \partial_y + \sigma_2 \partial_x \partial_z}[x^k y^{n_1} z^{n_2}].
        \end{equation*}
    \end{theorem}

    \begin{proof}
        First we use the following expansion for the determinant,

        \begin{equation*}
            \det( { A} + h { B} ) = \det({ A})  + h \, Tr \left( \mbox{adj} ({A}) \, { B} \right) + O \left(h^2\right).
        \end{equation*}
        \todo{Agregar un apéndice con esto}

        This means for $q_{A_i/\sqrt m, B_i/\sqrt m}(x,y,z)$ that

        \begin{align*}
            \E{q_{A_i/\sqrt m, B_i/\sqrt m}(x,y,z)} &= y^{n_1}z^{n_2}\E{ \det[ xI + \frac{1}{m}\left( y^{-1}\trans A_i A_i + z^{-1}\trans B_i BI \right) ] }, \\ 
            &= y^{n_1}z^{n_2}\left[ x^k + \frac{x^{k-1}y^{-1}}{m}\E{Tr(\trans A_iA_i)} + \frac{x^{k-1}z^{-1}}{m}\E{Tr(\trans B_i B_i)} + O\left(\frac{1}{m^2}\right))\right],\\ 
            &= x^k y^{n_1}z^{n-2} + \frac{\sigma_1 n_1 k}{m}x^{k-1}y^{n_1-1}z^{n-2} + \frac{\sigma_2 n_2 k}{m}x^{k-1}y^{n_1}z^{n_2-1} + O \left(\frac{1}{m^2}\right), \\
            &= \left( 1 + \frac{\sigma_1}{m}\partial_y\partial_x + \frac{\sigma_2}{m}\partial_z\partial_x + O \left(\frac{1}{m^2}\right)\right)[x^k y^{n_1} z^{n_2}]
        \end{align*}

    Once we have found $q_{A_i/\sqrt m, B_i/\sqrt{m}}(x,y,z)$ as a polynomial on the operators $\partial_x \partial_y$ and $\partial_x \partial_z$, we apply Theorem \ref{thm:multivariate_operators} to get, for $q_{C_m,D_m}(x,y,z)$,

    \begin{align*}
        \E{ q_{C_m,D_m}(x,y,z) } &= \left( 1 + \frac{\sigma_1}{m}\partial_y\partial_x + \frac{\sigma_2}{m}\partial_z\partial_x + O \left(\frac{1}{m^2}\right)\right)^m[x^k y^{n_1} z^{n_2}]
    \end{align*}

    Tanking $m\to \infty$ in the last expression we get

    \begin{align*}
        \lim_{m\to\infty} \E{q_{C_m,D_m}(x,y,z)} &= \lim_{m\to\infty} \left( 1 + \frac{\sigma_1}{m}\partial_y\partial_x + \frac{\sigma_2}{m}\partial_z\partial_x + O \left(\frac{1}{m^2}\right)\right)^m[x^k y^{n_1} z^{n_2}],\\
        &= e^{\sigma_1 \partial_x\partial_y + \sigma_2 \partial_x \partial_z}[x^k y^{n_1}z^{n_2}],
    \end{align*}

    \noindent which is the desired result.
    \end{proof}

    \begin{corollary}
        Let $A$ and $B$ be equal in law to $\sigma_1 N_1$ and $\sigma_2 N_2$ where $N_1$ and $N_2$ are matrices with all of the entries being independent standard normal random variables, then
        
        \begin{equation*}
            \E{ q_{A,B}(x,y,z) } = e^{\sigma_1 \partial_x \partial_y + \sigma_2 \partial_x \partial_z}[x^k y^{n_1} z^{n_2}].
        \end{equation*}
        \end{corollary}

        \begin{proof}
            %By Theorem \ref{thm:exp_operator} 
            Let $\{A_i\}_{i\in \N}$ and $\{B_i\}_{i\in \N}$ be as in Theorem \ref{thm:exp_operator} and define $C_m, D_m$ in the same way. On the one hand we have, as shown previously

            \begin{equation*}
                \lim_{m\to \infty} \E{ q_{C_m,D_m}(x,y,z)} =  e^{\sigma_1 \partial_x \partial_y + \sigma_2 \partial_x \partial_z}[x^k y^{n_1} z^{n_2}].
            \end{equation*}

            Due to the existence of the second moment of the entries, on the other hand, we have that $C_m, D_m$ converge in law to a Gaussian independent matrix whose entries have variance $\sigma_1^2$ and $\sigma_2^2$, respectively, i.e.

            \begin{equation*}
                \lim_{m\to \infty} \E{ q_{C_m,D_m}(x,y,z)} = \E{ q_{\sigma_1 N_1, \sigma_2 N_2}(x,y,z)}.
            \end{equation*}        
        \end{proof}

        With this we have an operator that allows us to compute $q_{A,B}(x,y,z)$ for $A,B$ independent Gaussian matrices with zero mean and variances $\sigma_1,\sigma_2$, respectively. Our goal is to be able to use this to study matrix-valued stochastic processes by letting the variances vary linearly in time. Hence, the entries of the matrices have the same law as a standard Brownian motion starting at zero. An interesting question is how this behavior would be affected if we start our processes at points different than zero. The answer is given by Theorem \ref{thm:multivariate_operators} and noticing that any polynomial $r(x,y,z)$ of orders $k,n_1,n_2$ can be seen as a polynomial differential operator acting on $x^ky^{n_1}y^{n_2}$, so if $S\in \M_{k,n_1}(\C),T\in \M_{k,n_2}(\C)$ are fixed matrices and $A, B$ are Gaussian independent matrices with variances $\sigma_1$ and $\sigma_2$, respectively, then

        \begin{equation*}
            \E{q_{S+A,T+B}(x,y,z)} = e^{\sigma_1 \partial_x \partial_y + \sigma_2 \partial_x \partial_z}[q_{S,T}(x,y,z)].
        \end{equation*}
        
        
        
        We are interested in $p_{A,B}(x,y,)$, the reciprocal polynomial in $y$ and $z$.  Let us denote by $R^z_{k}(\cdot)$ the reciprocal polynomial operator of order $k$ in the variable $z$, which means, for $r(x,y,z)$ a polynomial,

        \begin{equation*}
            R^z_k \left(r(x,y,z)\right) = z^kr\left(x,y,\frac1z\right).
        \end{equation*}

        Now notice that for $A,B$ Gaussian independent matrices with variances $\sigma_1,\sigma_2$ and given matrices $S,T$ we have $q_{S+A,T+B}(x,y,z)= R^z_{n_2}(R^y_{n_1}(p_{S+A,T+B}(x,y,z))$, so

        \begin{align*}
            \E{p_{A,B}(x,y,z)} &= R^z_{n_2}\left( R^y_{n_1}\left(\E{ q_{S+A,T+B}(x,y,z) } \right) \right), \\
            &= R^z_{n_2}\left( R^y_{n_1}\left(e^{\sigma_1 \partial_x\partial_y + \sigma_2 \partial_x\partial_z} q_{S,T}(x,y,z) \right) \right), \\
            &= R^z_{n_2}\left( R^y_{n_1}\left(e^{\sigma_1 \partial_x\partial_y + \sigma_2 \partial_x\partial_z} R^z_{n_2}(R^y_{n_1}(p_{S,T}(x,y,z) \right) \right),\\ 
            &= \left( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_1\partial_x\partial_y + \sigma_2 \partial_x\partial_y} \circ R^z_{n_2}\circ R^{y}_{n_1} \right)[p_{S,T}(x,y,z)].
        \end{align*}

    We will find an expression for this operator applied to $x^iy^jz^l$ with $j\le n_1, l \le n_2$. Then it is extended linearly to a general polynomial.

    \begin{align*}
        ( R^z_{n_2}\circ R^{y}_{n_1}&\circ e^{\sigma_1\partial_x\partial_y + \sigma_2 \partial_x\partial_y} \circ R^z_{n_2}\circ R^{y}_{n_1})[x^i y^j z^l]\\ 
        &=( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_1\partial_x\partial_y + \sigma_2 \partial_x\partial_y})[x^i y^{n_1-j}z^{n_2-l}],\\ 
        &= ( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_2\partial_x\partial_z}) \sum_{r=0}^\infty \frac{\sigma_1^r}{r!} \partial_x^r \partial_y^r [x^i y^{n_1-j}z^{n_2-l}], \\
        &= ( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_2\partial_x\partial_z}) \sum_{r=0}^\infty \frac{\sigma_1^r}{r!} \partial^r_x [x^i] \frac{(n_1-j)!}{(n_1 - j - r)!}y^{n_1-j-r}z^{n_2-l},\\ 
        &= ( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_2\partial_x\partial_z})z^{-l}\sum_{r=0}^\infty \binom{n_1-j}{r}y^{n_1 - j -r} \sigma_1^r\partial_x^r[x^i],
\intertext{doing the same procedure for $e^{\sigma_2\partial_x\partial_z}$ we get }
        &= ( R^z_{n_2}\circ R^{y}_{n_1}\circ e^{\sigma_2\partial_x\partial_z})z^{n_2-l}\left(y + \sigma_1\partial_x\right)^{n_1 - j}[x^i],\\
        &= R^z_{n_2}\circ R^{y}_{n_1}\left\{ (z+\sigma_2\partial_x)^{n_2-l} \left(y + \sigma_1\partial_x\right)^{n_1 - j}[x^i]\right\},\\
        &= (1+z\sigma_2\partial_x)^{n_2-l} \left(1 + y\sigma_1\partial_x\right)^{n_1 - j}[x^i y^j z^l].
    \end{align*}


    % With this, we can conclude that 

    % \begin{equation*}
    %     \E{p_{S+A,T+B}} = (1+z\sigma_2\partial_x)^{n_2-l} \left(1 + y\sigma_1\partial_x\right)^{n_1 - j}[p_{S,T}(x,y,z)].
    % \end{equation*}

    In the case when we start the processes at the origin we have that 

    \begin{equation*}
        p_{S,T}(x,y,z) = \det[ xI + y 0_{n_1,k} + z 0_{n_2,k}] =  x^k \det[I] = x^k,
    \end{equation*}

    \noindent which leads to

    \begin{equation} \label{eq:generalized_operator}
        \E{p_{A,B}(x,y,z)} = (1+z\sigma_2\partial_x)^{n_2-l} \left(1 + y\sigma_1\partial_x\right)^{n_1 - j}[x^k].
    \end{equation}

    If we let $\sigma_1 = \sigma_2 = 1$ and replace $y=-1, z=0$ in \eqref{eq:generalized_operator}, we recover the static Wishart matrix and we can conclude that

    \begin{equation} \label{eq:wishart_finite_free}
        \E{\det[xI - \trans{A}A]} = (1 - \partial_x)^{n_1}[x^k],
    \end{equation}

    \noindent which is exactly the associated Laguerre polynomial previously found in \todo{cita}.

    Keeping the unitary variances and replacing $x=0, y=(x-1), z=x$ we get the static Jacobi matrix, as shown previously, thus 

    \begin{equation*}
        \E{\det[(x-1)W_1 + xW_2]} = (1+x\partial_x)^{n_2} \left(1 + (x-1)\partial_x\right)^{n_1}[x^k] = P_k^{n_2 - k, n_1-k}(2x-1).
    \end{equation*}

    This is the Jacobi polynomial of order $k$ with parameters $n_2 - k$ and $n_1-k$ evaluated in $2x-1$. When we normalize to make it monic we get the result appearing in \cite{edelman1988eigenvalues}.

    Now let $\sigma_1= \sigma_2 = t$, then $A,B$ are equal in law to standard Brownian motions in $\M_{n_1,k}(\R)$ and $\M_{n_2,k}(\R)$, respectively. We should recover the expected characteristic polynomial of the Wishart and Jacobi processes. The Wishart case is evident by replacing $-1$ with $-t$ in \eqref{eq:wishart_finite_free} and compare it to equation\todo{referencia}. For the Jacobi process, we have

    \begin{equation} \label{eq:dynamic_jacobi}
        \E{\det[(x-1)W_1(t) + xW_2(t)]} = (1+xt\partial_x)^{n_2} \left(1 + (x-1)t\partial_x\right)^{n_1}[x^k].
    \end{equation}

    In analogy to what we found for the Dyson Brownian motion and the Wishart process, we need to verify if the roots of the polynomial defined in \eqref{eq:dynamic_jacobi} satisfy a differential equation given by the finite variation part of \eqref{eq:stochastic_jacobi}\todo{Agregar cita}. Let us work in full generality and define for polynomials of the form $x^i y^j z^l$, the operator

    \begin{equation*}
        Q_{n_1,n_2}^t [x^i y^j z^l] = (1+zt\partial_x)^{n_2-l} \left(1 + yt\partial_x\right)^{n_1 - j}[x^i y^j z^l],
    \end{equation*}

    \noindent that is later extended to general polynomials linearly. 

    Denote the time-dependant polynomial $Q_{n_1,n_2}^t[p(x,y,z)]$ by $\hat p(x,y,z,t)$. In analogy to what we have done with the Hernmite and Laguerre cases, we want to find for every $t$ the values $x(t), y(t), z(t)$ such that $\hat p(x(t),y(t),z(t),t)=0$. By derivating with respect to $t$, we get

    \begin{align*}
       0 &= \partial_t [\hat p(x(t),y(t),z(t),t)]\\ &= \left[(\partial_t x(t))\partial_x \hat p(x,y,z,t) + (\partial_t y(t))\partial_y  \hat p(x,y,z,t) + (\partial_t z(t))\partial_z \hat p(x,y,z,t) + \partial_t \hat p(x,y,z,t)\right]_{(x,y,z)=(x(t),y(t),z(t))}.
    \end{align*}

    \begin{lemma}
        The time-dependant polynomial $\hat p(x,y,z,t)$ satisfies

        \begin{equation} \label{eq:general_harmonic_roots}
            \partial_t[\hat p(x,y,z,t)]_{t=0} = (n_1y + n_2z)\partial_x[\hat p(x,y,z,t)]_{t=0} - y^2\partial_y\partial_x[\hat p(x,y,z,t)]_{t=0} - z^2\partial_z\partial_x[\hat p(x,y,z,t)]_{t=0}.
        \end{equation}
    \end{lemma}

    \begin{proof}
        A general polynomial $p$ on $x,y,z$, can be written as 

        \begin{equation*}
            p(x,y,z) = \sum_{i,j,l} c_{ijl}x^i y^j z^l,
        \end{equation*}

        \noindent where $c_{ijl}$ is the coefficiente associated to the term $x^i y^j z^l$. When we apply $Q_{n_1,n_2}^t$ to $p(x,y,z)$ we have

        \begin{align*}
            Q_{n_1,n_2}^t [p(x,y,z)] &= (1+zt\partial_x)^{n_2-l} \left(1 + yt\partial_x\right)^{n_1 - j}\left[\sum_{i,j,l}c_{ijl}x^i y^j z^l\right],\\
            &= \sum_{i,j,l}c_{ijl}(1+zt\partial_x)^{n_2-l} \left(1 + yt\partial_x\right)^{n_1 - j}\left[x^i y^j z^l\right]\\
            &= \sum_{i,j,l}c_{ijl}\sum_{r=0}^{n_2-l}\binom{n_2-l}{r}(zt\partial_x)^r\sum_{s=0}^{n_1-j}\binom{n_1-j}{s}(yt\partial_x)^{n_1-j}[x^iy^jk^l],\\
            &= \sum_{i,j,l} c_{ijl}\left\{ (n_2-l)zt\partial_x + (n_1-j)yt\partial_x + O(t^2) \right\}[x^iy^jk^l],\\ 
            &= \sum_{i,j,l} c_{ijl}\left\{ (n_2-l)itx^{i-1}y^j z^{l+1} + (n_1-j)itx^{i-1}y^{j+1}z^l \right\} + O(t^2).
        \end{align*}

        Then, differentiating in $t$ and evaluating at $t=0$ gives us

        \begin{align*}
            \left.\partial_t \left\{ Q_{n_1,n_2}^t [p(x,y,z)] \right\}\right|_{t=0} &= \left.\partial_t \left\{ \sum_{i,j,l} c_{ijl}\left\{ (n_2-l)itx^{i-1}y^j z^{l+1} + (n_1-j)itx^{i-1}y^{j+1}z^l \right\} + O(t^2) \right\}\right|_{t=0}, \\
            &= \sum_{ijl} c_{ijl}\{ (n_2-l)ix^{i-1}y^jz^{l+1} + (n_1-j)ix^{i-1}y^{j+1}z^l\}.
        \end{align*}

        We can re-write the last two terms in every summand as

        \begin{align*}
            (n_2-l)ix^{i-1}y^jz^{l+1} &= n_2z\partial_x[x^iy^jz^l] - z^2\partial_x\partial_z[x^iy^jz^l],\\
            (n_1-j)ix^{i-1}y^{j+1}z^{l} &= n_1y\partial_x[x^iy^jz^l] - y^2\partial_x\partial_y[x^iy^jz^l].
        \end{align*}

        Extending by linearity to $p$ we have that

        \begin{equation*}
            \partial_t[\hat p]_{t=0} = (n_1y + n_2z)\partial_x[p] - y^2\partial_y\partial_x[p] - z^2\partial_z\partial_x[p].
        \end{equation*}

        This finishes the proof since $p(x,y,z) = \hat p(x,y,z,t)|_{t=0}$.
    \end{proof}

    This last result together with \eqref{eq:general_harmonic_roots} give us that

    \begin{align*}
        \partial_t[x(t)]\partial_x[p(x(t),y(t),z(t))] &+ \partial_t[y(t)]\partial_y[p(x(t),y(t),z(t))] + \partial_t[z(t)]\partial_z[p(x(t),y(t),z(t)) \\
        &= - (n_1y + n_2z)\partial_x[p] + y^2\partial_y\partial_x[p] + z^2\partial_z\partial_x[p].
    \end{align*}

    \begin{lemma}
        Let $r(x,y,z)$ be a homogeneous polynomial of order $k$ in $x,y,z$, then

         \begin{align*}
            y^2\partial_y\partial_x r + z^\partial_z\partial_x r &= (y+z)(k-1)\partial_x r - yz (\partial_z\partial_x r + \partial_y\partial_x r) - (y+z)x\partial_{xx} r.
        \end{align*}
    \end{lemma}

    \begin{proof}
        The fact that $r$ is homogeneous with degree $k$ implies that $\partial_x r$ is homogeneus with degree $k-1$, then

        \begin{align*}
            z\partial_z\partial_x r + y \partial_y\partial_x r + x \partial_{xx}r = (k-1)\partial_x r.
        \end{align*}

        The last equality implies

        \begin{align*}
            y^2 \partial_y \partial_x r &= y(k-1)\partial_x r - yz\partial_z\partial_x r - yx\partial_{xx}r,\\
            z^2 \partial_z \partial_x r &= z(k-1)\partial_x r - zy\partial_y\partial_x r - zx\partial_{xx}r.
        \end{align*}

        And summing up these two expressions leads to

        \begin{align*}
            y^2\partial_y\partial_x r + z^2\partial_z\partial_x r &= (y+z)(k-1)\partial_x r - yz (\partial_z\partial_x r + \partial_y\partial_x r) - (y+z)x\partial_{xx} r.
        \end{align*}
    \end{proof}

    Then we have for $\hat p$

    \begin{align*}
        (\partial_t x)(\partial_x \hat p) &+ (\partial_t y)(\partial_y \hat p) + (\partial_t z)(\partial_z \hat p)|_{t=0} \\ 
        &= -(n_1y + n_2 z)\partial_x \hat p + (y+z)(k-1)\partial_x \hat p - yz (\partial_z\partial_x \hat p + \partial_y\partial_x \hat p) - (y+z)x\partial_{xx} \hat p|_{t=0}.
    \end{align*}

    Now we can substitute $y = w, z = (w-1)$ and define the polynomial $j(x,w,t)$ as

    \begin{equation*}
        j(x,w,t) = \hat p(x(t), w(t), w(t)-1, t).
    \end{equation*}

    Using the above results, we have for $j(w,t)$

    \begin{align*}
        (\partial_t x)(\partial_x j) &+ (\partial_t w)(\partial_w j)|_{t=0} \\
        &= -(n_1w + n_2w-n_2))\partial_x j + (2w-1)(k-1)\partial_x j - w(w-1)(\partial_x\partial_w j) - (2w-1)x\partial_{xx} j|_{t=0}.
    \end{align*}

    Letting $x=0$, we find the evolution equation for the matrix Jacobi process \todo{Agregar referencia}

    \begin{align*}
        \left.(\partial_t w)\frac{\partial_w j}{\partial_x j}\right|_{t=0} &= \left.-(n_1w + n_2w) + (2w-1)(k-1) - w(w-1)\frac{\partial_x\partial_w j}{\partial_x j}\right|_{t=0},\\
        \Rightarrow \partial_t w &= -[n_1w + n_2 (w-1) +(2w-1)(k-1)]\frac{\partial_x j}{\partial_w j} - w(w-1)\frac{\partial_x\partial_w j}{\partial_w j},\\ 
        &= [-(n_1-k+1)w - (n_2 -k+1)(w-1)]\frac{\partial_x j}{\partial_w j} - w(w-1)\frac{\partial_x\partial_w j}{\partial_w j}.
    \end{align*}

    To further simplify the last expression we need some hypothesis about the relationship between $\partial_x j$ and $\partial_w j$. We recover this from the definition as the generalized characteristic polynomial.

    \begin{align*}
        j(x,w,t) &= \E{ \det[ xI + (u-1)\trans A_1(t) A_1(t) + u \trans A_2(t) A_2(t) ] } = \E{ \det[ xI + (u-1)\trans A_1(t) A_1(t) + u (I - \trans A_1(t) A_1(t)) ] }, \\
        &= \E{ \det[ (x+u)I - \trans A_1(t) A_1(t) +(u-u)\trans A_1(t) A_1(t) ] } = \E{ \det[ (x+u)I - \trans A_1(t) A_1(t)] }.
    \end{align*}

    So this means that the derivatives of $j$ with respect to $x$ are the same as its derivatives with respect to $w$ and then we can equate $\partial_w j$ to $\partial_x j$ to get

    \begin{align*}
        \partial_t w_i &= -(n_1-k+1)w_i - (n_2 -k+1)(w_i-1) - w_i(w_i-1)\frac{\partial_{xx} j}{\partial_x j},\\ 
        &=-(n_1-k+1)w_i - (n_2 -k+1)(w_i-1) - w_i(w_i-1)\sum_{j \neq i} \frac{2}{w_i - w_j},\\ 
        &= n_2 - (n_1 + n_2)w_i + (w_i + w_i -1)(k-1) - \sum_{j\neq i} \frac{2w_i(w_i -1)}{w_i - w_j},\\
        &= n_2 - (n_1 + n_2)w_i + (w_i + w_i -1)\sum_{j \neq i}\frac{w_i - w_j}{w_i - w_j} - \sum_{j\neq i} \frac{2w_i(w_i -1)}{w_i - w_j},\\ 
        &= n_2 - (n_1 + n_2)w_i + \sum_{j \neq i}\frac{ (w_i + w_i -1)(w_i - w_j) - 2 w_i(w_i-1) }{w_i - w_j},\\ 
        &= n_2 - (n_1 + n_2)w_i + \sum_{j \neq i}\frac{ w_i^2 + w_i(w_i-1) - w_iw_j -w_j(w_i-1) -2w_i(w_i-1) }{w_i - w_j},\\
        &=  n_2 - (n_1 + n_2)w_i + \sum_{j \neq i}\frac{ - w_iw_j - w_j(w_i-1) +w_i }{w_i - w_j},\\
        &= n_2 - (n_1 + n_2)w_i + \sum_{j \neq i}\frac{w_j(1-w_i) +w_i(1-w_j)}{w_i - w_j}.
    \end{align*}
    %Notice that by definition, $p(x,y,z)$ is a homogeneous polynomial, and this allows us to 
    
    % Poner casos estáticos junto al Hermite
    % Determinar dinámica de las raíces
    % Escribir que la distancia más pequeña crece (por lo menos en el caso Hermite)

