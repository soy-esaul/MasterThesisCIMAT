\todo[inline]{Esta parte de la tesis será la última en trabajarse. Aunque es un área relativamente nueva, los resultados que se usarán están bien documentados, por lo que se espera que se pueda trabajar más rápidamente.}

\subsection{Convolution of polynomials}

\subsubsection{Symmetric additive convolution}

\begin{definition}[Symmetric additive convolution]\label{def:symadconv}
    Let $p(x), q(x)$ be two complex polynomials of $x$, with degree less or equal to $d$,

    \begin{align*}
        p(x) &= \sum_{j=0}^d x^{d-j}(-1)^{j}a_j,\\
        q(x) &= \sum_{j=0}^d x^{d-j}(-1)^{j}b_j.
    \end{align*}

    The $d$th symmetric additive convolution of $p$ and $q$ is

    \begin{align*}
        p(x) \boxplus_d q(x) &\coloneqq \sum_{k=0}^d x^{d-k}(-1)^k \sum_{i+j = k} \frac{(d-i)!(d-j)!}{d!(d-k)!}a_i b_j, \\
        &= \frac{1}{d!}\sum_{k=0}^d D^k p(x)D^{d-k}q(0),\\
        &= \frac{1}{d!}\sum_{k=0}^d D^k q(x)D^{d-k}p(0),
    \end{align*}

    \noindent with $D$ denoting the differentiation with respect to $x$.
\end{definition}



\subsubsection{Symmetric multiplicative convolution}

\begin{definition}[Symmetric multiplicative convolutions]
    Let $p$ and $q$ be as in Definition \ref{def:symadconv} with degree at most $d$, the $d$th symmetric multiplicative convolution of $p$ and $q$ is 

    \begin{align*}
        p(x) \boxtimes_d q(x) \coloneqq \sum_{i=0}^d x^{d-i}(-1)^i\frac{a_ib_i}{\binom di}.
    \end{align*}
\end{definition}


\subsubsection{Linearization of convolutions}

\todo[inline]{Tomarlo de la tesis de Daniel Perales}


\subsection{Finite free convolutions and random matrices}

\begin{theorem}
    Let $R$ be a $d\times d$ matrix and $R_{ij}$ be independent Gaussian random variables with mean 0 and variance 1, then

    \begin{equation*}
        E\left[ \chi_x\left( \frac{R + R^T}{\sqrt 2} \right) \right] = H_d(x),
    \end{equation*}

    \noindent where $H_d(x) = e^{D^2/2}x^d$ is the $d$th Hermite polynomial.
\end{theorem}

\begin{proof}
    
\end{proof}


\begin{theorem}
    Let $R$ be a $d\times d$ matrix and $R_{ij}$ be independent Gaussian random variables with mean 0 and variance 1, then

    \begin{equation*}
        E\left[ \chi_x\left(R\trans{R}\right) \right] = L_d(x),
    \end{equation*}

    \noindent where $L_d(x) = \left( 1 - \frac{\d}{\d x} \right)^d x^d$ is the $d$th Laguerre polynomial.
\end{theorem}

\begin{proof}
    
\end{proof}


\subsubsection{Minor orthogonality}

\begin{theorem}[Cauchy-Binet formula] \todo[inline]{Tal vez debería mover este a preliminares}
    Let $m,n,p,k$ be integers, $A$ an $m\times n$ matrix, and $B$ a $n\times p$ matrix, then

    \begin{equation*}
        [AB]_{S,T} = \sum_{|U|\subset \binom{[n]}{k}} [A]_{S,U} [B]_{U,T},
    \end{equation*}

    \todo[inline]{Hay que ver si voy a dejar la misma notación para el determinante o cambiarla.}

    \noindent where $S\in \binom{[m]}{k}, T \in \binom{[p]}{k}$.
\end{theorem}


\begin{definition}[Minor orthogonality]
    Let $R$ be an $m \times n$ random matrix. We say $R$ is minor orthogonal if for every $k,l \in \mathbb Z$ such that $k,l \le \max\{m.n\}$ and all sets $S,T,U,V$ with $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{V} = l$, it satisfies
    
    \begin{equation*}
        E_R\left[ [R]_{S,T} [R^*]_{U,V} \right] = \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{equation*}
\end{definition}

\begin{lemma} \label{lemma:orth_trans_is_minorth}
    If $R$ is minor orthogonal and $Q$ is a constant matrix such that $Q\hermit{Q} = I$, then $Q$ is minor orthogonal. If $\hermit{Q}Q = I$, then $RQ$ is minor orthogonal.
\end{lemma}

\begin{proof}
    Recall that by the Cauchy-Binet formula, for $|S|=|T| = k$ we have

    \begin{equation*}
        [QR]_{S,T} = \sum_{\abs{W} = k} [Q]_{S,W}[R]_{W,T},
    \end{equation*}

    \noindent so with $\abs{S} = \abs{T} = k, \abs{U}=\abs{V} = l$,

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= E_R \left[  \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[R]_{W,T} [\hermit{R}]_{U,Z}[\hermit{Q}]_{Z,V} \right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \left[ [R]_{W,T}[\hermit{R}]_{U,Z}\right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \frac{\delta_{W,Z}\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= \sum_{\abs{W}=k} [Q]_{S,W}[\hermit{Q}]_{W,V}\frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}}
        = [Q\hermit{Q}]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= [I]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},
        \intertext{Notice that $[I]_{S,V} = 1$ if and only if $S=V$, so we conclude that}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{align*}

    The case $\hermit{Q}Q =I$ is proven in the same way.
\end{proof}

\begin{definition}[Signed permutation matrix]
    A signed permutation matrix is a matrix that can be written $EP$ where $E$ is a diagonal matrix with entries $\pm 1$ and $P$ is a permutation matrix.
\end{definition}

\begin{lemma} \label{lemma:singed_per_is_minorth}
    A random matrix sampled uniformly from the set of signed permutation matrices is minor-orthogonal.
\end{lemma}

\begin{proof}
    Let $Q$ be a signed permutation matrix, we can write $Q = E P$, where $E$ is a diagonal random matrix with entries $\pm 1$ taken uniformly and $P$ is a matrix chosen uniformly from the permutation matrices, and both are independent. Then for $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{U} = l$,

    \begin{align*}
        E_Q\left[ [Q]_{S,T}[\hermit{Q}]_{U,V} \right] &= E_{E,P}\left[ [E P]_{S,T}[\hermit{P}E]_{U,V} \right],\\
        &= \sum_{\abs{W} = k} \sum_{\abs{Z} = l} E_{E,P} \left[ [E]_{S,W} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{Z,V} \right],
        \intertext{every $[E]_{S,W}$ is diagonal and the determinant would be zero if $S\neq W$, so}
        &= E_{E,P} \left[ [E]_{S,S} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{V,V} \right],\\
        \intertext{Let $\left\{\chi_i\right\}_{1\le i \le n}$ be the diagonal entries of $E$, then}
        &= E_{E}\left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right].
    \end{align*}

    Now we use that the variables $chi_i$ are independent and uniform in $\{-1,1\}$, so that $E[\chi_i] = 0$, but $E[\chi_i^2] = 1$ for all $i$, and this means 

    \begin{equation*}
        E_E \left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] = \delta_{S,V}.
    \end{equation*}

    This last equality leads to

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right],\\
        &=  \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,S} \right],\\ 
        &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[P]_{S,U} \right].
    \end{align*}

    The submatrix $P_{S,T}$ can be transformed in a diagonal matrix by a permutation matrix because it has at most a non zero entry for each row and each column. If the diagonal matrix has a zero entry in the diagonal, then the determinant $[P]_{S,T}$ is zero, in other case, it is different that zero. The only case when all of the diagonal entries of the diagonal matrix are not zero is when $T = \pi(S)$ with $\pi$ the permutation function corresponding to $P$. This means that in order to have a non-zero determinant we need $T = \pi(S) = U$, and $[P]_{S,U} = \in \{-1,1\}$, so

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}[P]_{S,T} \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}^2 \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  \delta_{T=\pi(S)} \right], \\ 
        &= \delta_{S,V} \delta_{T,U} \P{T = \pi(S)}.
    \end{align*}

    We are supposing that we are sampling uniformly from the permutation matrices of size $n \times n$, so the probability that $T = \pi(S)$ when $\pi$ is a permutation of $n$ elements and $\abs{S} = \abs{T} = k$ is $1/\binom{n}{k}$. So, we can conclude

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V} \delta_{T,U}}{\binom{n}{k}}.
    \end{align*}

    This is the definition of being minor-orthogonal. \todo[inline]{En el paper no lo prueban, pero el resultado se tiene también para una matriz de permutación con signos rectangular. En la siguiente prueba se usa esto, así que falta completar eso. La prueba es igual, simplemente tomando que $E$ es de tamaño $m\times m$ y $P$ de tamaño $m\times n$, todos los resultados se siguen.}

\end{proof}


\begin{corollary}
    An $m\times n$ random matrix sampled from the Haar measure on $\mathbb C_{n}^m$ is minor-orthogonal.
\end{corollary}

\begin{proof}
    Let $R$ be a Haar distributed random $m\times n$ matrix with $m \le n$ and $Q$ a random permutation matrix. Any random permutation matrix is unitary, so $RQ$ is Haar distributed for fixed $Q$, and by Lemmas \ref{lemma:lemma:orth_trans_is_minorth} and \ref{lemma:singed_per_is_minorth} we have that it is also minor-orthogonal. Then, if $Q$ is uniformly sampled from the signed permutation matrices,

    \begin{align*}
        E_R\left[ [R]_{S,T}[\hermit{R}]_{U,V} \right] = E_{R,Q}\left[ [RQ]_{S,T} [\hermit{(RQ)}]_{U,V}\right]. 
    \end{align*}

    Since $Q$ is minor orthogonal, $RQ$ is also minor orthogonal for fixed $R$ and 

    \begin{align*}
        E_R\left[ E_Q \left[ [RQ]_{S,T}[\hermit{(RQ)}]_{U,V} \right] \right] = E_R\left[ \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}}\right] = \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}},
    \end{align*}

    \noindent where $k = \abs{S} = \abs{T}$. 
\end{proof}


% Formulas

\begin{theorem} \label{thm:marcus_binet}
    Let $k,n$ be integers such that $k\le n$, $A,B$ two $n\times n$ matrices, and $S,T \in \binom{[n]}{k}$. Then
    
    \begin{equation*}
        [A+B]_{S,T} = \sum_{i=0}^k \sum_{V \in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} [A]_{U(S),V(T)}[B]_{\bar{U}(S),\bar{V}(T)},
    \end{equation*}

    \noindent with $\bar U = [k] \setminus U$.
\end{theorem}

\todo[inline]{Es un hecho conocido, si da tiempo lo pruebo.}

We denote by $\sigma_k(A)$ the coeficient of of $(-1)^{k}x^{d-k}$ in the characteristic polynomial of a $d$-dimensional matrix $A$. We will use the fact that \todo{Tal vez esto debería ir en preliminares}

\begin{equation*}
    \sigma_k(A) = \sum_{\abs{S} = k} [A]_{S,S}.
\end{equation*}

\begin{lemma} \label{lemma:conjugate_minorth}
    Let $m \le n$, $B$ an $n\times n$ random matrix and $R$ an $m\times n$ minor-orthogonal matrix independent from $B$. For all sets $S,T \subset \binom{[m]}{k}$ we have

    \begin{equation*}
        E_{B,R} \left[ [RB\hermit R]_{S,T} \right] = E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Using the Cauchy-Binet formula we have

    \begin{align*}
        E_{B,R} \left[ [RB\hermit{R}]_{S,T} \right] &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} E_R \left[ [R]_{S,X} [B]_{X,Y} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} E_R \left[ [R]_{S,X} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &=  E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} \frac{\delta_{S,T} \delta_{X,Y}}{\binom{n}{k}} \right],\\ 
        &= E_B \left[\sum_{X \in \binom{[n]}{k}} [B]_{X,X} \frac{\delta_{S,T}}{\binom{n}{k}} \right],\\
        &= E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{align*}
\end{proof}


\begin{lemma}
    Let $a > d$, $A$ an $a \times a$ random matrix and $Q$ a random $a \times d$ matrix sampled from the Haar measure on $\mathbb C_a^d$, then

    \begin{equation*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] = E_A \left[\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} \chi_x(Q) \right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $A$ be a fixed matrix and $Q$ a Haar unitary matrix on $\mathbb C_{a}^d$, the $k$th coefficient of the expected characterictic polynomial of $QA\hermit Q$ is 

    \begin{align*}
        E_Q \left[ \sigma_k(QA\hermit{Q}) \right] &= \sum_{\abs{S} = k} E_Q\left[ [QA\hermit Q]_{S,S} \right],\\ 
        &= \sum_{\abs{S} = k} \frac{\sigma_k(A)}{\binom ak},\\ 
        &= \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}}.
    \end{align*}

    Taking expectaton on the last expression we find

    \begin{align*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] &= E_A \left[ \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}} \right] = \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}},
    \end{align*}

    \noindent which is the $k$th coefficient of $\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} E_A \left[\chi_x(A) \right]$.
\end{proof}


\begin{theorem} \label{thm:implies_symmad}
    Let $A, B$ be $d\times d$ random matrices and $R$ a $d\times d$ minor-orthogonal matrix, such that $A, B, R$ are jointly independent, then we have

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}
\end{theorem}

\begin{proof}
    We use 

    \begin{equation*}
        \sigma_k(A) = \sum_{\abs{S}=k} [A]_{S,S},
    \end{equation*}

    \noindent together with Theorem \ref{thm:marcus_binet} and Lemma \ref{lemma:conjugate_minorth} to get \todo{Hay que aclarar las normas de matrices.}

    \begin{align*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit{R}) \right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[ [A + RB\hermit{R}]_{S,S} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] E_{B,R}\left[ [RB\hermit R]_{\overline{U}(S),\overline{V}(S)} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] \delta_{\overline{U}(S),\overline{V}(S)} \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}},\\ 
        \intertext{using that $U(S)=V(S)$ if and only if $\overline{U}(S) = \overline{V}(S)$,}
        &= \sum_{i=0}^k \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}} \sum_{S\in\binom{[d]}{k}} \sum_{U,V\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right].
    \end{align*}

    To finish the proof we need to find

    \begin{equation} \label{eq:suma_rara} \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right]. \end{equation}

    Clearly, we are summing over all of the sets $V \in \binom{[d]}{i}$, but they appear more than once in the sum. To find the number of times every element $V\in \binom{[d]}{i}$ appears in the sum, we can count the total number of terms we are summing in \eqref{eq:suma_rara} and divide by the total number of elements in $\binom{[d]}{i}$. We have that $\abs{\binom{[d]}i} = \binom{d}{i}$ and the number of summands is $\binom{d}{k}\binom{k}{i}$, so

    \begin{align*}
        \frac{\binom{d}{k}\binom{k}{i}}{\binom{d}{i}} &= \frac{\frac{d!}{k!(d-k)!}\frac{k!}{i!(k-i)!}}{\frac{d!}{i!(d-i)!}} = \frac{(d-i)!}{(d-k)!(k-i)!} = \binom{d-i}{k-i}.
    \end{align*}

    So, we have

    \begin{equation*}
        \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right] = \binom{d-i}{k-i} \sum_{V\in \binom{[d]}{i}}  E_A\left[ [A]_{V,V} \right] = \binom{d-i}{k-i}E_A\left[\sigma_{i}(A)\right].
    \end{equation*}

    Thus we can conclude

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}

    \todo{Hay que aclarar qué signofica $U(S)$.}
\end{proof}


\begin{theorem} \label{thm:symmad}
    If $p(x)$ is the characteristic polynomial of $A$ and $q(x)$ is the characteristic polynomial of $B$, where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxplus_d q(x) = E_Q\left[\chi_x(A + Q B Q^*)\right],
    \end{equation*}

    \noindent where $\chi_x(\cdot)$ denotes the characteristic polynomial of $\cdot$ with $x$ as a variable and $E_Q$ denotes taking expectation over $Q$ where $Q$ is sampled from the Haar measure on the unitary complex $d\times d$ matrices.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symmad} and definition of the symmetric additive convolution.
\end{proof}


\begin{theorem} \label{thm:implies_symm_mult}
    Let $A$ and $B$ be $d\times d$ random matrices and $R$ a minor-orthogonal $d\times d$ matrix, such that $A,B,R$ are jointly independent, then

    \begin{equation*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] = \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{equation*}
\end{theorem}

\begin{proof}
    \begin{align*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[[ARB\hermit{R}]_{S,S}\right],\\ 
        \intertext{By the Cauchy-Binet formula and independence}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[ [A]_{S,T} \right] E_{B,R} \left[ [RB\hermit{R}]_{T,S} \right],\\ 
        \intertext{By Lemma \ref{lemma:conjugate_minorth}}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[[A]_{S,T} \right] \delta_{T,S}\frac{E_B\left[ \sigma_k(B)\right]}{\binom{d}{k}},\\
        &= \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{align*}
\end{proof}


\begin{theorem}
    Let $p(x)$ be the characteristic polynomial of $A$ and $q(x)$ be the characteristic polynomial of $B$ where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxtimes_d q(x) = E_Q \left[ \chi_x (AQBQ^*) \right],
    \end{equation*}

    \noindent with $\chi_x$ and $E_Q$ as in Theorem \ref{thm:symmad}.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symm_mult} and definition of the symmetric multiplicative convolution.
\end{proof}