\section{Generalization for matrix-valued diffusion processes} \label{sec:matrix_difusions}

\begin{theorem} \label{thm:diffusion_real}
    Let $B = (B(t), t\ge 0)$ be a Brownian motion in $\M_{p,p}(\R)$ and $X(t)$ be a symmetric $p\times p$ matrix-valued stochastic process satisfying the stochastic differential equation

    \begin{equation}
        \d X(t) = g(X(t)) \d B(t) h(X(t)) + h(X(t)) \d \trans{B(t)} g(X(t)) + b(X(t))\d t, \label{eq:matrix_diffusion}
    \end{equation}

    where $g,h,b$ are real functions acting spectrally, and $X(0)$ is a symmetric $p\times p$ matrix with $p$ different eigenvalues. 

    Let $G(x,y) = g^2(x)h^2(y) + g^2(y)h^2(x)$, and
    
    \begin{equation}
        \tau = \inf\{ t: \lambda_i(t) = \lambda_j(t) \text{ for some } i\neq j \}. \label{eq:collision_time}
    \end{equation} 
    
    Then, for $t < \tau$ the eigenvalue process $\Lambda(t)$ verifies the following stochastic differential equations:

    \begin{equation}
        \d \lambda_i = 2g(\lambda_i)h(\lambda_i)\d W_i + \biggl( b(\lambda_i) + \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t, \label{eq:gen_dyson}
    \end{equation}

    \noindent where $(W_i)_{i}$ are independent Brownian motions.
\end{theorem}

\begin{proof}
    Recall that for every $t$, the process $X(t)$ admits a decomposition of the form 

    \[ X(t) = H \Lambda H^T, \]

    \noindent where both $\Lambda$ and $H$ are matrix-valued stochastic processes, $\Lambda = \mathrm{diag}(\lambda_1, \dots, \lambda_p)$ is the diagonal matrix of ordered eigenvalues of $X(t)$ and $H$ is the corresponding matrix of eigenvectors.

    Let us define the stochastic logarith of $H$ as

    \begin{equation*}
        \d A \coloneqq H^{-1} \partial H = H^T \partial H = H^T \d H + \frac12(\d H^T)\d H.
    \end{equation*}

    By using Itô's formula on $I = H^T H$ we find

    \begin{align*}
        0 = \d I = \d(H^T H) = H^T\d H + (\d H)^T H + (\d H)^T \d H = H^T\partial H + (\partial H)^T H = A + A^T.
    \end{align*}

    Which means $A$ is skew symmetric. Using that $H^T H = I$, we have $\Lambda = H^T H \Lambda H^T H = H^T X H$, by the matrix Itô formula, we find \todo{Revisar bien esta cuenta}


    \begin{align*} 
        \d \Lambda & = \d(H^TXH) = (\partial H^TX)H + H^TX\partial H,\\ 
        & = (\partial H)^T XH + H^T(\partial X)H + H^T X \partial H,\\
        & = (\partial H)^T H\Lambda + H^T(\partial X) H + \Lambda H^T \partial H,\\
        & = (\partial A)^T \Lambda + H^T(\partial X) H + \Lambda \partial A,\\
        & = H^T(\partial X) H -  (\partial A)\Lambda + \Lambda \partial A.
    \end{align*}
    The entries in the diagonals of $(\partial A)\Lambda$ and $\Lambda\partial A$ coincide, and thus the diagonal of $\Lambda\partial A-(\partial A)\Lambda$ is zero. Let us denote $\d N = H^T(\partial X) H$, then

    \[ \d \lambda_j = \d N_{jj}, \]

    \noindent and, using that $\Lambda$ is a diagonal matrix, if $i\neq j$,

    \[ 0 = \d N_{i,j} + (\lambda_i - \lambda_j)\d A_{ij}. \]

    This leads to the following representation for $A_{ij}$,

    \begin{equation} \label{eq:eigenAN}
        \d A_{i,j} = \frac{\d N_{i,j}}{\lambda_j - \lambda_i}, \qquad i \neq j.
    \end{equation}

    From \eqref{eq:matrix_diffusion} we compute the quadratic covariation $\d X_{ij}\d X_{km}$,

    \begin{align*}
        \d X_{ij}\d X_{km} &= \d \bigl< (g(X(t))\d B(t)h(X(t)))_{ij} + (h(X_t)\d \trans{B}(t)(g(X(t))))_{ij},\\
        &(g(X(t))\d B(t)h(X(t)))_{km} + (h(X_t)\d \trans{B}(t)(g(X(t))))_{km} \bigr>,\\
        &= \d \bigl< (g(X(t))\d B(t)h(X(t)))_{ij}, (g(X(t))\d B(t)h(X(t)))_{km} \bigr> \\
        & + \d \bigl< (g(X(t))\d B(t)h(X(t)))_{ij}, (h(X_t)\d \trans{B}(t)(g(X(t))))_{km} \bigr> \\
        & + \d \bigl< (h(X_t)\d \trans{B}(t)(g(X(t))))_{ij} , (h(X_t)\d \trans{B}(t)(g(X(t))))_{km} \bigr> \\
        & + \d \bigl< (h(X_t)\d \trans{B}(t)(g(X(t))))_{ij}, (g(X(t))\d B(t)h(X(t)))_{km} \bigr>
    \end{align*}

        Let us first find $\d \bigl< (g(X(t))\d B(t)h(X(t)))_{ij}, (g(X(t))\d B(t)h(X(t)))_{km} \bigr>$, the other summands are analogous,

    \begin{align*}
        \d \bigl< (g(X(t))&\d B(t)h(X(t)))_{ij}, (g(X(t))\d B(t)h(X(t)))_{km} \bigr>, \\
        &=\d \biggl< \sum_{p,q} g(X(t))_{ip}\d B(t)_{pq}h(X(t))_{qj}, \sum_{r,s} g(X(t))_{kr}\d B(t)_{rs}h(X(t))_{sm} \biggr> 
        \intertext{using the independence between the entries in the brownian matrix,}
        &= \sum_{p,q} \d\bigl< g(X(t))_{ip}\d B(t)_{pq}h(X(t))_{qj} , g(X(t))_{kp}\d B(t)_{pq}h(X(t))_{qm} \bigr> \\
        &= \sum_{pq} g(X(t))_{ip}h(X(t))_{qj}, g(X(t))_{kp}h(X(t))_{qm}\d t,\\
        &= \biggl( \sum_p g(X(t))_{ip}g(X(t))_{kp}\biggr)\biggl(\sum_q h(X(t))_{qj}h(X(t))_{qm}\biggr) \d t, \\
        &= \bigl(g(X(t))\trans{g(X(t))}\bigr)_{ik}\bigl( \trans{h(X(t))}h(X(t))\bigr)_{jm}\d t, \\
        &= \left( Hg(\Lambda)\trans H Hg(\Lambda)\trans H \right)_{ik}\left( Hh(\Lambda)\trans H Hh(\Lambda)\trans H \right)_{jm} \d t,\\
        &= \left( Hg^2(\Lambda)\trans H \right)_{ik}\left( Hh^2(\Lambda)\trans H \right)_{jm} \d t = g^2(X)_{ik} h^2(X)_{jm} \d t.
    \end{align*}

    Proceeding similarly with the other four summands we find

    \begin{align*}
        \d X_{ij} \d X_{km} = (g^2(X)_{ik}h^2(X)_{jm} + g^2(X)_{im}h^2(X)_{jk} + g^2(X)_{jk}h^2(X)_{im} + g^2(X)_{jm}h^2(X)_{ik})\d t.
    \end{align*}

    Since $\d N = H^T(\partial X)H$ only differs in a finite variation part of $H^T(\d X) H$, the martingale part of both processes coincide and then the quadratic covariation of the entries of $N$ is

    \begin{align*}
        \d N_{ij}\d N_{km} &= \d \bigl< (\trans H\d X H)_{ij},(\trans H \d X H)_{km} \bigr> = \sum_{pqrs} \d\bigl< \trans{H}_{ip}\d X_{pq} H_{qj}, \trans{H}_{kr}\d X_{rs} H_{sm} \bigr>, \\
        &= \sum_{pqrs} \trans{H}_{ip}H_{qj}\trans{H}_{kr}H_{sm}\d X_{pq} \d X_{rs},\\ 
        &= \sum_{pqrs} \trans{H}_{ip}H_{qj}\trans{H}_{kr}H_{sm}\bigl(g^2(X)_{pr}h^2(X)_{qs} + g^2(X)_{ps}h^2(X)_{qr} + g^2(X)_{qs}h^2(X)_{pr} \\ 
        &+ g^2(X)_{qr}h^2(X)_{ps}\bigr)\d t.
    \end{align*}

    We find first $\sum_{pqrs} \trans{H}_{ip}H_{qj}\trans{H}_{kr}H_{sm}g^2(X)_{pr}h^2(X)_{qs}$ and the other terms are similar,

    \begin{align*}
        \sum_{pqrs} \trans{H}_{ip}H_{qj}\trans{H}_{kr}H_{sm}g^2(X)_{pr}h^2(X)_{qs} &= \biggl(\sum_{pr}\trans{H}_{ip}g^2(X)_{pr}H_{rk}\biggr)\biggl(\sum_{qs}\trans{H}_{jq}h^2(X)_{qs}H_{sm}\biggr),\\
        &= \bigl( \trans{H}Hg^2(\Lambda)\trans{H}H\bigr)_{ik}\bigl(\trans{H}Hh^2(\Lambda)\trans{H}H\bigr)_{jm} = g^2(\Lambda)_{ik}h^2(\Lambda)_{jm}.
    \end{align*}

    Repeating the analogous procedure with all of the terms we find that the covariation is

    \begin{equation*} \label{eq:quadvarN}
        \d N_{ij}\d N_{km} = (g^2(\Lambda)_{ik}h^2(\Lambda)_{jm} + g^2(\Lambda)_{im}h^2(\Lambda)_{jk} + g^2(\Lambda)_{jk}h^2(\Lambda)_{im} + g^2(\Lambda)_{jm}h^2(\Lambda)_{ik})\d t.
    \end{equation*}

    It follows that the quadratic variation in the diagonal is

    \begin{equation*}
        \d N_{ii} \d N_{jj} = 4\delta_{ij}g^2(\lambda_i)h^2(\lambda_j)\d t.
    \end{equation*}

    Now, in order to compute $F$, the finite variation part of $N$, we use \eqref{eq:matrix_diffusion},

    \begin{align*}
        \d F &= H^Tb(X)H\d t + \frac12(\d H^T \d X H + H^T \d X \d H),\\
        &= b(\Lambda)\d t + \frac12 \left( (\d H^T H)(H^T\d X H) + (H^T \d X H)(H^T \d H) \right),\\
        \intertext{using that the martingale part of $H^T\d H$ and $H^T\partial H$ coincide and the same with $H^T(\partial X)H$ and $H^T (\d X) H$,}
        &= b(\Lambda)\d t + \frac12( (\d N \d A)^T + \d N \d A).
    \end{align*}

    Now we can use \eqref{eq:eigenAN} and \eqref{eq:quadvarN} to find $\d N \d A$,

    \begin{align*}
        (\d N \d A)_{ij} &= \sum_{k \neq j} \d N_{ik}\d A_{kj} = \sum_{k\neq j} \frac{\d N_{ik}\d N_{kj}}{\lambda_j - \lambda_k} = \delta_{ij}\sum_{k\neq j} \frac{ g^2(\lambda_i)h^2(\lambda_k) + g^2(\lambda_k)h^2(\lambda_i) }{\lambda_i - \lambda_k} \d t.
    \end{align*}

    Recalling that $G(x,y) = g^2(x)h^2(x) + g^2(y)h^2(y)$, we have that

    \begin{equation*}
        (\d N \d A)_{ij} = \delta_{ij}\sum_{k\neq j} \frac{ G(\lambda_i,\lambda_k) }{\lambda_i - \lambda_k} \d t.
    \end{equation*}

    From \eqref{eq:quadvarN} we have that the martingale part of $N_{ii}$ has the form
    $2g(\lambda_i)h(\lambda_i)\d W_i$ for some Brownian motion $W_i$. Putting together the martingale and finite variation parts of $N$ we have that

    \begin{equation*}
        \d N_{ii} = 2g(\lambda_i)h(\lambda_i)\d W_i + \sum_{k\neq j} \frac{ G(\lambda_i,\lambda_k) }{\lambda_i - \lambda_k} \d t.
    \end{equation*}

    Since $\d \lambda_i = \d N_{ii}$, this finishes the proof.
\end{proof}





\begin{theorem} \label{thm:diffusion_complejo}
    Let $W(t)$ be a complex $p\times p$ Brownian matrix. Suppose that $X = (X(t), t \ge 0)$ is a matrix-valued process taking values in the group of self adjoint matrices and it satisfies the following matrix stochastic differential equation:

    \begin{equation}\label{eq:complex_diff}
        \d X(t) = g(X(t))\d  W(t) h(X(t)) + h(X(t))\hermit{\d W(t)} g(X(t)) + b(X(t))\d t,
    \end{equation}

    \noindent with $g,h,b: \R \to \R$ and $X_0$ is a hermitian $p\times p$ random matrix with $p$ different eigenvalues.

    Let $G(x,y) = g^2(x)h^2(y) + g^2(y)h^2(x)$, and
    
    \[ \tau = \inf\{ t: \lambda_i(t) = \lambda_j(t) \text{ for some } i\neq j \}. \]
    
    Then, for $t < \tau$ the eigenvalue process $\Lambda_t$ verifies the following stochastic differential equations:

    \begin{equation}
        \d \lambda_i = 2g(\lambda_i)h(\lambda_i)\d W_i + \biggl( b(\lambda_i) + 2\sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k} \biggr)\d t,
    \end{equation}

    \noindent where $(W_i)_{i}$ are independent Brownian motions.
\end{theorem}

\begin{proof} 
    Recall that for a complex Brownian motion $Z$ we have that

    \[\d\langle Z,Z\rangle(t) = 0, \qquad \d \langle Z ,\overline Z\rangle(t) = 2\d t.\]

    
    Then we can compute the quadratic covariation $\d X_{ij}\d X_{kl}$ using \eqref{eq:complex_diff},

    \begin{align}
        \d X_{ij}\d X_{kl} &=  \d\langle X_{ij},X_{kl} \rangle(t), \\
        &= \d\langle \left( g(X)\d  W h(X) + h(X)\d \hermit{W} g(X) \right)_{ij}, \left(g(X)\d W h(X) + h(X)\d \hermit{W} g(X)\right)_{kl} \rangle(t),\\
        &= \d\langle \left(g(X)\d  W h(X)\right)_{ij}, \left(h(X)\d \hermit{W} g(X)\right)_{kl}\rangle + \d\langle \left(g(X)\d  W h(X)\right)_{kl}, \left(h(X)\d \hermit{W} g(X)\right)_{ij}\rangle(t),\\
        &= 2g^2(X)_{il}h^2(X)_{jk}\d t + 2g^2(X)_{kj}h^2(X)_{li}\d t,\\
        &= 2\left( g^2(X)_{il}h^2(X)_{kj} + g^2(X)_{jk}h^2(X)_{il} \right)\d t.
    \end{align}


    Analogously to the real case, we define $A$, the stochastic logarithm of $H$, as

    \begin{equation*}
        A \coloneqq H^{-1}\partial H = H^* \partial H.
    \end{equation*}

    By using Itô's formula we find,

    \begin{equation*}
        0 = \d I = \d(H^*H) = H \partial H^* + (\partial H) H^* = A^* + A,
    \end{equation*}

    \noindent which means $A$ is skew-Hermitian. This implies that the real parto of the terms in the diagonal of $A$ is zero. Let us now apply Itô's formula to $\Lambda = H^* X H$,

    \begin{align*} % Mejorar esto.
        \d \Lambda &= \d(H^* X H) = H^*(\d(XH)) + (\d H^*) XH + \d(H^*)\d (XH),\\
        &= H^*(\d X) H + H^* X \d H + H^*(\d X\d H) + (\d H^*)X H + \d(H^*)(\d X)H + \d(H^*)X\d H + \d H^*\d X\d H,\\
        &= H^*(\partial X)H + H^*X\partial H + (\partial H^*)XH = H^*(\partial X)H + \Lambda H^* \partial H + (\partial H^*)H\Lambda,\\
        &= H^*(\partial X)H + \Lambda \partial A + \trans{\partial A}\Lambda = H^*(\partial X)H + \Lambda \partial A - \partial A \Lambda.
    \end{align*}

    By the relationship between Itô's and Stratanovich's integrals,

    \begin{equation*}
        H^*(\partial X)H = H^*(\d X)H + \frac12(\d H^*(\d X)H + H^*\d X\d H),
    \end{equation*}

    \noindent so using that $X$ is hermitian, we have that $H^*(\partial X)H$ is also hermitian and its diagonal elements are real. The process $\Lambda \partial A - (\partial A)\Lambda$ is zero in the diagonal and thus $\d \lambda_i = (H^*(\partial X)H)_{ii}$.  If $i \neq j$, we have

    \[ 0 = (H^*(\partial X)H)_{ij} + \lambda_i\partial A_{ij} - \lambda_j \partial A_{ji} = (H^*(\partial X)H)_{ij} + (\lambda_i - \lambda_j)\partial A_{ij}. \]

    The last part implies $\partial A_{ij} = \frac{(H^*(\partial X)H)_{ij}}{\lambda_j - \lambda_i}$, whenever $i\neq j$.

    Define $\d N = \d H^*(\partial X)H$. The martingale part of $N$ and $H^*(\d X)H$ is the same, since they differ only in a finite variation term. We can find $\d N_{ij}\d N_{kl}$ using $\d X_{ij} \d X_{kl}$,

    \begin{equation*}
        \d N_{ij}\d N{kl} = 2(g^2(\Lambda)_{il}h^2(\Lambda)_{jk} + g^2(\Lambda)_{jk}h^2(\Lambda)_{il})\d t.
    \end{equation*}

    Then, for the elements in the diagonal we have

    \begin{equation}
        \d N_{ii}\d N_{jj} = 4\delta_{ij}(g^2(\lambda_i)h^2(\lambda_i))\d t.
    \end{equation}

    Now we compute the finite variation part of $\d N$ from \eqref{eq:complex_diff}. Let us denote it as $\d F$.

    \begin{align*}
        \d F &= H^*b(X)H\d t + \frac12(\d H^*(\d X)H + H^*\d X\d H),\\
             &= b(\Lambda)\d t + \frac12 \bigl((\d H^*H) (H^*\d XH) + (H^*\d X H)(H^*\d H)\bigr),\\
             &= b(\Lambda)\d t + \frac12\bigl((\d N \d A)^* + \d N \d A \bigr).
    \end{align*}

    Using the quadratic variation of $\d N$ and $\d A$ we find their covariation,

    \begin{align*}
        (\d N\d A)_{ij} &= \sum_{k} (\d N)_{ik}(\d A)_{kj} = \sum_k \frac{(\d N)_{ik}(\d N)_{kj}}{\lambda_j - \lambda_i}, \\
        &= 2\delta_{ij}\sum_{k\neq j} \frac{ g^2(\lambda_i)h^2(\lambda_k) + g^2(\lambda_k)h^2(\lambda_j)}{\lambda_j - \lambda_k} + \d N_{ij}\d A_{jj}.
    \end{align*}

    By the properties shown above for $\d N$ and $\d A$, if$i=j$, $\d N_{jj}$ is real and $\d A_{jj}$ is purely imaginary. By independence of the real and imaginary parts of the complex Brownian motion, this implies that $\d N_{jj}\d A_{jj}=0$. We have

    \begin{equation*}
        \d F_{ii} = \left(b(\lambda_i) + 2 \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_j}\right)\d t,
    \end{equation*}

    \noindent where $G(x,y)=g^2(x)h^2(y) + g^2(y)h^2(x)$.

    Using the quadratic variation of $\d N$, we find that the martingale part of $\d N_{ii}$ is 

    \begin{equation*}
        \d M_{ii} = 2g(\lambda_i)h(\lambda_i)\d W_i,
    \end{equation*}

    \noindent for some Brownian motion. Recall that $\d \lambda_i = \d N_{ii}$, then we have that there exist $W_1,\dots,W_p$ independent Brownian motions such that

    \begin{equation*}
        \d \lambda_i = \d N_{ii} = 2g(\lambda_i)h(\lambda_i)\d W_i + \left(b(\lambda_i) + 2 \sum_{k\neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_j}\right)\d t.
    \end{equation*}

    This ends the proof.
\end{proof}


\begin{theorem}[Multidimensional Yamada-Watanabe Theorem \cite{article:multiyamada}]%First part of Multidimensional Yamada-Watanabe] 
    \label{thm:mult_yamada_watanabe}
    Let $p\in \N$ and %,q,r\in \N$ and 

    \begin{align*}
        b_i&: \R^p \to \R, \qquad i = 1, \dots, p,%\\
        %c_k&: \R^{p+r} \to \R, \qquad k = p+1, \dots, p + q,\\
        %d_j&: \R^{p+r} \to \R, \qquad j = p+1, \dots, p + r, 
    \end{align*}

    \noindent be real-valued continuous functions satisfying the following Lipschitz conditions for $C>0$,

    \begin{align*}
        \abs{b_i(y_1) - b_i(y_2)} &\le C \norm{y_1 - y_2}, \quad i = 1, \dots, p,%\\
        %\abs{c_k(y_1,z_1) - c_k(y_2,z_2)} &\le C\norm{(y_1,z_1) - (y_2,z_2)}, \quad k = p+1, \dots, p + q,\\
        %\abs{d_j(y_1,z_1) - d_j(y_2,z_2)} &\le C\norm{(y_1,z_1) - (y_2,z_2)}, \quad j = p+1, \dots, p + r,
    \end{align*}

    \noindent for every $y_1, y_2 \in \R^p$. %and $z_1,z_2\in \R^r$. 

    Further, let $\sigma_i:\R \to \R, i =1, \dots, p$ be a set of measurable functions such that

    \[ \abs{\sigma_i(x) - \sigma_i(y)}^2 \le \rho_i(\abs{x-y}), \quad x,y \in \R, \]

    \noindent where $\rho_i:(0,\infty)\to(0,\infty)$ are measurable functions such that 

    \[ \int_{0^+} \rho_i^{-1}(x)~\d x = \infty.\]

    Then the pathwise uniqueness holds for the following system of stochastic differential equations

    \begin{align}
        \d Y_i &= \sigma_i(Y_i)\d B_i + b_i(Y)\d t, \qquad i = 1, \dots, p,%\\
        %\d Z_j &= \sum_{k=p+1}^{p+q} c_k(Y,Z) \d B_k + d_j(Y,Z)\d t, \quad j = p + 1, \dots, p+r,
    \end{align}

    \noindent where $B_1, \dots, B_{p}$ are independent Brownian motions.
\end{theorem}

\begin{proof}
    Let $Y$ and $\hat Y$ be two solutions with respect to the same multidimensional Brownian motion $B = (B_i)_{i \le p}$ such that $Y(0) = \hat Y(0)$ %and $Z(0) = \hat Z(0)$ 
    a.s., for $i\le p$ we have

    \begin{equation}
        Y_i(t) - \hat Y_i(t) = \int_0^t \sigma_i(Y_i) - \sigma_i(\hat Y_i) ~\d B_i(s) + \int_0^t b_i(Y_i) - b_i(\hat Y_i) ~\d s.
    \end{equation}

    We can then see that

    \begin{equation*}
        \int_0^t \frac{\mathds{1}_{\{Y_i(s) > \hat Y_i(s)\}}}{\rho_i(Y_i(s)- \hat Y_i(s))} \d \langle Y_i - \hat Y_i, Y_i - \hat Y_i \rangle = \int_0^t \frac{( \sigma_i (Y_i(s)) - \sigma_i (\hat Y_i(s)) )^2}{\rho_i(Y_i(s) - \hat Y_i(s))} \mathds 1_{\{Y_i(s) > \hat Y_i(s)\}} ~\d s \le t.
    \end{equation*}
    
    Applying Theorem \ref{thm:local_zero} we have that the local time of $Y_i - \hat Y_i$ at 0 is 0. Then, we can use the Tanaka formula to find

    \begin{align*}
        \abs{ Y_i(t) - \hat Y_i(t) } &= \int_0^t \mathrm{sgn}(Y_i(s) - \hat Y_i(s)) ~\d( Y_i(s) - \hat Y_i(s) ),\\
        &= \int_0^t \mathrm{sgn}(Y_i(t) - \hat Y_i(t))(\sigma_i(Y_i) - \sigma_i(\hat Y_i)) ~\d B_i(s)\\
        &\phantom{espacioteeeee}+ \int_0^t \mathrm{sgn}(Y_i(s) - \hat Y_i(s))(b_i(Y_i(s)) - b_i(\hat Y_i(s))) ~ \d s.
    \end{align*}

    Since $\sigma_i$ is bounded, we have that $\sgn(Y_i(t) - \hat Y_i(t))(\sigma_i(Y_i(t)) - \sigma_i(\hat Y_i(t)))$ is bounded and therefore the first integral in the last expression is a martingale with mean 0, which in turns implies that

    \begin{equation*}
        \abs{ Y_i(t) - \hat Y_i(t) } - \int_0^t \sgn(Y_i(s) - \hat Y_i(s))(b_i(Y_i(s)) - b_i(\hat Y_i(s)))~\d t,
    \end{equation*}

    \noindent is a zero-mean martingale. Then, by using the Lipschitz properties of $b_i$ we have

    \begin{align*}
        \E{\abs{Y_i(t) - \hat Y_i(t)}} &= \E{  \int_0^t \sgn(Y_i(s) - \hat Y_i(s))(b_i(Y_i(s)) - b_i(\hat Y_i(s)))~\d s },\\
        &\le \E{ \int_0^t \abs{b_i(Y_i(s)) - b_i(\hat Y_i(s))}~\d s},\\
        &= \int_0^t \E{ \abs{ b_i(Y_i(s)) - b_i(\hat Y_i(s)) } }~\d s 
        \le  C\int_0^t \E{\abs{Y_i(s) - \hat Y_i(s)}}~\d s.
    \end{align*}

    Summing for every $i$ we get

    \begin{equation*}
        \E{\abs{Y(t) - \hat Y(t)}} \le  Cp\int_0^t \E{\abs{Y(t) - \hat Y(s)}}~\d s.
    \end{equation*}

    Using Gronwall's lemma (\ref{lemma:gronwall}) we get that

    \begin{equation*}
        \E{\abs{Y(t) - \hat Y(t)}} = 0, 
    \end{equation*}

    \noindent which implies $Y(t) = \hat Y(t)$ a.s. for every $t>0$, ending the proof.
\end{proof}




\begin{theorem}[Spectral matrix Yamada-Watanabe theorem] \label{thm:spectral_yamadawatanabe}
    Let $X(t)$ be a $p\times p$ symmetric matrix-valued process satisfying the equation \eqref{eq:matrix_diffusion} with initial condition $X(0)$ that is a symmetric $p\times p$ matrix with $p$ different eigenvalues. Suppose further that

    \begin{equation}
        \abs{ g(x)h(x) - g(y)h(y) }^2 \le \rho(\abs{x-y}), \qquad x,y \in \R,
    \end{equation}

    \noindent with $\rho:(0,\infty)\to(0,\infty)$ a measurable function satisfying

    \[ \int_{0^+} \rho^{-1}(x) \d x = \infty, \]

    \noindent that $G(x,y) \coloneqq g^2(x)h^2(y) + g^2(y)h^2(x)$ is locally Lipschitz and strictly positive on the set $\{ x \neq y\}$ and that $b$ is locally Lipschitz. Then if $\tau$ is defined as in \eqref{eq:collision_time}, for $t < \tau$, the process of eigenvalues satisfying \eqref{eq:gen_dyson} has a pathwise unique solution.
\end{theorem}

\begin{proof}
    Let $PN \trans{P}$ be a diagonalization for $X(0)$. We need to show that a unique strong solution exists for \eqref{eq:gen_dyson} when $\Lambda(0)=N$. The functions
    
    \[ b_i(\lambda_1, \dots,\lambda_p) = b(\lambda_i) + \sum_{k \neq i} \frac{G(\lambda_i,\lambda_k)}{\lambda_i - \lambda_k}, \]

    \noindent are locally Lipschitz continuous on $\Delta_p$ so they can be extended from the compact sets 

    \[ D_m = \{ 0 \le \lambda_1 < \lambda_2 < \cdots < \lambda_p < m, \lambda_{i+1} - \lambda_i \ge 1/m \}, \]

    \noindent to bounded Lipschitz functions on $\R^p$. Let $b_i^m$ denote such extension for $m \in \N \setminus \{0\}$. For $i = 1, \dots, p$, we consider the system of SDEs,

    \[ \d \lambda_i = 2g(\lambda_i^m)h(\lambda_i^m)\d W_i + b_i^m(\Lambda^m) \d t. \]

    We have that $\abs{g(x)h(x) - g(y)h(y)}^2 \le \rho(\abs{x-y})$ and $\int_{0^+} \rho(x)^{-1}\d x=\infty$, and using Theorem \ref{thm:mult_yamada_watanabe} we get that there is a unique strong solution for the system of SDEs. Since $D_m \subset D_{m+1}$, we have that $\lim_{m\to\infty} D_m = \Delta_p$, so there exists a unique strong solution $\Lambda(t)$ for the SDEs system up to the first exit time from $\Delta_p$. This time is $\tau$, the first collision time of the eigenvalues.

\end{proof}

\begin{corollary}
    Suppose that $b, g^2, h^2$ are Lipschitz continuous, $g^2h^2$ is convex or or continuously differentiable with derivative uniformly Lipschitz on $\R$ and that $G(x,y) \coloneqq g^2(x)h^2(y) + g^2(y)h^2(x)$ is strictly positive on $\{x \neq y\}$. Then the system of SDEs \eqref{eq:gen_dyson} for the eigenvalue process satisfying \eqref{eq:matrix_diffusion} has a unique strong solution on $[0,\infty)$.
\end{corollary}

\begin{proof}
    Recall that if $f$ is a non-negative Lipschitz continuous function, then $\sqrt f$ is $1/2$-Hölder continuous. Since $g^2$ and $h^2$ are Lipschitz continuous, then $g^2h^2$ is locally Lipschitz continuous and $gh$ is $1/2$-Hölder continuous. Then

    \begin{align*}
        \abs{g(x)h(x) - g(y)h(y)}^2 &\le \left( K\abs{x-y}^{\frac12} \right)^2 = K^2 \abs{x-y}.
    \end{align*}

    Taking $\rho(|x-y|) = K^2\abs{x-y}$ we see that the conditions of Theorem \ref{thm:spectral_yamadawatanabe} are satisfied and then the uniqueness and existence of the strong solution applies on $[0,\tau)$. By Theorem \ref{thm:collision} we have that $\tau=\infty$ a.s., and thus the existence and uniqueness is satisfied on $[0,\infty)$.
\end{proof}



\subsection{Wishart process}


The Wishart process is a dynamical version of the Wishart matrix, which was first described by John Wishart \cite{article:wishart}. If we assume a data population to consist of $n$ features observed in $k$ individuals, then we can form a rectangular $n\times k$ array with this data, let us name $X$ to such array. If we further assume that the variables and individualsa re totally non-correlated and they individual data points ($i$th feature of the $j$th individual) follow a standard normal distribution, then $X$ is a standard independent Gaussian matrix of size $n\times k$. The matrix $X$ can be thought as a size $k$ sample observation of an independent Normal vector $\vec v$ in $\R^k$. It is a well known fact in statistics that an estimator for the covariance matrix of $\vec v$ is $W \coloneqq \trans{X}X$. Under the former assumptions, $W$ follows a Wishart distribution.

Besides estimating a covariance matrix, the Wishart matrix distribution has some other uses in multivariate statistics. A notable one is its use for Principal Component Analysis (PCA). In \cite{book:hastie_tibshirani}, Principal Components are described as ``a sequence of projections of the data, mutually uncorrelated and ordered in variance''. The key idea is that if we find the eigenvalues of a covariance matrix and order them, then we can know what are the most influential features in the data variance, i.e.\ the eigenvector associated to the largest eigenvalue carries out the most variance of the data. Every eigenvector is a ``projection of the data'' and uncorrelated to any other eigenvector (as they form an orthogonal basis). 

A natural question that comes to mind when doing PCA is what would happen to the Principal Components if we add a mild perturbation (e.g. Gaussian noise). Bru \cite{bru1989diffusions} tackled this problem by considering adding a Brownian motion as a noise. With this, she was able to use stochastic calculus techniques to study the behaviour of the Principal Components when the added variance fluctuated. 


In this subsection, we use the Theorems proven previously to replicate Bru's results. It is worthmentioning that the techniques in this section taken from \cite{article:multiyamada} were originally inspired by the work of Bru in \cite{bru1989diffusions}.


Once we have Theorems \ref{thm:diffusion_real} and \ref{thm:diffusion_complejo}, proving the form of the eigenvalues in a Wishart process is quite straight-forward.



\begin{corollary}
    Let $\tilde B = (\tilde B(t), t\ge 0)$ be a Brownian motion in $\mathcal M_{n,m}(\R)$ with $n\ge m$ and define $X = \trans{\tilde B}\tilde B$. Then the eigenvalues of $X$, $\lambda_1 > \lambda_2 > ... > \lambda_n$ are given by the unique strong solution to the following system of stochastic differential equations

    \begin{equation*}
        \d \lambda_i = 2 \sqrt{\lambda_i} \d W_i + \left(m + \sum_{k\neq i} \frac{\abs{\lambda_i} + \abs{\lambda_k}}{ \lambda_i - \lambda_k} \right) \d t.
    \end{equation*}

    Morover, if $Y$ is any matrix-valued stochastic process satisfying the stochastic differential equation

    \begin{equation*}
        \d Y(t) = \sqrt{Y(t)} \d B(t) + \d \trans{B}(t) \sqrt{Y(t)} + \alpha I \d t,
    \end{equation*}

    with respect to $B = (B(t), t \ge 0)$ a Brownian motion in $\M_{n\times n}(\R)$ and $n\ge p-1$, then its eigenvalues are the unique strong solution to the system of stochastic differential equations

    \begin{equation} \label{eq:wishart}
        \d \lambda_i = 2 \sqrt{\lambda_i} \d W_i + \left(\alpha + \sum_{k\neq i} \frac{\abs{\lambda_i} + \abs{\lambda_k}}{ \lambda_i - \lambda_k} \right) \d t.
    \end{equation}

\end{corollary}

\begin{proof}

    We prove first that $X$ satisfies

    \begin{equation*}
        \d X(t) = \sqrt{X(t)} \d B(t) + \d \trans{B}(t) \sqrt{X(t)} + \alpha I \d t,
    \end{equation*}

    for an $n\times n$ matrix-valued Brownian motion, and then use Theorem \ref{thm:diffusion_real}. 

    By the matrix Itô formula we have for $X$,

    \begin{equation*}
        \d X(t) = (\d \trans{\tilde B}(t)) \tilde B(t) + (\trans{\tilde B}(t)) \d B(t) + \d \langle \trans{\tilde B}, \tilde B \rangle(t).
    \end{equation*}

    For the covariation term we can find

    \begin{align*}
        \d \langle \trans{\tilde B}, \tilde B \rangle(t)_{ij} &= \sum_{k=1}^m \d \langle \tilde B_{ki}, B_{kj} \rangle(t) = m\delta_{ij} \d t.
    \end{align*}

    This means $\d \langle \trans{\tilde B}, \tilde B \rangle(t) = m I \d t$.

    For the remaining terms, we find the covariation,

    \begin{equation*}
        (\trans {\tilde B} \d B)_{ij} (\trans{(\d \tilde B)}B)_{kl} = X_{il}\delta_{jk} \d t,
    \end{equation*}

    \noindent which in total accounts for 

    \begin{equation*}
        \d \langle X_{ij}, X_{kl} \rangle(t) = ( X_{ik}\delta_{jl} + X_{il}\delta_{jk} + X_{jk}\delta_{il} + X_{jl}\delta_{ik} )\d t.
    \end{equation*}

    With this, we can find the quadratic variation for the diagonal and off-diagonal entries of $X$.

    \begin{align*}
        \d \langle X_{ij}, X_{ij} \rangle (t) &= \left\{ \begin{array}{cc}
            (X_{ii} + X_{jj})\d t & \text{if $i\neq j$},\\
            4 X_{ii} \d t & \text{if $i=j$}.
        \end{array} \right.
    \end{align*}

    With this covariations, we find that the entries of $\d X$ coincide with those of $\sqrt{X(t)} \d B(t) + \d \trans{B}(t) \sqrt{X(t)} + \alpha I \d t$. Now, in Theorem \ref{thm:diffusion_real} substitute $g(x) = \sqrt{x}, h(x) \equiv 1$ and $b(x) \equiv \alpha$ to find that \eqref{eq:wishart} is satisfied.
\end{proof}

Using Brownian motions in $\M_{n,m}(\C)$ instead and repeating all the steps we find the corresponding equation for the complex Wishart process. Also, the matrix can be rescaled as in the Dyson Brownian motion case to find a version where the $\beta$ parameter affects the martingale part.

\subsection{Jacobi process}

Similarly to the Wishart case, the Jacobi process is a dynamical generalization of a random matrix used in statistics. There are basically two contexts in where the matrix jacobi process appears, one of them is in analysis of variance \cite{book:multivariate_statistics} and there it is defined as the ``quotient'' of a Wishart matrix and its sum to an independent Wishart matrix, i.e.

\begin{equation*}
    J \coloneqq (W_1 + W_2)^{-1}W_1,
\end{equation*}

\noindent where $W_1, W_2$ are indepenent Wishart matrices. In this context, the Jacobi matrix (known as MANOVA matrix) is something as a generalization of an $F$ distribution in the context of univariate ANOVA.

The second context, and the one we are interested in is the Generalized Singular Value Decomposition (GSVD) algorithm \cite{article:GSVD_van_loan} which is used for a block matrix. The construction we give here is found in \cite{article:marcus_finite_free_point_processes} and it coincides with the construction of a Beta-matrix in \cite{doumerc2005matrices}. 

Let $M$ be an independent Gaussian matrix in $\M_{m,n}(\R)$ with $n \le m$. We can decompose $M$ as a block matrix in the way

\begin{equation*}
    M = \begin{bmatrix} M_1 \\ M_2 \end{bmatrix} \begin{matrix} n_1 \\ n_2 \end{matrix}.
\end{equation*}

So $M_1 \in \M_{n_1,n}$ and $M_2 \in \M_{n_2,n}$ are independent Gaussian matrices in its respective spaces, and $n_1 + n_2 = m$. With the GSVD algorithm we can find simultaneously singular value decompositions for $M_1$ and $M_2$ such that $M_1 = U_1 CH, M_2 = U_2 S H$ where $U_1 \in \M_{n_1,n_1}(\R), U_2 \in \M_{n_2,n_2}(\R)$ are orthogonal matrices, $C\in \M_{n_1,n}, S \in \M_{n_2,n}(\R)$ are pseudo diagonals satisfying $\trans{C}C + \trans{S}S = I_{n\times n}$, and $H \in \M_{n,n}$ is invertible. Although the decomposition is not unique, it can be taken so that $U_1,U_2,H$ are Haar distributed, mutually independent and independent from $C,S$. 

Then we take $W_1 = \trans{M_1}M_1$ and $W_2 = \trans{M_1}M_1$, we have that $W_1$ and $W_2$ are $n\times n$ Wishart matrices with shape parameters $n_1$ and $n_2$, respectively. Our matrix of interest is

\begin{equation*}
    J \coloneqq (W_1 + W_2)^{-\frac12} W_1 (W_1 + W_2)^{-\frac12}.
\end{equation*}

    With the singular values decomposition of $M_1$ and $M_2$, we notice that 

\begin{align*}
    \det[J] &= \det[(W_1 + W_2)^{-\frac12} W_1 (W_1 + W_2)^{-\frac12}],\\ 
    &= \det[(\trans H \trans C C H + \trans H \trans SS H)^{-\frac12}\trans H \trans C C H(\trans H \trans C C H + \trans H \trans SS H)^{-\frac12}],\\
    &= \det[(\trans C C + \trans S S)^{\frac12}\trans CC (\trans C C + \trans S S)^{\frac12}],\\
    &= \det{\trans CC}.
\end{align*}

So $J$ has the same eignevalues of $\trans CC$. If we give a singular values decomposition for $M$, $M = VDU$ with $D \in \M_{m,n}(\R)$ pseudodiagonal $D = (\Delta, 0)^T$, $\Delta$ diagonal, and $U \in \M_{n,n}(\R),V \in \M_{m,m}(\R)$ Haar unitaries and independent. Then $\trans M M = \trans U \Delta^2 U$, and

\begin{equation*}
    (W_1 + W_2)^{\frac12} = (\trans U \Delta U)^{\frac12}.
\end{equation*}

Letting $X$ be the $m\times n_1$ upper left corner of $V$ we have that $M_1 = U\Delta X$, and then

\begin{equation*}
    \trans{M_1}M_1 = \trans U \Delta \trans X X \Delta U = (\trans M M)^{\frac12} (\trans U \trans X X U)  (\trans M M)^{\frac12}.
\end{equation*}

Substituting this in our previous definition for $J$ we have

\begin{align*}
    J &= (W_1 + W_2)^{-\frac12} W_1 (W_1 + W_2)^{-\frac12},\\ &= (\trans M M)^{-\frac12} (\trans M M)^{\frac12} (\trans U \trans X X U)  (\trans M M)^{\frac12} (\trans M M)^{-\frac12} = \trans{(XU)}XU.
\end{align*}

Since $X$ is invarant under multiplication by orthogonal matrices, and $U,X$ are indeopndent, then law of $J$ is equal to the law of $\trans X X$. The conclusion is that we can build the Jacobi matrix in two different ways, one using the standard definition as in the MANOVA case and another as the square of the upper left corner in a Haar distributed random matrix. This was discovered by Collins \cite{thesis:collins} and is used by Doumerc to construct the dynamical version of this matrix \cite{doumerc2005matrices}.

For the stochastic process case, it is shown in \cite{doumerc2005matrices} that if $X$ is the upper corner of a Haar unitary Brownian motion, then $J \coloneqq \trans X X$ satisfies the following stochastic differential equation

\begin{equation*}
    \d J (t) = \sqrt{J(t)} \d B(t) \sqrt{I_n - J(t)} + \sqrt{I_n - J(t)} \d B(t) \sqrt{J(t)} + (n_2 I_n - (n_1 + n_2) J(t) ) \d t.
\end{equation*}

With this differential equation, it is easy again to use Theorem \ref{thm:diffusion_real} to conclude the next corollary: 

\begin{corollary}
    Let $X$ be an $n\times n$ matrix valued process satisfying the following stochastic differential equation

    \begin{equation*}
        \d X (t) = \sqrt{X(t)} \d B(t) \sqrt{I_n - X(t)} + \sqrt{I_n - X(t)} \d B(t) \sqrt{X(t)} + (n_2 I_n - (n_1 + n_2) X(t) ) \d t.
    \end{equation*}

    Then its eigenvalues are the unique strong solution to the system of stochastic differential equations

    \begin{equation} \label{eq:jacobi}
        \d \lambda_i = 2 \sqrt{\lambda_i(1-\lambda_i)} \d W_i + \left( n_2 -(n_1 + n_2)\lambda_i + \sum_{k\neq i} \frac{\lambda_i(1-\lambda_k) + \lambda_k(1-\lambda_i)}{\lambda_i - \lambda_k} \right) \d t,
    \end{equation}

    \noindent where $\left\{ W_{i} \right\}_{i=1}^n$ are $n$ independent standard Brownian motions.
\end{corollary}


\begin{proof}
    Let $g(x) = \sqrt{\abs{x}}, h(x) = \sqrt{\abs{1-x}}$ and $b(x) = n_2 - (n_1 + n_2)x$ in \ref{thm:diffusion_real}. \todo{Mencionar en estas pruebas también cómo se da la existencia y unicidad de las soluciones fuertes (por los teoremas probados anteriormente.)}
\end{proof}


\section{Path simulations}

A standard technique for the simulation of solutions to stochastic differential equations is the Euler-Maruyama method. Details about the method can be found in \cite{book:asmussen}[Chapter 10]. The code used for the following figures can be found in the Appendix \ref{appendix:codes}. 

These path simulations have the purpose of visualizing the behavior of the eigenvalue process and how the presence of the Brownian motion term affects the trajectories. Comparing with the simulations of the deterministic processes in Chapter \ref{ch:determinist} will allow us to appreciate how mcuh the finite-variation part of the process ``determines'' its evolution.

In Figure \ref{fig:dyson_comparison}, we can see the path of a nine-dimensional Dyson Brownian motion compared to the finite-variation term. The color of each line represents the correspondence between the drift and the stochastic visualization. We can notice that both the random and the deterministic version do not collide. In the deterministic version, the values separate over time, while in the stochastic one, they ar affected by a noise that causes them to deviate slightly.

\begin{figure}[h!] 
    \input{img/dyson_comparison.pgf}
    \caption{Superposition of simulation of a Dyson Brownian motion and its finite-variation part.\label{fig:dyson_comparison}}
\end{figure}

In Figure \ref{fig:four_dyson} there are four different path simulations for a Dyson Brownian motion with dimension equal to four, five, seven and nine. The initial condition in the third last cases is with $n$ equally spaced points around zero. In the first case, the initial condition is slightly different because the two points in the middle have a separation of size one, while the surrounding points are separated by 0.1 of them. This appears to have little effect on this system, but in Chapter \ref{ch:determinist} we will see that in the deterministic case, it has an effect.

Although the distances between paths in every simulation of Figure \ref{fig:four_dyson} are stochastic, we can notice that the spacing between consecutive points is more or less uniform, which is what we would expect as the particles reject each other with a similar strength. Also, it is interesting to notice that, although the number of paths increases in every chart of the figure, the total path spacing until $t=10$ is more or less uniform in all of them. 

\begin{figure}[h!] 
    \input{img/four_dysons.pgf}
    \caption{Simulation of four different Dyson Brownian motions with different dimension.\label{fig:four_dyson}}
\end{figure}

The ``uniform spacing'' behavior of the Dyson process contrasts with the path simulations for the Wishart process in Figure \ref{fig:wishart_comparison}. In this figure, we have the path for a Wishart process of dimension nine with the corresponding finite variation path. As we can see, the spacing of the particles with the biggest values tends to grow faster. This can be explained because for the Wishart process, the interaction term is proportional to the value of both functions.

\begin{figure} 
    \input{img/wishart_comparison.pgf}
    \caption{Superposition of path simulations of a Wishart process and its finite dimensional part.\label{fig:wishart_comparison}}
\end{figure}

For the Jacobi process path simulations in Figure \ref{fig:jacobi}, we notice a more stable behavior. In Chapter \ref{ch:determinist} we will see that this process has an stationary distribution and thus it is expected that, given any initial condition, the process converges to it. Notice that in this simulation the convergence appears to have rather quickly, as the points appear to be evenly separated at time $t=0.05$.

\begin{figure} 
    \includegraphics[width=6in]{img/jacobi.pdf}
    \caption{Path simulation for a Jacobi process.\label{fig:jacobi}}
\end{figure}