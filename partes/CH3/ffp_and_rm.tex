%\section{Finite free probability and random matrices}


\section{Expected characteristic polynomials} \label{sec:minor_orthogonality}

In this section we show the relationship between the polynomial convolutions defined before and Random Matrix Theory, specifically through the expected characteristic polynomial. Although several results can be found, we will restrict to the symmetric additive case and its relationship to sums of random matrices. The main result in this section is Theorem \ref{thm:symmad}, a similar theorem can also be stated for the product of matrices and symmetric multiplicative convolution. The first subsection introduces minor orthogonality, which is a key concept to prove the following results. In the second subsection we prove the main result.

\subsection{Minor orthogonality}

The next is a rather technical definition, but that allows to prove several of our results of interest.

\begin{definition}[Minor orthogonality]
    Let $R$ be an $m \times n$ random matrix. We say $R$ is minor orthogonal if for every $k,l \in \mathbb Z$ such that $k,l \le \max\{m,n\}$ and all sets $S,T,U,V$ with $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{V} = l$, it satisfies
    
    \begin{equation*}
        \mathbb E_R\left[ \det[R]_{S,T} \det[R^*]_{U,V} \right] = \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{equation*}
\end{definition}

    In the last definition, $\mathbb E_R$ denotes taking the expectation with respect to $R$. We use this convention because in what follows we will have computations of expectations with several random matrices involved and it is convenient to know what is the matrix we are taking the expectation of in every step.

    Later we will prove that some well-known matrix ensembles are minor orthogonal, and that property will help us to make use of the related results. Before doing that, we will prove a lemma that allows us to conclude that some transformations of minor orthogonal matrices are also minor orthogonal.

\begin{lemma} \label{lemma:orth_trans_is_minorth}
    If $R$ is minor orthogonal and $Q$ is a constant matrix such that $Q\hermit{Q} = I$, then $Q$ is minor orthogonal. If $\hermit{Q}Q = I$, then $RQ$ is minor orthogonal.
\end{lemma}

\begin{proof}
    Recall that by the Cauchy-Binet formula (Theorem \ref{thm:cauchy_binet}), for $|S|=|T| = k$ we have

    \begin{equation*}
        \det[QR]_{S,T} = \sum_{\abs{W} = k} \det[Q]_{S,W}\det[R]_{W,T},
    \end{equation*}

    \noindent so with $\abs{S} = \abs{T} = k, \abs{U}=\abs{V} = l$,

    \begin{align*}
        \mathbb E_R \left[ \det [QR]_{S,T}\det [\hermit{R}\hermit{Q}]_{U,V} \right] &= \mathbb E_R \left[  \sum_{\abs{W}=k} \sum_{\abs{Z} = l} \det[Q]_{S,W} \det[R]_{W,T} \det[\hermit{R}]_{U,Z}\det[\hermit{Q}]_{Z,V} \right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} \det[Q]_{S,W}\det[\hermit{Q}]_{Z,V} \mathbb E_R \left[ \det[R]_{W,T}\det[\hermit{R}]_{U,Z}\right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} \det[Q]_{S,W}\det[\hermit{Q}]_{Z,V} \mathbb E_R \left[\frac{\delta_{W,Z}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}\right],\\
        &= \sum_{\abs{W}=k} \det[Q]_{S,W}\det[\hermit{Q}]_{W,V}\frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= \det[Q\hermit{Q}]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= \det[I]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},
        \intertext{Notice that $\det[I]_{S,V} = 1$ if and only if $S=V$, so we conclude that}
        \mathbb E_R \left[ \det[QR]_{S,T}\det[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{align*}

    The case $\hermit{Q}Q =I$ is proven in the same way.
\end{proof}

The former theorem implies that minor orthogonality is preserved under unitary transformations. Now, we will introduce the signed permutation matrix ensemble and prove that it is minor orthogonal. This ensemble will be later used to conclude minor orthogonality in a broader class of random matrices.

\begin{definition}[Signed permutation matrix]
    A signed permutation matrix is a matrix that can be written $EP$ where $E$ is a diagonal matrix with entries $\pm 1$ and $P$ is a permutation matrix.
\end{definition}

\begin{lemma} \label{lemma:singed_per_is_minorth}
    A random matrix sampled uniformly from the set of signed permutation matrices is minor-orthogonal.
\end{lemma}

\begin{proof}
    Let $Q$ be a signed permutation matrix, we can write $Q = E P$, where $E$ is a diagonal random matrix with entries $\pm 1$ taken uniformly and $P$ is a matrix chosen uniformly from the permutation matrices, and both are independent. Then for $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{V} = l$, we have

    \begin{align*}
        \mathbb E_Q\left[ \det[Q]_{S,T}\det[\hermit{Q}]_{U,V} \right] &= \mathbb E_{E,P}\left[ \det[E P]_{S,T}\det[\hermit{P}E]_{U,V} \right],\\
        &= \sum_{\abs{W} = k} \sum_{\abs{Z} = l} \mathbb E_{E,P} \left[ \det [E]_{S,W} \det[P]_{W,T} \det[\hermit{P}]_{U,Z} \det[E]_{Z,V} \right],
        \intertext{every $[E]_{S,W}$ is a diagonal or shift matrix and the determinant would be zero if $S\neq W$, so}
        \mathbb E_Q\left[ \det[Q]_{S,T}\det[\hermit{Q}]_{U,V} \right]&= \mathbb E_{E,P} \left[ \det[E]_{S,S} \det[P]_{W,T} \det[\hermit{P}]_{U,Z} \det[E]_{V,V} \right],\\
        \intertext{Let $\left\{\chi_i\right\}_{1\le i \le n}$ be the diagonal entries of $E$, then}
        \mathbb E_Q\left[ \det[Q]_{S,T}\det[\hermit{Q}]_{U,V} \right]&= \mathbb E_{E}\left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] \mathbb E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right].
    \end{align*}

    Now we use that the variables $\chi_i$ are independent and uniform in $\{-1,1\}$, so that $\mathbb E[\chi_i] = 0$, but $\mathbb E[\chi_i^2] = 1$ for all $i$, and this means 

    \begin{equation*}
        \mathbb E_E \left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] = \delta_{S,V}.
    \end{equation*}

    The last equality leads to

    \begin{align*}
        \mathbb E_R \left[ \det[QR]_{S,T}\det[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} \mathbb E_{P}\left[  \det[P]_{S,T}\det[\hermit{P}]_{U,V} \right],\\
        &=  \delta_{S,V} \mathbb E_{P}\left[  \det[P]_{S,T}\det[\hermit{P}]_{U,S} \right],\\ 
        &= \delta_{S,V} \mathbb E_{P}\left[  \det[P]_{S,T}\det[P]_{S,U} \right].
    \end{align*}

    The submatrix $P_{S,T}$ can be transformed in a diagonal matrix by a permutation matrix because it has at most a non zero entry for each row and each column. If the diagonal matrix has a zero entry in the diagonal, then the determinant $\det[P]_{S,T}$ is zero, in other case, it is different that zero. The only case when all of the diagonal entries of the diagonal matrix are not zero is when $T = \pi(S)$ with $\pi$ the permutation function corresponding to $P$. This means that in order to have a non-zero determinant we need $T = \pi(S) = U$, and $\det[P]_{S,U} \in \{-1,1\}$, so

    \begin{align*}
        \mathbb E_R \left[ \det[QR]_{S,T}\det[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} \delta_{T,U} \mathbb E_{P}\left[ \det [P]_{S,T}\det[P]_{S,T} \right],\\ 
        &= \delta_{S,V} \delta_{T,U} \mathbb E_{P}\left[ \det [P]_{S,T}^2 \right],\\ 
        &= \delta_{S,V} \delta_{T,U} \mathbb E_{P}\left[  \delta_{T=\pi(S)} \right], \\ 
        &= \delta_{S,V} \delta_{T,U} \P{T = \pi(S)}.
    \end{align*}

    We are supposing that we are sampling uniformly from the permutation matrices of size $n \times n$, so the probability that $T = \pi(S)$ when $\pi$ is a permutation of $n$ elements and $\abs{S} = \abs{T} = k$ is $1/\binom{n}{k}$. So, we can conclude

    \begin{align*}
        \mathbb E_R \left[ \det[QR]_{S,T}\det[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V} \delta_{T,U}}{\binom{n}{k}}.
    \end{align*}

    This is the definition of being minor-orthogonal. %\todo{En el paper no lo prueban, pero el resultado se tiene también para una matriz de permutación con signos rectangular. En la siguiente prueba se usa esto, así que falta completar eso. La prueba es igual, simplemente tomando que $E$ es de tamaño $m\times m$ y $P$ de tamaño $m\times n$, todos los resultados se siguen.}
\end{proof}

Repeating the former proof but with $E$ having size $m\times m$ and $P$ being $m\times n$, we have the corollary for rectangular signed permutations.

\begin{corollary}
    Let $E$ be an $m\times m$ diagonal random matrix with independent entries such that $\P{E_{ii} = 1} = \P{E_{ii} = -1} = 1/2$, and let $P$ be an $m\times n$ random matrix taken uniformly from the set of $m\times n$ permutation matrices independent from $E$. Then $EP$ is minor orthogonal.
\end{corollary}

Now that we know that the signed permutations ensemble is minor orthogonal, we can use this and Lemma \ref{lemma:orth_trans_is_minorth} to prove that Haar unitary ensembles are minor orthogonal.


\begin{corollary}
    An $m\times n$ random matrix sampled from the left Haar measure on the Stiefel manifold of $\mathcal M_{n,m}(\C)$ is minor-orthogonal.
\end{corollary}

\begin{proof}
    Let $R$ be a random $m\times n$ matrix sampled from the left Haar measure on the Stiefel manifold of $\mathcal M_{n,m}(\C)$ with $m \le n$ and $Q$ a random signed permutation matrix. Any random signed permutation matrix is unitary, so $RQ$ is Haar distributed for fixed $Q$, and by Lemmas \ref{lemma:orth_trans_is_minorth} and \ref{lemma:singed_per_is_minorth} we have that it is also minor-orthogonal. Then, if $Q$ is uniformly sampled from the signed permutation matrices,

    \begin{align*}
        \mathbb E_R\left[ \det[R]_{S,T}\det[\hermit{R}]_{U,V} \right] = \mathbb E_{R,Q}\left[ \det[RQ]_{S,T} \det[\hermit{(RQ)}]_{U,V}\right]. 
    \end{align*}

    Since $Q$ is minor orthogonal, $RQ$ is also minor orthogonal for fixed $R$ and 

    \begin{align*}
        \mathbb E_R\left[ \mathbb E_Q \left[ \det[RQ]_{S,T}\det[\hermit{(RQ)}]_{U,V} \right] \right] = \mathbb E_R\left[ \frac{\delta_{S,V}\delta_{T,U}}{\binom{n}{k}}\right] = \frac{\delta_{S,V}\delta_{T,U}}{\binom{n}{k}},
    \end{align*}

    \noindent where $k = \abs{S} = \abs{T}$. 
\end{proof}


% Formulas


Let us denote by $\sigma_k(A)$ the coefficient of of $(-1)^{k}x^{d-k}$ in the characteristic polynomial of a $d$-dimensional matrix $A$. We will use the fact that 

\begin{equation*}
    \sigma_k(A) = \sum_{\abs{S} = k} \det[A]_{S,S}.
\end{equation*}

The following two lemmas will help us to find explicit coefficients for expected characteristic polynomials with the aid of minor orthogonality properties.

\begin{lemma} \label{lemma:conjugate_minorth}
    Let $m \le n$, $B$ an $n\times n$ random matrix and $R$ an $m\times n$ minor-orthogonal matrix independent from $B$. For all sets $S,T \subset \binom{[m]}{k}$ we have

    \begin{equation*}
        \mathbb E_{B,R} \left[ \det[RB\hermit R]_{S,T} \right] = \mathbb E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Using the Cauchy-Binet formula we have

    \begin{align*}
        \mathbb E_{B,R} \left[ \det[RB\hermit{R}]_{S,T} \right] &= \mathbb E_B \left[\sum_{X,Y \in \binom{[n]}{k}} \mathbb E_R \left[ \det[R]_{S,X} \det[B]_{X,Y} \det[\hermit{R}]_{Y,T} \right] \right],\\ 
        &= \mathbb E_B \left[\sum_{X,Y \in \binom{[n]}{k}} \det[B]_{X,Y} \mathbb E_R \left[ \det[R]_{S,X} \det[\hermit{R}]_{Y,T} \right] \right],\\ 
        &= \mathbb E_B \left[\sum_{X,Y \in \binom{[n]}{k}} \det[B]_{X,Y} \frac{\delta_{S,T} \delta_{X,Y}}{\binom{n}{k}} \right],\\ 
        &= \mathbb E_B \left[\sum_{X \in \binom{[n]}{k}} \det [B]_{X,X} \frac{\delta_{S,T}}{\binom{n}{k}} \right],\\
        &= \mathbb E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{align*}
\end{proof}


\begin{lemma}
    Let $m > n$, $A$ an $m \times m$ random matrix and $Q$ a random $m \times n$ matrix sampled from the Haar measure on the Stiefel manifold of $\mathcal M_{m,n}(\C)$, then

    \begin{equation*}
        \mathbb E_A \left[ \mathbb E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] = \mathbb E_A \left[\frac{n!}{m!} \partial_x^{(m-n)} \chi_x(A) \right],
    \end{equation*}

    \noindent where $\chi_z(\cdot)$ denotes the characteristic polynomial of $\cdot$ with $z$ as a variable.
\end{lemma}

\begin{proof}
    Let $A$ be a fixed matrix and $Q$ a matrix sampled from the left Haar measure on the Stiefel manifold $\mathcal M_{m,n}(\C)$. The $k$th coefficient of the expected characteristic polynomial of $QA\hermit Q$ is 

    \begin{align*}
        \mathbb E_Q \left[ \sigma_k(QA\hermit{Q}) \right] &= \sum_{\abs{S} = k} \mathbb E_Q\left[ \det [QA\hermit Q]_{S,S} \right],\\ 
        &= \sum_{\abs{S} = k} \frac{\sigma_k(A)}{\binom mk} = \frac{\binom{n}{k} \sigma_k(A)}{\binom{m}{k}}.
    \end{align*}

    Taking expectation with respect to $A$ in the last expression we find

    \begin{align*}
        \mathbb E_A \left[\mathbb E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] &= \mathbb E_A \left[ \frac{\binom{n}{k} \sigma_k(A)}{\binom{m}{k}} \right] = \frac{\binom{n}{k} \mathbb E_A[\sigma_k(A)]}{\binom{m}{k}},
    \end{align*}

    \noindent which is the $k$th coefficient of $\frac{n!}{m!} \frac{\d^{(m-n)}}{\d x} \mathbb E_A \left[\chi_x(A) \right]$.
\end{proof}

With these results, now we are ready to prove the main theorems relating polynomial convolutions to the expected characteristic polynomials of sums and products of random matrices.

\subsection{Relation to polynomial convolution}

Until now, we have used minor orthogonality properties to find some expressions for expected characteristic polynomials. In this section, we will see that under suitable circumstances, those expressions are actually related to polynomial convolutions. We prove the relationship for the symmetric additive and symmetric multiplicative cases. An analogous for the asymmetric additive case can be found in \cite{article:finitefree}. %\todo{Hay que corregir la introducción de la sección (y capítulo) donde dije que estas relaciones sólo se probaban en el caso simétrico aditivo.}

The relationship with Free Probability Theory will be more evident for the additive case in the next section where we introduce the $\mathcal R_n$ finite transform. For the multiplicative case a similar transform can be introduced, but it is not included in this work. 

\begin{theorem} \label{thm:symmad}
    If $p(z)$ is the characteristic polynomial of $A$ and $q(z)$ is the characteristic polynomial of $B$, where $A$ and $B$ are $n\times n$ normal matrices with complex entries, then 

    \begin{equation*}
        p(z) \boxplus_n q(z) = \mathbb E_Q\left[\chi_z(A + Q B \hermit Q)\right],
    \end{equation*}

    \noindent where $\chi_z(\cdot)$ denotes the characteristic polynomial of $\cdot$ with $z$ as a variable and $\mathbb E_Q$ denotes taking expectation over $Q$ where $Q$ is sampled from the Haar measure on the unitary complex $n\times n$ matrices.
\end{theorem}

% \begin{proof}
%     It follows directly from Theorem \ref{thm:implies_symmad} and definition of the symmetric additive convolution.
% \end{proof}

% \begin{theorem} \label{thm:implies_symmad}
%     Let $A, B$ be $n\times n$ random matrices and $R$ a $n\times n$ minor-orthogonal matrix, such that $A, B, R$ are jointly independent, then we have

%     \begin{equation*}
%         \mathbb E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{n-i}{k-i}}{\binom{n}{k-i}}\mathbb E_{A} \left[\sigma_i(A)\right] \mathbb E_{A}\left[\sigma_{k-i}(B)\right],
%     \end{equation*}

%     \noindent where $\sigma_j(X)$ represents the $j$th coefficient in the characteristic polynomial of the matrix $X$.
% \end{theorem}

\begin{proof}
    We will find the $k$th coefficient of the expected characteristic polynomial. We use that

    \begin{equation*}
        \sigma_k(A) = \sum_{\abs{S}=k} \det[A]_{S,S},
    \end{equation*}

    \noindent together with Theorem \ref{thm:marcus_binet} and Lemma \ref{lemma:conjugate_minorth} to get %\todo{Hay que aclarar las normas de matrices.}

    \begin{align*}
        \mathbb E_{A,B,R}&\left[ \sigma_k(A + RB\hermit{R}) \right] = \sum_{S \in \binom{[n]}{k}} \mathbb E_{A,B,R} \left[ \det[A + RB\hermit{R}]_{S,S} \right].\\ 
        \intertext{Denote by $\overline U$ the complement of $U$, then}
        \mathbb E_{A,B,R}&\left[ \sigma_k(A + RB\hermit{R}) \right]\\
        &= \sum_{S\in\binom{[n]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} \mathbb E_A\left[ \det [A]_{U(S),V(S)} \right] \mathbb E_{B,R}\left[ \det[RB\hermit R]_{\overline{U}(S),\overline{V}(S)} \right],\\ 
        &= \sum_{S\in\binom{[n]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} \mathbb E_A\left[ \det[A]_{U(S),V(S)} \right] \delta_{\overline{U}(S),\overline{V}(S)} \frac{\mathbb E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{n}{k-i}}.\\ 
        \intertext{Using that $U(S)=V(S)$ if and only if $\overline{U}(S) = \overline{V}(S)$,}
        \mathbb E_{A,B,R}&\left[ \sigma_k(A + RB\hermit{R}) \right] = \sum_{i=0}^k \frac{\mathbb E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{n}{k-i}} \sum_{S\in\binom{[n]}{k}} \sum_{U,V\in \binom{[k]}{i}}  \mathbb E_A\left[\det[A]_{U(S),U(S)} \right].
    \end{align*}
    To finish the proof we need to find

    \begin{equation} \label{eq:suma_rara} \sum_{S\in\binom{[n]}{k}} \sum_{U\in \binom{[k]}{i}}  \mathbb E_A\left[\det[A]_{U(S),U(S)} \right]. \end{equation}

    We are summing over all of the sets $V \in \binom{[n]}{i}$, but they appear more than once in the sum. To find the number of times every element $V\in \binom{[n]}{i}$ appears in the sum, we can count the total number of terms we are summing in \eqref{eq:suma_rara} and divide by the total number of elements in $\binom{[n]}{i}$. We have that $\abs{\binom{[n]}i} = \binom{n}{i}$ and the number of summands is $\binom{n}{k}\binom{k}{i}$, so

    \begin{align*}
        \frac{\binom{n}{k}\binom{k}{i}}{\binom{n}{i}} &= \frac{\frac{n!}{k!(n-k)!}\frac{k!}{i!(k-i)!}}{\frac{n!}{i!(n-i)!}} = \frac{(n-i)!}{(n-k)!(k-i)!} = \binom{n-i}{k-i}.
    \end{align*}

    Thus, we have

    \begin{equation*}
        \sum_{S\in\binom{[n]}{k}} \sum_{U\in \binom{[k]}{i}}  \mathbb E_A\left[ \det[A]_{U(S),U(S)} \right] = \binom{n-i}{k-i} \sum_{V\in \binom{[n]}{i}} \mathbb E_A\left[ \det[A]_{V,V} \right] = \binom{n-i}{k-i}\mathbb E_A\left[\sigma_{i}(A)\right].
    \end{equation*}

    With this, we can conclude

    \begin{equation*}
        \mathbb E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{n-i}{k-i}}{\binom{n}{k-i}}\mathbb E_{A} \left[\sigma_i(A)\right] \mathbb E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}

    Which is precisely the $k$th coefficient of the symmetric additive convolution between $\E{\chi_z(A)}$ and $\E{\chi_z(A)}$.

    %\todo{Hay que aclarar qué significa $U(S)$.}
\end{proof}





The next theorem is the analogous of the previous one for the symmetric multiplicative convolution. As it was stated previously, these convolutions can be seen as ``approximations'' to the free additive and free multiplicative convolutions.

\begin{theorem}
    Let $p(z)$ be the characteristic polynomial of $A$ and $q(z)$ be the characteristic polynomial of $B$ where $A$ and $B$ are $n\times n$ normal matrices with complex entries, then 

    \begin{equation*}
        p(z) \boxtimes_n q(z) = \mathbb E_Q \left[ \chi_z (AQBQ^*) \right],
    \end{equation*}

    \noindent with $\chi_z$ and $\mathbb E_Q$ as in Theorem \ref{thm:symmad}.
\end{theorem}

% \begin{proof}
%     It follows directly from Theorem \ref{thm:implies_symm_mult} and definition of the symmetric multiplicative convolution.
% \end{proof}


% \begin{theorem} \label{thm:implies_symm_mult}
%     Let $A$ and $B$ be $n\times n$ random matrices and $R$ a minor-orthogonal $n\times n$ matrix, such that $A,B,R$ are jointly independent, then

%     \begin{equation*}
%         \mathbb E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] = \frac{\mathbb E_A[\sigma_k(A)]\mathbb E_B[\sigma_k(B)]}{\binom{n}{k}}.
%     \end{equation*}
% \end{theorem}

\begin{proof}
Again, we find the $k$th coefficient of the expected characteristic polynomial. By the Cauchy-Binet formula and independence
    \begin{align*}
        \mathbb E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] &= \sum_{S \in \binom{[n]}{k}} \mathbb E_{A,B,R} \left[\det[ARB\hermit{R}]_{S,S}\right],\\ 
        &= \sum_{S,T \in \binom{[n]}{k}} \mathbb E_A\left[ \det[A]_{S,T} \right] \mathbb E_{B,R} \left[ \det [RB\hermit{R}]_{T,S} \right],\\ 
        \intertext{Now we use Lemma \ref{lemma:conjugate_minorth} to conclude}
        \mathbb E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] &= \sum_{S,T \in \binom{[n]}{k}} \mathbb E_A\left[\det[A]_{S,T} \right] \delta_{T,S}\frac{\mathbb E_B\left[ \sigma_k(B)\right]}{\binom{n}{k}},\\
        &= \frac{\mathbb E_A[\sigma_k(A)]\mathbb E_B[\sigma_k(B)]}{\binom{n}{k}}.
    \end{align*}
    This is the desired result.
\end{proof}




With the results in this section we related polynomial convolutions to expected characteristic polynomials. In the following section we will be able to link the convolutions to Free Probability Theory by assigning an empirical measure to the roots of a polynomial in a similar fashion one can assign a measure to the eigenvalues of a matrix.


\section{The finite \texorpdfstring{$\mathcal R_n$}{R_n} transform} 

In finite probability, the $\mathcal R$ transform linearizes free additive convolution, which means the $\mathcal R$ transform of a free additive convolution of measures is simply the sum of the $\mathcal R$ tranforms of each measure. The $\mathcal R$ transform of a measure $\mu$ is often defined as a formal series

\begin{equation*}
    \mathcal R_\mu(s) = \sum_{k=0}^\infty s^{k} r_k,
\end{equation*}

\noindent with $r_k$ being the free cumulants of the measure. For the finite free case, a similar transform can be defined, and we will prove that it converges to the $\mathcal R$ transform. %A similar finite transform converging to the corresponding free one can be defined for the multiplicative convolution, we do not include those results here. 

In the first part of this section we introduce some technical definitions and lemmas, then we define the finite $\mathcal R_n$ transform and prove its two main properties, namely that it converges to an $\mathcal R$ transform and that it linearizes symmetric additive convolution of polynomials. In the following two subsections we use the transform to find the polynomials associated to finite free distributions and later to prove the analogous in Finite Free Probability Theory to the most classical limit theorems in Probability Theory. The content of this section is taken from \cite{article:marcus_finite}.

Before stating the results in this section, it is important to introduce the notation for ``congruent polynomials''. Let $p,q$ be two polynomials on $z$, we say that $p$ is congruent with $q$ modulo $z^{n}$ if the first $n+1$ coefficients of $p$ and $q$ coincide. We will denote this relationship by

\begin{equation*}
    p(z) \equiv q(z), \qquad \mod [z^n].
\end{equation*}

If we think of a formal power series as an ``infinite polynomial'', the definition of polynomial congruence can be extended. Notice that if $f(z)$ is a formal power series, then $f(z) \mod [z^n]$ is a polynomial of degree at most $n$.

\subsection{Definition of the transform}

Given any polynomial $p(z)$ with order $p$ we can associate an empirical measure $\mu_p$ to its roots $z_i$ given by

\begin{equation*}
    \mu_p(\{x\}) = \frac1n\sum_{j=1}^p \delta_{x,z_j}.
\end{equation*}

This measure is similar to the spectral empirical measure of a random matrix. We can find its Cauchy transform in terms of the polynomial with the following lemma.

\begin{lemma} \label{lemma:cauchy_empirical_polynomial}
    Let $p$ be a monic polynomial of order $p$ with roots $\{z_i\}_{i=1}^n$, then the Cauchy transform of the empirical measure associated to the roots $z_i$ is given by 

    \begin{equation*}
        G_{\mu_p}(z) \coloneqq \frac1n \sum_{j=1}^n \frac1{z - z_j} = \frac{\partial_z p }{n p}(z) = \frac1n \partial_z \ln p(z).
    \end{equation*}

\end{lemma}

\begin{proof}
    $p(z)$ is a monic polynomial with roots $\left\{ z_j \right\}_{j \in [n]}$, then we can write,

    \begin{equation*}
        p(z) = \prod_{j=1}^n (z-z_j).
    \end{equation*}

    By the Leibnitz rule we find

    \begin{equation*}
        \partial_z p(z) = \sum_{j=1}^n \prod_{k\neq j} (z-z_k).
    \end{equation*}

    Using the last equation we have

    \begin{align*}
        \frac{\partial_z p}{n p}(z) &= \frac1n\sum_{j=1}^n \frac{\prod_{k\neq j} (z-z_k) }{ \prod_{l=1}^n (z-z_l) } = \frac1n\sum_{j=1}^n \frac{1}{z - z_j} \eqqcolon G_{\mu_p}(z).
    \end{align*}
\end{proof}

Now we will define two objects that are auxiliary in the following proofs. Both are named transforms, but it is important to remark that one of them is an operator acting on functions while the other is a multiset which is uniquely related to another multiset in some way.

The existence of the $U$ transform can be stated by the following lemma that we state without proof. The proof can be found in \cite{anaya2016cumulantes}.

\begin{lemma}[$U$ transform]
    Let $S$ be a multiset of complex numbers and denote by $\abs S=n$ its number of elements with multiplicity. Then there exists an unique multiset of complex numbers $T$ such that $\abs{T}=n$ and 

    \begin{equation*}
        \prod_{s_i \in S} (x-s_i) = \frac 1n \sum_{t_i \in T} (x-t_i)^n.
    \end{equation*}
    We call such multiset the $U$ transform of $S$.
\end{lemma}

We will use the $U$ transform of a set of eigenvalues (or roots of a polynomial) in order to find explicit expressions for the $\mathcal R_n$ transform and prove some of its properties. 

The Legendre transform, and the properties that will be proven later, will be useful to prove that the finite $\mathcal R_n$ transform converges to an $\mathcal R$ transform.

\begin{definition}[Legendre's transform]
    Let $f$ a convex function in a domain $D\subset \R$ and define

    \begin{equation} \label{eq:legendre_set}
        D^* \coloneqq \left\{ x^* \in \R \mathrel{:} \sup_{x \in D} \{xx^* - f(x)\} < \infty \right\}.
    \end{equation}

    We define $f^*$ the Legendre transform of $f$ as the function

    \begin{align*}
        f^* : D^* &\to \R.\\
        s \mapsto &\sup_{x \in D} \{ xs - f(x) \}.
    \end{align*}
\end{definition}

\begin{lemma} \label{lemma:strictly_convex}
    Let $f$ be a strictly convex function in a domain $D \subset \R$ and such that its derivative exists in a point $x_0 \in D$. Then $\left.\partial_x [f(x)]\right|_{x=x_0} \in D^*$ and 

    \begin{equation*}
        f^*(f'(x_0)) = x_0 f'(x_0) - f(x_0).
    \end{equation*}
If additionally, $f$ has a second derivative, then the following two results are satisfied

    \begin{align*}
        (f')^{-1}(x_0) &= (f^*)'(x_0), \\
        f''((f^*)'(x_0)) &= \frac{1}{(f^*)''(x_0)}.
    \end{align*}
\end{lemma}


\begin{proof}
    Since $f$ is strictly convex and differentiable at $x_0$ we have for $x\in D, x\neq x_0$ that 
    
    \begin{align*}
        f(x) &> f(x_0) + (x-x_0)f'(x_0), 
\intertext{resting $xf'(x_0)$ in both sides leads to}
        f(x) -xf'(x_0) &> f(x_0) - x_0 f'(x_0),
\intertext{the inequality is reversed when we multiply by $-1$;}
        xf'(x_0) - f(x) &< x_0 f'(x_0) - f(x_0).
    \end{align*}

    This means that 

    \begin{equation*}
        \sup_{x\in D}\{ xf'(x_0) - f(x) \} = x_0f'(x_0) - f(x_0) < \infty.
    \end{equation*}

    Then, by the definition of $D^*$, we have that $f'(x_0) \in D^*$ and the Legendre transform in this point is $f^*(f'(x_0)) = x_0f'(x_0) - f(x_0)$.

    For the second part, if $f$ has a second derivative at $x_0$, differentiate the last equation to find

    \begin{equation*}
        (f^*)'(f'(x_0))f''(x_0) = x_0 f''(x_0) + f'(x_0) - f'(x_0) = x_0 f''(x_0). 
    \end{equation*}

    The second derivative $f''(x_0)$ can not be zero because $f$ is strictly convex, and therefore $(f^*)'(f'(x_0)) = z$, which means $f^*$ and $f'$ are inverse under composition (in any point where $f$ is twice differentiable). For the second equation we use the fact that these functions are inverse and derive,

    \begin{align*}
        f'((f^*)'(z)) &= z,
        \intertext{deriving with respect to $z$ gives us}
        f''((f^*)(x_0))(f^*)''(x_0) &= 1,
        \intertext{when we divide both sides by $(f^*)''(x_0)$}
        f''((f^*)(x_0)) &= \frac1{(f^*)''(x_0)}.
    \end{align*}
\end{proof}

    The following theorem relates the Legendre transform to an $L^\infty$ norm and will be used in the proof of the convergence of the finite $\mathcal R_n$ transform to an $\mathcal R$ transform.

\begin{lemma} \label{lemma:legendre_transform_norm}
    Let $D \subset \R$ and $\mu$ a measure that is absolutely continuous with respect to the Lebesgue measure, then for any continuous function $f: D \to \R$ we have that

    \begin{equation*}
        f^*(s) = \ln \norm{ e^{xs - f(x)} }_{\infty},
    \end{equation*}

    \noindent for all $s \in D^*$ where the Legendre transform and the norm are taken over $D$.
\end{lemma}

\begin{proof}
    We can write $f^*$ as $f^*(s) = \ln(\exp(f^*(s)))$, then

    \begin{align*}
        f^*(s) &= \ln(\exp(f^*(s))) = \ln \left(\exp\left\{ \sup_{x\in D} \{xs-f(x)\} \right\}\right), \\
        \intertext{using that $\exp(x)$ is monotone increasing,}
        &= \ln \left( \sup_{x \in D} \exp(xs - f(x))\right) = \ln \norm{ e^{xs-f(x)} }_{\infty}.
    \end{align*}
\end{proof}


Just as in the free case, the finite $\mathcal R_n$ transform can be defined in terms of another transform that we will call the finite $\mathcal K_n$ transform.


\begin{definition}[The $\mathcal K_n$ transform \cite{anaya2016cumulantes}]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal K_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal K_n^{\mu_A} (s) \coloneqq - \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+ [x I - A] }_n,
    \end{equation*}

    \noindent where $\Delta^+$ represents the normalized determinant
    
    \begin{equation*}
        \Delta^+[A] = \left\{ \begin{array}{cc}
            \det[A]^{\frac1n} & \text{if $A$ is positive definite,}\\
            0 & \text{otherwise.}
        \end{array} \right.
    \end{equation*}
    
    \noindent and the integration domain for the norm is $(\rho_A, \infty)$ with $\rho_A$ the spectral radius of $A$.
\end{definition}

The operator $\Delta^+$ acting on $A$ is a particular case of a much more general object called the Fuglede-Kadison determinant, which acts on a broad set of operators and satisfies many of the properties of determinants. More information about it can be found in \cite{article:fuglede_kadisen_determinant}.

With the finite $\mathcal K_n$ transform it is possible to define the finite $\mathcal R_n$ transform. 

\begin{definition}[The $\mathcal R_n$ transform]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal R_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal R_n^{\mu_A} (s) = \mathcal K_n^{\mu_A}(s) - \left( 1 + \frac1n \right) \frac1s.
    \end{equation*}
\end{definition}

Notice that for bigger $n$, the definition above is more similar to the definition of the $\mathcal R$ transform in terms of the $\mathcal K$ transform given in Chapter \ref{ch:preliminaries}. If we assume that $\mathcal K_n^{\mu_A}(s)$ converges to some $\mathcal K$ transform when $n\to\infty$, then $\mathcal R_n^{\mu_A}(s)$ would converge to the corresponding $\mathcal R$ transform. In the next theorem we will prove exactly this convergence.

\begin{theorem} Let $A$ be a self-adjoint $n \times n$ matrix with empirical spectral distribution $\mu_A$, then

    \begin{equation*}
        \lim_{n\to\infty} \mathcal K_n^{\mu_A} (s) = G_{\mu_A}^{-1}(s),
    \end{equation*}
\noindent with $s \in (\rho_A, \infty)$ and where $G_{\mu_A}^{-1}(s)$ is the inverse under composition of $G_{\mu_A}(s)$.
\end{theorem}

\begin{proof}
    
    We begin defining the function $g(x) \coloneqq -\ln \Delta^+[xI - A]$ and let $\lambda_1, \dots, \lambda_n$ be the ordered eigenvalues of $A$. Notice that without loss of generality we can assume $I-A$ to be positive definite. When we differentiate $g$ with respect to $x$, we find 

    \begin{align*}
        \partial_x [g(x)] &= \partial_x \left[-\ln \Delta^+[xI - A]\right] = -\partial_x \left\{ \ln\left[ \left(\prod_{j=1}^n (x - \lambda_j) \right)^{\frac1n}\right] \right\}, \\
        &= -\partial_x \left\{\ln \left[ \prod_{j=1}^n (x-\lambda_j)^{\frac1n} \right]\right\} = - \partial_x \left[\sum_{j=1}^n \frac1n \ln (x-\lambda_j)\right],\\ 
        &= -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} = - G_{\mu_A}(x).
    \end{align*}

    For the second derivative we have

    \begin{align*}
        \partial_{xx} [g(x)] = \partial_x \left\{\left[ -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} \right] \right\}= \frac1n \sum_{j=1}^n \frac{1}{(x-\lambda_j)^2} > 0.
    \end{align*}

    So $g(x)$ is strictly convex and has a second derivative. Using Lemma \ref{lemma:strictly_convex} we get 

    \begin{equation} \label{eq:inverse_cauchy}
        (g^*)'(x) = (g')^{-1}(x) = (-G_{\mu_A})^{-1}(x) = G_{\mu_A}^{-1}(-x).
    \end{equation}

    Now we find $(g^*)'(x)$ with the help of Lemma \ref{lemma:legendre_transform_norm}.

    \begin{align*}
        (g^*)'(s) &= \frac{\partial}{\partial s} \ln \norm{ e^{xs} - g(x) }_\infty = \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+(xI - A) }_\infty.
    \end{align*}

    Substituting the last in \eqref{eq:inverse_cauchy} we get 

    \begin{equation*}
        G_{\mu_A}^{-1}(s) = (g^*)'(-s) = -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    On the other hand, the limit of $\mathcal K_n^{\mu_A}$ is 

    \begin{equation*}
        \lim_{n\to\infty}  \mathcal K_n^{\mu_A}(s) = - \lim_{n\to\infty} \frac{\partial}{\partial s} \ln \norm{ e^{xs}\Delta^+(xI - A) }_n =  -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    This gives the desired result.
\end{proof}


The simplest finite $\mathcal K_n$ transform we can find is the one related to the measure $\mu_0 = \delta_0$. Knowing this transform will also be useful for proving further results. 

\begin{lemma}
    Let $\mu_0$ be the probability measure corresponding to an atom in $0$ with probability 1, i.e.

    \begin{equation*}
        \mu_0(\{x\}) = \left\{ \begin{array}{cc}
            1, & \text{si $x=0$},\\
            0, & \text{otherwise.}
        \end{array} \right.
    \end{equation*}
Then its finite $\mathcal K_n$ transform is

    \begin{equation*}
        \mathcal K_n^{\mu_0} (s) = \left( 1 + \frac1n \right)\frac1s.
    \end{equation*}
\end{lemma}

\begin{proof}
    Notice that for every $m$, the measure $\mu_0$ is the empirical spectral measure associated to the matrix $0_{n\times n}$, so we can write

    \begin{align*}
        \norm{ e^{-xs}\Delta^+ (xI) }_n &= \norm{ e^{-xs}(x^n)^{\frac1n} }_n.
        \intertext{Remember the integral is taken from $\rho_A$ to $\infty$. In this case all the eigenvalues are 0, so}
        \norm{ e^{-xs}\Delta^+ (xI) }_n &= \left( \int_0^\infty (e^{-xs}x)^n \right)^{\frac1n} = (\mathcal L[x^n](ns))^{\frac1n},
        \intertext{where $\mathcal L[f]$ represents the Laplace transform of the function $f$. With this, we have}
        \norm{ e^{-xs}\Delta^+ (xI) }_n &= \left( \frac{n!}{(ns)^{n+1}} \right)^{\frac1n}.
    \end{align*}

    The logarithm of the previous expression is

    \begin{equation*}
        \ln \left( \frac{n!}{(ns)^{n+1}} \right)^{\frac1n} = \frac1n \ln \left( \frac{n!}{(ns)^{n+1}} \right) = \frac1n \log\left( \frac{n!}{(n)^{n+1}}\right) - \frac{n+1}{n} \log s.
    \end{equation*}
    Finally, we differentiate to find the desired expression.

    \begin{equation*}
        \ln \left(  \frac1n \log\left( \frac{n!}{(n)^{n+1}}\right) - \frac{n+1}{n} \log s \right) = - \partial_s\left[ - \frac{n+1}{n} \log s \right] = \left(\frac{n+1}{n}\right) \frac1s.
    \end{equation*}
\end{proof}

Notice that once we have this, an alternative definition for $\mathcal R_n^{\mu_A}(s)$ would be $\mathcal R_n^{\mu_A}(s) = \mathcal K_n^{\mu_A}(s) - \mathcal K_n^{\mu_0}(s)$.

Our next goal is to prove that the finite $\mathcal R_n$ transform linearizes the symmetric additive convolution. In order to do so, we will first prove a couple basic lemmas.

\begin{lemma} \label{lemma:Utrans_convolution}
    Let $p,q$ be polynomials with degree $n$ and $U_p,U_q$ be the $U$ transforms of its sets of roots, then

    \begin{equation*}
        [p \boxplus_n q](x) = \frac1{n^2} \sum_{u_j \in U_p, u_k \in U_q} (x - u_j -u_k)^n.
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $p(x) = \sum_{j=0}^n x^{n-j}(-1)^j p_j$. Let $x_i$ be its roots, by definition of the $U$ transform we have,

    \begin{equation*}
        p(x) = \prod_{j=0}^n (x - x_j) = \frac1n \sum_{u_j \in U_p} (x-u_j)^n = \sum_{j=0}^n x^{n-j} (-1)^j \binom{n}{j} \frac{1}{n} \sum_{u_k \in U_p} u_k^j.
    \end{equation*}

    Equating coefficients we find that $p_j = \binom{n}{j} \frac{1}{n} \sum_{u_k \in U_p} u_k^j$ and the analogous happens for $q_k$. Finally, we find the convolution

    \begin{align*}
        [p\boxplus_n q](x) &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{(n-i)!(n-j)!}{(n-k)!n!} \binom{n}{i}\binom{n}{j} \frac{1}{n} \sum_{u_m \in U_p} u_m^i \frac{1}{n} \sum_{u_l \in U_q} u_l^j ,\\%\E{U_p^i} \E{U_q^j},\\
        &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{n!}{i!j!(n-k)!} \frac1{n^2}  \sum_{u_m \in U_p, u_l \in U_q} u_m^i u_l^j, \\ %\E{U_p^i U_q^j}, \\
        &= \frac1{n^2}\sum_{u_m \in U_p, u_l \in U_q}\sum_{k=0}^n \binom nk x^{n-k}(-1)^k \sum_{j=0}^k \binom{k}{j}u_m^{k-j} u_l^j, \\
        &= \frac1{n^2}\sum_{u_m \in U_p, u_l \in U_q} \sum_{k=0}^n \binom nk x^{n-k}(-1)^k (u_m + u_l)^k,\\%\E{(U_p + U_q)^k},\\
        &= \frac1{n^2}\sum_{u_m \in U_p, u_l \in U_q} (x-u_m-u_l)^n. %&= \E{ (x - U_p - U_q)^n }. 
    \end{align*}
\end{proof}






\begin{lemma}  \label{lemma:quotient_norms}
    Let $A$ be an $n\times n$ real symmetric matrix. Denote by $\lambda(A)$ its spectrum and by $U_A$ the $U$ transform of $\lambda(A)$, then 

    \begin{equation*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \equiv \frac1n\sum_{u_i \in U_A} e^{-ns u_i}, \qquad \mod [s^{n+1}]. 
    \end{equation*} 
\end{lemma}

\begin{proof}
    Suppose, without loss of generality that $A$ is positive definite. By definition of the $U$ transform, for any $n\times n$ such symmetric matrix $A$ we have

    \begin{equation*}
        \Delta^+(xI - A)^n = \det[xI - A] = \prod_{j=1}^n (x - \lambda_j) = \frac1n \sum_{u_i \in U_A} (x-u_i)^n.
    \end{equation*}

    Using this and the previously found result for $\norm{e^{-xs}\Delta^+(xI)}_n^n$ we find

    \begin{align*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac{(ns)^{n+1}}{n!}\norm{e^{-xs}\Delta^+(xI - A)}_n^n = \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \Delta^+(xI - A)^n \d x.
        \intertext{By definition of the $U$ transform}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \frac1n \sum_{u_i \in U_A} (x-u_i)^n \d x.
        \intertext{Use the change of variables $y = x + \rho_A$}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac{(ns)^{n+1}e^{-ns\rho_A}}{n!}\int_{0}^\infty e^{-nsy} \frac1n \sum_{u_i \in U_A} (y + \rho_A-u_i)^n \d y. \\
        \intertext{Using linearity and the definition of the Laplace transform}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \mathcal L\{(y + \rho_A - U_A)^n\}(ns).
        \intertext{By linearity of the Laplace transform}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &=  \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \binom nk  (\rho_A - u_i)^{n-k}\mathcal L\{y^k\}(ns).
        \intertext{Finally, we expand and cancel terms to arrive to}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n}
        &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \frac{n!}{k!(n-k)!}  (\rho_A - u_i)^{n-k}\frac{k!}{(ns)^{k+1}}, \\
        &= \frac1n \sum_{u_i \in U_A} e^{-ns\rho_A} \sum_{k=0}^n \frac{(ns)^{n-k}(\rho_A - u_i)^{n-k}}{(n-k)!}.\\
        \intertext{We can use the polynomial congruence relation for the series expansion of the exponential function and the truncated series $\sum_{k=0}^n x^n/n!$.}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &\equiv \frac1n \sum_{u_i\in U_A} e^{-ns\rho_A} e^{ns(\rho_A - u_i)}, \quad \mod [s^{n+1}],\\
        &\equiv \frac1n \sum_{u_i\in U_A} e^{-nsu_i}, \quad \mod [s^{n+1}].
    \end{align*}
\end{proof}

With the previous result in hand, it is now easier to find the finite $\mathcal R_n$ transform of a spectral measure in terms of the $U$ transform of its spectrum.


\begin{corollary} \label{corollary:R_n_as_a_logarithm}
    Let $A$ be a an $n\times n$ symmetric matrix with real entries, with spectrum $\lambda(A)$ and $U_A$ be the $U$ transform of $\lambda(A)$, then 

    \begin{equation*}
        \mathcal R_{n}^{\mu_A}(s) \equiv - \frac1n \frac{\partial}{\partial s} \ln \left(\frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right), \qquad \mod [s^n].
    \end{equation*}
\end{corollary}

\begin{proof}
    Taking logarithm in both sides of Lemma \ref{lemma:quotient_norms} we get 

    \begin{equation*}
        \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \right) \equiv \ln\left( \frac1n\sum_{u_i \in U_A} e^{-ns u_i} \right), \qquad \mod [s^{n+1}].
    \end{equation*}

    The first $n+1$ coefficients of the power series coincide, so the first $k$ coefficients of the derivatives also coincide. Now we use the definition of the $\mathcal R_n$ transform

    \begin{align*}
        \mathcal R_{n}^{\mu_A}(s) &= \mathcal K_{n}^{\mu_A}(s) - \mathcal K_n^{\mu_0}(s), \\
        &= - \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI -A) }_n + \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI) }_n,\\
        &= - \frac1n \frac{\partial}{\partial s} \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI -A)}_n^n}{ \norm{e^{-xs}\Delta^+(xI)}_n^n } \right), \\
        &\equiv -\frac1n \frac{\partial}{\partial s} \ln \left( \frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right), \qquad \mod[s^n].
    \end{align*}
\end{proof}

One of the central results in this section is the equivalence of the symmetric additive convolution of two polynomials and the sum of the finite $\mathcal R_n$ transform of the measures associated to its roots (which is in turn the empirical measure associated to the eigenvalues of some matrix). The following theorem shows us that the analogy between the finite $\mathcal R_n$ transform and the $\mathcal R$ transform is not only in terms of their convergence, but also in the role of linearizing convolution. 

\begin{theorem}
    Let $A$ and $B$ be two $n\times n$ real symmetric matrices. The following are equivalents:

    \begin{itemize}
        \item \[ \mathcal R_n^{\mu_A}(s) + \mathcal R_n^{\mu_B}(s) = \mathcal R_n^{\mu_{A+B}}(s), \quad \mod[s^n]. \]
        \item \[ \det[xI - A] \boxplus_n \det[xI-B] = \det[xI - A - B]. \]
    \end{itemize}
\end{theorem}

\begin{proof}
    Let us denote by $U_A, U_B$ and $U_{A+B}$ the $U$ transforms of the $\lambda(A), \lambda(B)$ and $\lambda(A+B)$, respectively. By Lemma \ref{lemma:Utrans_convolution}, the second statement is equivalent to 

    \begin{equation*}
        \E{ (x - U_A - U_B)^n } = \E{ (x - U_{A+B})^n }.
    \end{equation*}

    Expanding the power and equating terms, the last happens if and only if the first $m$ moments of $U_A + U_B$ and $U_{A+B}$ coincide. This is in turn equivalent to 

    \begin{equation*}
        \E{e^{-ms(U_A + U_B)}} = \E{e^{-ms(U_A + U_B)}} = \E{ e^{-msU_{A+B}} }, \qquad \mod [s^{n+1}].
    \end{equation*}

    Define the function $f_A(s) \coloneqq - \frac1n \ln \E{ e^{-msU_A} }$ and similarly for $B$ and $A+B$. Using this function, the second statement is equivalent to 

    \begin{equation} \label{eq:f_matrix_module}
        f_A(s) + f_B(s) \equiv f_{A+B}(s), \qquad \mod [s^{n+1}].
    \end{equation}

    Using Corollary \ref{corollary:R_n_as_a_logarithm}, the first statement is equivalent to 

    \begin{equation} \label{eq:differential_matrix_module}
        \frac{\partial}{\partial_s} f_A(s) + \frac{\partial}{\partial_s} f_B(s) \equiv \frac{\partial}{\partial_s} f_{A+B}(s), \qquad \mod[s^n].
    \end{equation}

    So proving the equivalence of the two statements reduces to prove the equivalence between \eqref{eq:f_matrix_module} and \eqref{eq:differential_matrix_module}. Because the formal series in \eqref{eq:f_matrix_module} coincide up to the $(n+1)$th term, the derivatives coincide up to the $n$th term, so one implication is trivial. For the second implication, it suffices to prove that $f_A(0) + f_B(0) = f_{A+B}(0)$, but we have that $f_A(0)=f_B(0)=f_{A+B}(0)$, so the result follows.
\end{proof} 

As it was mentioned several times before in this chapter, the relationship between symmetric additive convolution and free additive convolution shown by these results is not exclusive. Similar results can be given for the symmetric multiplicative convolution and free multiplicative convolution. Although we are not including them here, they can be consulted in \cite{marcus2021polynomial} and \cite{article:finitefree}.

For the last part of this chapter, we will use the recently exposed tools to find the polynomials corresponding to three classical laws in the finite free setting; namely the constant, Gaussian and Poisson distributions. After that, we will prove the finite free version of the three corresponding limit theorems; the law of large numbers, the central limit theorem and the Poisson limit theorem.

\subsection{Basic distributions in Finite Free Probability}

We have seen that the finite $\mathcal R_n$ transform is a power series that linearizes finite free convolution. In free probability terms, we can say that the corresponding coefficients in the expansion are the finite free cumulants. Using this and the fact that we know what are the cumulants for several well known distributions, we can find the polynomials corresponding to these laws in Finite Free Probability Theory. As a prerequisite, we prove a theorem that allows us to recover a polynomial from the finite $\mathcal R_n$ transform of its roots.

\begin{theorem} \label{thm:if_and_only_if}
    Let $P(n^{-1}\partial_z)$ be a polynomial on $n^{-1}\partial_z$ such that the linear differential operator applied to $x^n$, $P(n^{-1}\partial_z)[z^n]$ is a monic polynomial. Then $P(n^{-1}\partial_z) = \det[zI - A]$ for a matrix $A$ if and only if 

    \begin{equation} \label{eq:cauchy_transform_R}
        \frac1n \frac{\partial_s P(s)}{P(s)} \equiv - \mathcal R_n^{\mu_A}(s), \qquad \mod [s^{n+1}].
    \end{equation}
\end{theorem}

\begin{proof}
    Recall that

    \begin{equation*}
        \mathcal R_{n}^{\mu_A}(s) \equiv - \frac1n \frac{\partial}{\partial s} \ln \left(\frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right), \qquad \mod [s^n].
    \end{equation*}

    Also 

    \begin{align*}
        \frac1n \frac{\partial_s P(s)}{P(s)} &= \frac1n \partial_s[\ln P(s)].
    \end{align*}

    Then \eqref{eq:cauchy_transform_R} is satisfied if and only if 

    \begin{equation*}
        P(s) \equiv \frac1n \sum_{u_i \in U_A} e^{-nsu_i}, \qquad \mod [s^{n+1}].
    \end{equation*}

    The last relationship in turn is equivalent to 

    \begin{equation*}
        P(n^{-1}\partial_z)[z^n] = \left( \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z} \right)[z^n] = \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z}[z^n].
    \end{equation*}

    Expanding the last term and using the definition of $U_A$, we find

    \begin{align*}
        \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z}[z^n] &=  \frac1n \sum_{u_i \in U_A} \sum_{j=0}^n \frac{(-u_i)^j}{j!} \partial_z^j[z^n],\\
        &= \frac1n \sum_{u_i \in U_A} \sum_{j=0}^n (-u_i)^j \frac{n!}{j!(n-j)!}z^{n-j} = \frac1n \sum_{u_i \in U_A} (z - u_i)^n = \det[zI - A].
    \end{align*}\end{proof}

    With this theorem, we are ready to find the polynomials corresponding to the basic finite free distributions.

\subsubsection{Constant distribution}

A constant random variable has a first cumulant equal to $c \in \R$ and all the remaining cumulants equal to zero. Then its $\mathcal R_n^{\mu_A}$ transform is the constant formal series

\begin{equation*}
    \mathcal R_n^{\mu_A}(s) = c.
\end{equation*}

We can use Theorem \ref{thm:if_and_only_if} to find the polynomial $p$ with this $\mathcal R_n^{\mu_A}$ transform.

\begin{align*}
    -c &= \frac{1}{n}\partial_s[\ln P(s)],\\
    \Rightarrow -\int nc\d s &= \ln P(s),\\
    \Rightarrow \exp\left\{-ncs + c_0\right\} &= P(s),
\end{align*}

\noindent for some constant $c_0$. Then $p(z)$ is the monic scaling of the polynomial $P(n^{-1}\partial_z)[z^n] = e^{-c\partial_z}[z^n]$. Expanding we find

\begin{align*}
    p(z) &= e^{-c\partial_z}[z^n] = \sum_{k=0}^n \frac{(-c)^k}{k!}\partial_z^k[z^n],\\
    &= \sum_{k=0}^n \frac{n!}{k!(n-k)!}(-c)^k z^{n-k} = (z-c)^n.
\end{align*}

Thus, as we would expect, $p$ is the monic polynomial with all the roots equal to $c$ which is also the expected characteristic polynomial of the deterministic matrix $cI$.

\subsubsection{Gaussian distribution}

In free and classical probability, the central limit random variable is characterized for having the first two cumulants non-zero and the rest of the cumulants equal to zero. We already know what is the polynomial corresponding to a constant distribution in finite free probability. To find the general polynomial corresponding to a Gaussian, we can find the polynomial with the second cumulant non-zero and the rest equal to zero. If the second cumulant is equal to $\sigma^2$, then the formal series expansion of $\mathcal R_n^{\mu_A}$ is

\begin{equation*}
    \mathcal R_n^{\mu_A}(s) = s\sigma^2.
\end{equation*}

We can use Theorem \ref{thm:if_and_only_if} again to find

\begin{align*}
    \frac1n \partial_s[ \ln P(s) ] &= -s\sigma^2,\\ 
    \Rightarrow \ln P(s) &= -n\sigma^2 \int s \d s,\\
    \Rightarrow P(s) &= \exp\left\{ -n \sigma^2\frac {s^2}2 + c_0  \right\},
\end{align*}

\noindent where the constant $c_0$ accounts for making the polynomial monic. Now, evaluating $P$ in $n^{-1}\partial_z$ and applying to $z^n$, we have

\begin{align*}
    p(z) &= P(n^{-1}\partial_z)[z^n] = e^{-\sigma^2\frac{\partial_z^2}{2n}}[z^n] \eqcolon H_n(z,\sigma^2/n). 
\end{align*}

The last expression corresponds to the Hermite polynomial with variance $\sigma^2/n$.

\subsubsection{Poisson distribution}

The Poisson distribution has all of its cumulants equal to a constant $\nu \in \R$, the formal series expansion of its $\mathcal R_n^{\mu_A}$ transform is 

\begin{align*}
    \mathcal R_n^{\mu_A}(s) &= \sum_{k=0}^\infty \nu s^n = \nu \sum_{k=0}^\infty s^n = \frac{\nu}{1-s}.
\end{align*}

Where the convergence is taken in $\abs{s} < 1$. Another use of Theorem \ref{thm:if_and_only_if} leads to

\begin{align*}
    \frac{\nu}{1-s} &= \mathcal R_n^{\mu_A}(s) = -\frac1n\partial_s[\ln P(s)], \\
    \Rightarrow -\int \frac{n\nu}{s-1} \d s &= \ln P(s),\\
    \Rightarrow \exp\left\{ n\nu \ln (1-s) \right\} &= P(s).
\end{align*}

Finally, we evaluate in $n^{-1}\partial_z$ and apply to $z^n$ and find the polynomial that has the given cumulants.

\begin{align} \label{eq:ff_poisson}
    P(n^{-1}\partial_z)[z^n] &= \left( 1 - \frac{\partial_z}{n} \right)^{n\nu}[z^n] \eqcolon L_n^{(\nu-1)n}(z).
\end{align}

So the finite free Poisson distribution corresponds to a generalized Laguerre polynomial. Notice that when $\nu=1$, we recover the original definition of the Laguerre polynoamials given in Section \ref{sec:polynomials}.

\subsection{Finite Free Limit Theorems}

One of the central topics in Probability Theory is the convergence of sums of random variables to some laws. In Free Probability we have analogous convergence theorems for the main distributions. Three of the most famous limit theorems have the previously shown laws as limits (constant, Gaussian, Poisson). In non-commutative Probability Theory in general, these laws are characterized by their cumulants and happen to be limits of the corresponding sums of random variables. In what follows, we will show that when we re-scale and convolute polynomials under standard assumptions, we also recover our three limit laws.

\begin{theorem}[Finite Free Law of Large Numbers]
    Let $(p_i(z))_{i=1}^n$ be a sequence of degree $n$ monic polynomials with real roots $r_{ij}$ such that

    \begin{equation*}
        p_i(z) = \prod_{j=1}^n (z - r_{ij}).
    \end{equation*}

    And for every fixed $i$,

    \begin{align*}
        \frac1n\sum_{j=1}^n r_{ij} &= m,\\
        \frac1n\sum_{j=1}^n r_{ij}^2 &< c,
    \end{align*}

    \noindent for some positive constant $c$. Define $q_i(k,z) \coloneqq k^{-n}p_i(kz)$. Then

    \begin{equation*}
        \lim_{k\to \infty} [ q_1(k,z) \boxplus_n q_2(k,z) \boxplus_n \cdots \boxplus_n q_k(k,z) ] = (x-m)^n.
    \end{equation*}
\end{theorem}

\begin{proof}
    Write for every $i$ the polynomial $p_i(z)$ as

    \begin{equation*}
        p_i(z) = z^n + a_{i1} z^{n-1} + \cdots 
    \end{equation*}

    If $P_i(\partial_z)$ is a linear operator that acts on $z^n$ to generate $p_i(z)$, then 

    \begin{equation*}
        P_i(\partial_z) = 1 + a_{i1} n \partial_z + \cdots 
    \end{equation*}

    Since the coefficient $a_{i1}$ is the sum of the roots, by hypothesis we have that $a_{i1} = m$ for every $i$. Now, the last equation implies that if $q_i$ is generated by a linear differential operator $Q_i(\partial_z)$, then

    \begin{equation*}
        Q_i(\partial_z) = 1 - \frac{m}{k}\partial_z + O(k^{-2}).
    \end{equation*}

    Using the multiplicative Theorem \ref{thm:multiplicative_operators} we find

    \begin{align*}
        \lim_{k\to \infty} [ q_1(k,z) \boxplus_n q_2(k,z) \boxplus_n \cdots \boxplus_n q_k(k,z) ] &= \lim_{k\to\infty}\prod_{j=1}^k Q_j(\partial_z)[z^n],\\
        &= \lim_{k\to\infty} \left( 1 - \frac{m}{k}\partial_z + O(k^{-2}) \right)^k[z^n],\\
        &= e^{-m\partial_z}[z^n] = (z-m)^n.
    \end{align*}
\end{proof}

\begin{theorem}[Finite Free Central Limit Theorem \cite{marcus2021polynomial}]
    Let $p_1, p_2, \dots$ be a sequence of degree $n$ real rooted polynomials with $p_i = \prod_j (x - r_{i,j})$ such that

    \begin{equation} \label{eq:hypotheses_ffclt}
        \sum_{j=1}^n r_{i,j} = 0, \qquad \frac1n \sum_{j=1}^n r^2_{i,j} = \sigma^2,
    \end{equation}

    \noindent for all $i$. Define $q_i(z) = k^{-n/2}p_i(\sqrt{k}z)$, then 

    % \begin{equation*}
    %     \lim_{n\to\infty} \left( q_1 \boxplus_d \cdots \boxplus_d q_n \right) = \left( \frac{d-1}{\sigma^2} \right)^{-d/2} H_d\left( x \sqrt{\frac{d-1}{\sigma^2}} \right),
    % \end{equation*}

    \begin{equation*}
        \lim_{k\to\infty} \left( q_1 \boxplus_n \cdots \boxplus_n q_k \right) = H_n(z,\sigma^2/(n-1)).
    \end{equation*}

    \noindent with $H_n(z,\sigma^2/(n-1))$ represents the $n$th Hermite polynomial with variance $\sigma^2/(n-1)$.
\end{theorem}


\begin{proof}
    Using the Vieta's formulas and the hypotheses \eqref{eq:hypotheses_ffclt}, we have that for every $i$ it is satisfied

    \begin{align*}
        a_1 &= \sum_{j=1}^n r_{ij} = 0,\\
        a_2 &= \sum_{1 \le j < m \le n} r_{ij}r_{im} = \frac12 \sum_{j=1}^n r_{ij}\sum_{m\neq j} r_{im} = \frac12 \sum_{j=1}^n r_{ij} \left(a_1 - r_{ij}\right),\\
        &= - \frac12 \sum_{j=1}^n r_{ij}^2 = - \frac12 n\sigma^2.
    \end{align*}

    So every $p_i$ has the form

    \begin{equation*}
        p_i(z) = z^n + (0)z^{n-1} - \frac{n\sigma^2}{2n(n-1)} z^{n-2} + \cdots
    \end{equation*}

    Multiplying by the factors to get $q$, we get

    \begin{align*}
        q_i(z) &= z^n + (0)z^{n-1} - \frac{n\sigma^2k^{-n/2}}{2n(n-1)} k^{\frac{n-2}{2}}z^{n-2} + \cdots ,\\
        &= z^n - \frac{\sigma^2}{2(n-1)k}z^{n-2}.
    \end{align*}

    This means that the linear differential operator $Q_i(\partial_z)$ that generates $q_i$ must have the form

    \begin{equation*}
        Q_i(\partial_z)[z^n] = \left( 1 - \frac{\sigma^2}{2(n-1)k}\partial_z^2 + O(k^{-3/2})\right)[z^n].
    \end{equation*}

    We recall the multiplicative Theorem \ref{thm:multiplicative_operators} to find,

    \begin{align*}
        q_1 \boxplus_n \cdots \boxplus_n q_k &= \prod_{j=1}^k Q_j(\partial_z)[z^n], \\
        &= \left( 1 - \frac{\sigma^2}{2(n-1)k}\partial_z^2 + O(k^{-3/2})\right)^k[z^n].
    \end{align*}

    Finally, letting $k\to\infty$, we can find

    \begin{equation*}
        \lim_{k\to\infty} \left(q_1 \boxplus_n \cdots \boxplus_n q_k\right) = e^{ - \frac{\sigma^2}{2(n-1)} }[z^n].
    \end{equation*}

    The last expression is the definition given previously for $H_n(z,\sigma^2/(n-1))$.
\end{proof}


\begin{theorem}[Finite Free Poisson Limit Theorem]
    Let $p(z) = z^{n-1}(z-1)$. And for $\nu n \in \N$, the $\nu n$ times symmetric additive convolution of $p(z)$ with itself is a polynomial corresponding to the finite free Poisson distribution.

    \begin{equation*}
        \underbrace{p(z) \boxplus_n p(z) \boxplus_n \cdots \boxplus_n p(z)}_\text{$\nu n$ times} = \left(1-\frac1n \partial_z \right)^{\nu n}[z^n].%(\nu n)!(-n)^{-\nu n}z^{n(1-\nu)}L_{\nu n}^{n(1-\nu)}(zn).
    \end{equation*}
\end{theorem}

\begin{proof}
    Notice that $z^{n-1}(z-1) = z^n - z^{n-1}$ can be written as generated by a linear differential operator in the following way

    \begin{equation*}
        \left(1 - \frac1n \partial_z\right)[z^n] = z^n - z^{n-1}.
    \end{equation*}

    Now, applying once again Theorem \ref{thm:multiplicative_operators} we get

    \begin{align*}
        \underbrace{p(z) \boxplus_n p(z) \boxplus_n \cdots \boxplus_n p(z)}_\text{$\nu n$ times} &= \left(1-\frac1n \partial_z \right)^{\nu n}[z^n].
    \end{align*}

    Which is exactly the polynomial corresponding to the finite free Poisson distribution given in  \eqref{eq:ff_poisson}.
\end{proof}