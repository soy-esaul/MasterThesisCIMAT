%\section{Finite free probability and random matrices}


\section{Expected characteristic polynomials} \label{sec:minor_orthogonality}

\subsection{Minor orthogonality}

\begin{definition}[Minor orthogonality]
    Let $R$ be an $m \times n$ random matrix. We say $R$ is minor orthogonal if for every $k,l \in \mathbb Z$ such that $k,l \le \max\{m.n\}$ and all sets $S,T,U,V$ with $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{V} = l$, it satisfies
    
    \begin{equation*}
        E_R\left[ [R]_{S,T} [R^*]_{U,V} \right] = \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{equation*}
\end{definition}

\begin{lemma} \label{lemma:orth_trans_is_minorth}
    If $R$ is minor orthogonal and $Q$ is a constant matrix such that $Q\hermit{Q} = I$, then $Q$ is minor orthogonal. If $\hermit{Q}Q = I$, then $RQ$ is minor orthogonal.
\end{lemma}

\begin{proof}
    Recall that by the Cauchy-Binet formula, for $|S|=|T| = k$ we have

    \begin{equation*}
        [QR]_{S,T} = \sum_{\abs{W} = k} [Q]_{S,W}[R]_{W,T},
    \end{equation*}

    \noindent so with $\abs{S} = \abs{T} = k, \abs{U}=\abs{V} = l$,

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= E_R \left[  \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[R]_{W,T} [\hermit{R}]_{U,Z}[\hermit{Q}]_{Z,V} \right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \left[ [R]_{W,T}[\hermit{R}]_{U,Z}\right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \frac{\delta_{W,Z}\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= \sum_{\abs{W}=k} [Q]_{S,W}[\hermit{Q}]_{W,V}\frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}}
        = [Q\hermit{Q}]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= [I]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},
        \intertext{Notice that $[I]_{S,V} = 1$ if and only if $S=V$, so we conclude that}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{align*}

    The case $\hermit{Q}Q =I$ is proven in the same way.
\end{proof}

\begin{definition}[Signed permutation matrix]
    A signed permutation matrix is a matrix that can be written $EP$ where $E$ is a diagonal matrix with entries $\pm 1$ and $P$ is a permutation matrix.
\end{definition}

\begin{lemma} \label{lemma:singed_per_is_minorth}
    A random matrix sampled uniformly from the set of signed permutation matrices is minor-orthogonal.
\end{lemma}

\begin{proof}
    Let $Q$ be a signed permutation matrix, we can write $Q = E P$, where $E$ is a diagonal random matrix with entries $\pm 1$ taken uniformly and $P$ is a matrix chosen uniformly from the permutation matrices, and both are independent. Then for $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{U} = l$,

    \begin{align*}
        E_Q\left[ [Q]_{S,T}[\hermit{Q}]_{U,V} \right] &= E_{E,P}\left[ [E P]_{S,T}[\hermit{P}E]_{U,V} \right],\\
        &= \sum_{\abs{W} = k} \sum_{\abs{Z} = l} E_{E,P} \left[ [E]_{S,W} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{Z,V} \right],
        \intertext{every $[E]_{S,W}$ is diagonal and the determinant would be zero if $S\neq W$, so}
        &= E_{E,P} \left[ [E]_{S,S} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{V,V} \right],\\
        \intertext{Let $\left\{\chi_i\right\}_{1\le i \le n}$ be the diagonal entries of $E$, then}
        &= E_{E}\left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right].
    \end{align*}

    Now we use that the variables $chi_i$ are independent and uniform in $\{-1,1\}$, so that $E[\chi_i] = 0$, but $E[\chi_i^2] = 1$ for all $i$, and this means 

    \begin{equation*}
        E_E \left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] = \delta_{S,V}.
    \end{equation*}

    This last equality leads to

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right],\\
        &=  \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,S} \right],\\ 
        &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[P]_{S,U} \right].
    \end{align*}

    The submatrix $P_{S,T}$ can be transformed in a diagonal matrix by a permutation matrix because it has at most a non zero entry for each row and each column. If the diagonal matrix has a zero entry in the diagonal, then the determinant $[P]_{S,T}$ is zero, in other case, it is different that zero. The only case when all of the diagonal entries of the diagonal matrix are not zero is when $T = \pi(S)$ with $\pi$ the permutation function corresponding to $P$. This means that in order to have a non-zero determinant we need $T = \pi(S) = U$, and $[P]_{S,U} = \in \{-1,1\}$, so

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}[P]_{S,T} \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}^2 \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  \delta_{T=\pi(S)} \right], \\ 
        &= \delta_{S,V} \delta_{T,U} \P{T = \pi(S)}.
    \end{align*}

    We are supposing that we are sampling uniformly from the permutation matrices of size $n \times n$, so the probability that $T = \pi(S)$ when $\pi$ is a permutation of $n$ elements and $\abs{S} = \abs{T} = k$ is $1/\binom{n}{k}$. So, we can conclude

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V} \delta_{T,U}}{\binom{n}{k}}.
    \end{align*}

    This is the definition of being minor-orthogonal. \todo{En el paper no lo prueban, pero el resultado se tiene también para una matriz de permutación con signos rectangular. En la siguiente prueba se usa esto, así que falta completar eso. La prueba es igual, simplemente tomando que $E$ es de tamaño $m\times m$ y $P$ de tamaño $m\times n$, todos los resultados se siguen.}

\end{proof}


\begin{corollary}
    An $m\times n$ random matrix sampled from the Haar measure on $\mathbb C_{n}^m$ is minor-orthogonal.
\end{corollary}

\begin{proof}
    Let $R$ be a Haar distributed random $m\times n$ matrix with $m \le n$ and $Q$ a random permutation matrix. Any random permutation matrix is unitary, so $RQ$ is Haar distributed for fixed $Q$, and by Lemmas \ref{lemma:orth_trans_is_minorth} and \ref{lemma:singed_per_is_minorth} we have that it is also minor-orthogonal. Then, if $Q$ is uniformly sampled from the signed permutation matrices,

    \begin{align*}
        E_R\left[ [R]_{S,T}[\hermit{R}]_{U,V} \right] = E_{R,Q}\left[ [RQ]_{S,T} [\hermit{(RQ)}]_{U,V}\right]. 
    \end{align*}

    Since $Q$ is minor orthogonal, $RQ$ is also minor orthogonal for fixed $R$ and 

    \begin{align*}
        E_R\left[ E_Q \left[ [RQ]_{S,T}[\hermit{(RQ)}]_{U,V} \right] \right] = E_R\left[ \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}}\right] = \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}},
    \end{align*}

    \noindent where $k = \abs{S} = \abs{T}$. 
\end{proof}


% Formulas


We denote by $\sigma_k(A)$ the coeficient of of $(-1)^{k}x^{d-k}$ in the characteristic polynomial of a $d$-dimensional matrix $A$. We will use the fact that \todo{Tal vez esto debería ir en preliminares}

\begin{equation*}
    \sigma_k(A) = \sum_{\abs{S} = k} [A]_{S,S}.
\end{equation*}

\begin{lemma} \label{lemma:conjugate_minorth}
    Let $m \le n$, $B$ an $n\times n$ random matrix and $R$ an $m\times n$ minor-orthogonal matrix independent from $B$. For all sets $S,T \subset \binom{[m]}{k}$ we have

    \begin{equation*}
        E_{B,R} \left[ [RB\hermit R]_{S,T} \right] = E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Using the Cauchy-Binet formula we have

    \begin{align*}
        E_{B,R} \left[ [RB\hermit{R}]_{S,T} \right] &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} E_R \left[ [R]_{S,X} [B]_{X,Y} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} E_R \left[ [R]_{S,X} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &=  E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} \frac{\delta_{S,T} \delta_{X,Y}}{\binom{n}{k}} \right],\\ 
        &= E_B \left[\sum_{X \in \binom{[n]}{k}} [B]_{X,X} \frac{\delta_{S,T}}{\binom{n}{k}} \right],\\
        &= E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{align*}
\end{proof}


\begin{lemma}
    Let $a > d$, $A$ an $a \times a$ random matrix and $Q$ a random $a \times d$ matrix sampled from the Haar measure on $\mathbb C_a^d$, then

    \begin{equation*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] = E_A \left[\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} \chi_x(Q) \right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $A$ be a fixed matrix and $Q$ a Haar unitary matrix on $\mathbb C_{a}^d$, the $k$th coefficient of the expected characterictic polynomial of $QA\hermit Q$ is 

    \begin{align*}
        E_Q \left[ \sigma_k(QA\hermit{Q}) \right] &= \sum_{\abs{S} = k} E_Q\left[ [QA\hermit Q]_{S,S} \right],\\ 
        &= \sum_{\abs{S} = k} \frac{\sigma_k(A)}{\binom ak},\\ 
        &= \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}}.
    \end{align*}

    Taking expectaton on the last expression we find

    \begin{align*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] &= E_A \left[ \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}} \right] = \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}},
    \end{align*}

    \noindent which is the $k$th coefficient of $\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} E_A \left[\chi_x(A) \right]$.
\end{proof}

\subsection{Relationship to polynomial convolution}

\begin{theorem} \label{thm:implies_symmad}
    Let $A, B$ be $d\times d$ random matrices and $R$ a $d\times d$ minor-orthogonal matrix, such that $A, B, R$ are jointly independent, then we have

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}
\end{theorem}

\begin{proof}
    We use 

    \begin{equation*}
        \sigma_k(A) = \sum_{\abs{S}=k} [A]_{S,S},
    \end{equation*}

    \noindent together with Theorem \ref{thm:marcus_binet} and Lemma \ref{lemma:conjugate_minorth} to get \todo{Hay que aclarar las normas de matrices.}

    \begin{align*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit{R}) \right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[ [A + RB\hermit{R}]_{S,S} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] E_{B,R}\left[ [RB\hermit R]_{\overline{U}(S),\overline{V}(S)} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] \delta_{\overline{U}(S),\overline{V}(S)} \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}},\\ 
        \intertext{using that $U(S)=V(S)$ if and only if $\overline{U}(S) = \overline{V}(S)$,}
        &= \sum_{i=0}^k \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}} \sum_{S\in\binom{[d]}{k}} \sum_{U,V\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right].
    \end{align*}

    To finish the proof we need to find

    \begin{equation} \label{eq:suma_rara} \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right]. \end{equation}

    Clearly, we are summing over all of the sets $V \in \binom{[d]}{i}$, but they appear more than once in the sum. To find the number of times every element $V\in \binom{[d]}{i}$ appears in the sum, we can count the total number of terms we are summing in \eqref{eq:suma_rara} and divide by the total number of elements in $\binom{[d]}{i}$. We have that $\abs{\binom{[d]}i} = \binom{d}{i}$ and the number of summands is $\binom{d}{k}\binom{k}{i}$, so

    \begin{align*}
        \frac{\binom{d}{k}\binom{k}{i}}{\binom{d}{i}} &= \frac{\frac{d!}{k!(d-k)!}\frac{k!}{i!(k-i)!}}{\frac{d!}{i!(d-i)!}} = \frac{(d-i)!}{(d-k)!(k-i)!} = \binom{d-i}{k-i}.
    \end{align*}

    So, we have

    \begin{equation*}
        \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right] = \binom{d-i}{k-i} \sum_{V\in \binom{[d]}{i}}  E_A\left[ [A]_{V,V} \right] = \binom{d-i}{k-i}E_A\left[\sigma_{i}(A)\right].
    \end{equation*}

    Thus we can conclude

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}

    \todo{Hay que aclarar qué signofica $U(S)$.}
\end{proof}


\begin{theorem} \label{thm:symmad}
    If $p(x)$ is the characteristic polynomial of $A$ and $q(x)$ is the characteristic polynomial of $B$, where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxplus_d q(x) = E_Q\left[\chi_x(A + Q B Q^*)\right],
    \end{equation*}

    \noindent where $\chi_x(\cdot)$ denotes the characteristic polynomial of $\cdot$ with $x$ as a variable and $E_Q$ denotes taking expectation over $Q$ where $Q$ is sampled from the Haar measure on the unitary complex $d\times d$ matrices.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symmad} and definition of the symmetric additive convolution.
\end{proof}



\begin{theorem} \label{thm:implies_symm_mult}
    Let $A$ and $B$ be $d\times d$ random matrices and $R$ a minor-orthogonal $d\times d$ matrix, such that $A,B,R$ are jointly independent, then

    \begin{equation*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] = \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{equation*}
\end{theorem}

\begin{proof}
    \begin{align*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[[ARB\hermit{R}]_{S,S}\right],\\ 
        \intertext{By the Cauchy-Binet formula and independence}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[ [A]_{S,T} \right] E_{B,R} \left[ [RB\hermit{R}]_{T,S} \right],\\ 
        \intertext{By Lemma \ref{lemma:conjugate_minorth}}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[[A]_{S,T} \right] \delta_{T,S}\frac{E_B\left[ \sigma_k(B)\right]}{\binom{d}{k}},\\
        &= \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{align*}
\end{proof}


\begin{theorem}
    Let $p(x)$ be the characteristic polynomial of $A$ and $q(x)$ be the characteristic polynomial of $B$ where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxtimes_d q(x) = E_Q \left[ \chi_x (AQBQ^*) \right],
    \end{equation*}

    \noindent with $\chi_x$ and $E_Q$ as in Theorem \ref{thm:symmad}.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symm_mult} and definition of the symmetric multiplicative convolution.
\end{proof}

\section{The $\mathcal R_n$ finite transform} 

\todo{Introducir la notación de $p(x) \equiv q(x) \mod x^n$.}

Given any polynomial $p(z)$ with order $p$ we can associate an empirical measure $\mu_p$ to its roots $z_i$ given by

\begin{equation*}
    \mu_p(\{x\}) = \frac1n\sum_{j=1}^p \delta_{x,z_j}.
\end{equation*}

This measure is similar to the spectral empirical measure of a random matrix. We can find its Cauchy transform in terms of the polynomial with the following lemma.

\begin{lemma} \label{lemma:cauchy_empirical_polynomial}
    Let $p$ be a monic polynomial of order $p$ with roots $\{z_i\}_{i=1}^n$, then the Cauchy transform of the empirical measure associated to the roots $z_i$ is given by 

    \begin{equation*}
        G_{\mu_p}(z) \coloneqq \frac1n \sum_{j=1}^n \frac1{z - z_j} = \frac{\partial_z p }{n p}(z) = \frac1n \partial_z \ln p(z).
    \end{equation*}

\end{lemma}

\begin{proof}
    $p(z)$ is a monic polynomial with roots $\left\{ z_j \right\}_{j \in [n]}$, then we can write,

    \begin{equation*}
        p(z) = \prod_{j=1}^n (z-z_j).
    \end{equation*}

    By the Leibnitz rule we find

    \begin{equation*}
        \partial_z p(z) = \sum_{j=1}^n \prod_{k\neq j} (z-z_k).
    \end{equation*}

    Using the last equation we have

    \begin{align*}
        \frac{\partial_z p}{n p}(z) &= \frac1n\sum_{j=1}^n \frac{\prod_{k\neq j} (z-z_k) }{ \prod_{l=1}^n (z-z_l) } = \frac1n\sum_{j=1}^n \frac{1}{z - z_j} \eqqcolon G_{\mu_p}(z).
    \end{align*}
\end{proof}

We state without proof the following result. The proof  can be found in \cite{anaya2016cumulantes}.

\begin{lemma}[$U$ transform]
    Let $S$ be a multiset of complex numbers and denote by $\abs S=n$ its number of elements with multiplicity. Then exists an unique multiset of complex numbers $T$ such that $\abs{T}=n$ and 

    \begin{equation*}
        \prod_{s_i \in S} (x-s_i) = \frac 1n \sum_{t_i \in T} (x-t_i)^n.
    \end{equation*}

    We call such multiset the $U$ transform of $S$.
\end{lemma}

\begin{definition}[Legendre's transform]
    Let $f$ a convex function in a domain $D\subset \R$ and define

    \begin{equation} \label{eq:legendre_set}
        D^* \coloneqq \left\{ x^* \in \R \mathrel{:} \sup_{x \in D} \{xx^* - f(x)\} < \infty \right\}.
    \end{equation}

    We define $f^*$ the Legendre transform of $f$ as the function

    \begin{align*}
        f^* : D^* &\to \R.\\
        s \mapsto &\sup_{x \in D} \{ xs - f(x) \}.
    \end{align*}
\end{definition}

\begin{lemma} \label{lemma:strictly_convex}
    Let $f$ be a strictly convex function in a domain $D \subset \R$ and such that its derivative exists in a point $x_0 \in D$. Then $\left.\partial_x [f(x)]\right|_{x=x_0} \in D^*$ and 

    \begin{equation*}
        f^*(f'(x_0)) = x_0 f'(x_0) - f(x_0).
    \end{equation*}

    If additionally, $f$ has a second derivative, then the following two results are satisfied

    \begin{align*}
        (f')^{-1}(x_0) &= (f^*)'(x_0), \\
        f''((f^*)'(x_0)) &= \frac{1}{(f^*)''(x_0)}.
    \end{align*}
\end{lemma}


\begin{proof}
    Since $f$ is strictly convex and differentiable at $x_0$ we have for $x\in D, x\neq x_0$ that 
    
    \begin{align*}
        f(x) &> f(x_0) + (x-x_0)f'(x_0), \\
        \Rightarrow f(x) -xf'(x_0) &> f(x_0) - x_0 f'(x_0),\\
        \Rightarrow xf'(x_0) - f(x) &< x_0 f'(x_0) - f(x_0).
    \end{align*}

    This means that 

    \begin{equation*}
        \sup_{x\in D}\{ xf'(x_0) - f(x) \} = x_0f'(x_0) - f(x_0) < \infty.
    \end{equation*}

    Then, by the definition of $D^*$, we have that $f'(x_0) \in D^*$ and the Legendre transform in this point is $f^*(f'(x_0)) = x_0f'(x_0) - f(x_0)$.

    For the second part, if $f$ has a second derivative at $x_0$, differentiate the last equation to find

    \begin{equation*}
        (f^*)'(f'(x_0))f''(x_0) = x_0 f''(x_0) + f'(x_0) - f'(x_0) = x_0 f''(x_0). 
    \end{equation*}

    The second derivative $f''(x_0)$ can not be zero because $f$ is strictly convex, so $(f^*)'(f'(x_0)) = z$, which means $f^*$ and $f'$ are inverse under composition (in any point where $f$ is twice differentiable). For the second equation we use the fact that these functions are inverse and derive,

    \begin{align*}
        f'((f^*)'(z)) &= z,\\
        \Rightarrow f''((f^*)(x_0))(f^*)''(x_0) &= 1,\\
        \Rightarrow f''((f^*)(x_0)) &= \frac1{(f^*)''(x_0)}.
    \end{align*}
\end{proof}

\begin{lemma} \label{lemma:legendre_transform_norm}
    Let $D \subset \R$ and $\mu$ a measure that is absolutely continuous with respect to the Lebesgue measure, then for any continuous function $f: D \to \R$ we have that

    \begin{equation*}
        f^*(s) = \ln \norm{ e^{xs - f(x)} }_{\infty},
    \end{equation*}

    \noindent for all $s \in D^*$ where the Legendre transform and the norm are taken over $D$.
\end{lemma}

\begin{proof}
    We can write $f^*$ as $f^*(s) = \ln(\exp(f^*(s)))$, then

    \begin{align*}
        f^*(s) &= \ln(\exp(f^*(s))) = \ln \left(\exp\left\{ \sup_{x\in D} \{xs-f(x)\} \right\}\right), \\
        \intertext{using that $\exp(x)$ is monotone increasing,}
        &= \ln \left( \sup_{x \in D} \exp(xs - f(x))\right) = \ln \norm{ e^{xs-f(x)} }_{\infty}.
    \end{align*}
\end{proof}

\begin{definition}[The $\mathcal K_n$ transform \cite{anaya2016cumulantes}]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal K_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal K_n^{\mu_A} (s) \coloneqq - \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+ [x I - A]^{\frac1n} }_n,
    \end{equation*}

    \noindent where $\Delta^+$ represents the normalized determinant
    
    \begin{equation*}
        \Delta^+[A] = \left\{ \begin{array}{cc}
            \det[A]^{\frac1n} & \text{if $A$ is positive definite,}\\
            0 & \text{otherwise.}
        \end{array} \right.
    \end{equation*}
    
    \noindent and the integration domain for the norm is $(\rho_A, \infty)$ with $\rho_A$ the spectral radius of $A$.
\end{definition}


\begin{definition}[The $\mathcal R_n$ transform]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal R_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal R_n^{\mu_A} (s) = \mathcal K_n^{\mu_A} - \left( 1 + \frac1n \right) \frac1s.
    \end{equation*}
\end{definition}

\begin{theorem} Let $A$ be a self-adjoint $n \times n$ matrix with empirical spectral distribution $\mu_A$, then

    \begin{equation*}
        \lim_{n\to\infty} \mathcal K_n^{\mu_A} (s) = G_{\mu_A}^{-1}(s),
    \end{equation*}
\noindent with $s \in (\rho_A, \infty)$ and where $G_{\mu_A}^{-1}(s)$ is the inverse under composition of $G_{\mu_A}(s)$.
\end{theorem}

\begin{proof}
    
    We begin defining the function $g(x) \coloneqq -\ln \Delta^+[xI - A]$ and let $\lambda_1, \dots, \lambda_n$ be the ordered eigenvalues of $A$. When we differentiate $g$ with respect to $x$, we find 

    \begin{align*}
        \partial_x g(x) &= \partial_x \left[-\ln \Delta^+[xI - A]\right] = -\partial_x \ln\left[ \left(\prod_{j=1}^n (x - \lambda_j) \right)^{\frac1n}\right], \\
        &= -\partial_x \ln \left[ \prod_{j=1}^n (x-\lambda_j)^{\frac1n} \right] = - \partial_x \sum_{j=1}^n \frac1n \ln (x-\lambda_j) = -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} = - G_{\mu_A}(x).
    \end{align*}

    For the second derivative we have

    \begin{align*}
        \partial_{xx} g(x) = \partial_x \left[ -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} \right] = \frac1n \sum_{j=1}^n \frac{1}{(x-\lambda_j)^2} > 0.
    \end{align*}

    So $g(x)$ is strictly convex and has a second derivative. Using Lemma \ref{lemma:strictly_convex} we find that 

    \begin{equation} \label{eq:inverse_cauchy}
        (g^*)'(x) = (g')^{-1}(x) = (-G_{\mu_A})^{-1}(x) = G_{\mu_A}^{-1}(-x).
    \end{equation}

    Now we find $(g^*)'(x)$ with the aide of Lemma \ref{lemma:legendre_transform_norm}.

    \begin{align*}
        (g^*)'(s) &= \frac{\partial}{\partial s} \ln \norm{ e^{xs} - g(x) }_\infty = \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+(xI - A) }_\infty.
    \end{align*}

    Substituting the last in \eqref{eq:inverse_cauchy} we get 

    \begin{equation*}
        G_{\mu_A}^{-1}(s) = (g^*)'(-s) = -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    On the other hand, the limit of $\mathcal K_n^{\mu_A}$ is 

    \begin{equation*}
        \lim_{n\to\infty}  \mathcal K_n^{\mu_A}(s) = - \lim_{n\to\infty} \frac{\partial}{\partial s} \ln \norm{ e^{xs}\Delta^+(xI - A) }_n =  -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    This gives the desired result.
\end{proof}




% \begin{proof}
%     Expanding the term $(x-t_i)^n$ we get,

%     \begin{equation*}
%         \frac 1m \sum_{t_i \in T} (x-t_i)^n = \frac 1m \sum_{t_i \in T} \sum_{k=0}^n \binom nk x^{n-k} (-1)^k t_i^k = \sum_{k=0}^n \binom nk x^{n-k} (-1)^k \E{T^k}. 
%     \end{equation*}

%     Where we use $\E{f(T)}$ to denote the expectation with respect to the empirical distribution associated to the multiset $T$, $\E{f(T)} = \frac1{\abs T} \sum_{t_i \in T} f(t_i)$.
% \end{proof}

\begin{lemma} \ref{lemma:Utrans_convolution}
    Let $p,q$ be polynomials with degree $n$ and $U_p,U_q$ be the $U$ transforms of its sets of roots, then

    \begin{equation*}
        [p \boxplus_n q](x) = \frac1n \sum_{u_j \in U_p, u_k \in U_q} (x - u_j -u_k)^n.
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $p(x) = \sum_{j=0}^n x^{n-j}(-1)^j p_j$. Let $x_i$ be its roots, by definition of the $U$ transform we have,

    \begin{equation*}
        p(x) = \prod_{j=0}^n (x - x_j) = \frac1n \sum_{u_j \in U_p} (x-u_j)^n = \sum_{j=0}^n x^{n-j} (-1)^j \binom n j \E{U_p^j}.
    \end{equation*}

    Equating coefficients we find that $p_j = \binom nj\E{U_p^j}$ and the analogous happens for $q_k$. Finally, we find the convolution

    \begin{align*}
        [p\boxplus_n q](x) &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{(n-i)!(n-j)!}{(n-k)!n!} \binom{n}{i}\binom{n}{j} \E{U_p^i} \E{U_q^j},\\
        &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{n!}{i!j!(n-k)!} \E{U_p^i U_q^j}, \\
        &= \sum_{k=0}^n \binom nk x^{n-k}(-1)^k \sum_{j=0}^k \binom{k}{j}\E{U_p^{k-j} U_q^j}, \\
        &= \sum_{k=0}^n \binom nk x^{n-k}(-1)^k \E{(U_p + U_q)^k},\\
        &= \E{ (x - U_p - U_q)^n }. 
    \end{align*}
\end{proof}

\begin{lemma}  \label{lemma:quotient_norms}
    Let $A$ be an $n\times n$ real symmetric matrix. Denote by $\lambda(A)$ its spectrum and by $U_A$ the $U$ transform of $\lambda(A)$, then 
\todo{Encontrar $R_0^n$}
    \begin{equation*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \equiv \frac1n\sum_{u_i \in U_A} e^{-ns u_i} \qquad \mod [s^{n+1}]. 
    \end{equation*} 
\end{lemma}

\begin{proof}
    By definition of the $U$ transform, for any $n\times n$ symmetric matrix $A$ we have

    \todo{Sin pérdida de generalidad, podemos suponer que es positiva definida.}

    \begin{equation*}
        \Delta^+(xI - A)^n = \det[xI - A] = \prod_{j=1}^n (x - \lambda_j) = \frac1n \sum_{u_i \in U_A} (x-u_i)^n.
    \end{equation*}

    Using this and the previously found result for $\norm{e^{-xs}\Delta^+(xI)}_n^n$ we find

    \begin{align*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac{(ns)^{n+1}}{n!}\norm{e^{-xs}\Delta^+(xI - A)}_n^n,\\
        &= \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \Delta^+(xI - A)^n \d x,\\
        &= \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \frac1n \sum_{u_i \in U_A} (x-u_i)^n \d x,\\
        &= \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A}\int_{0}^\infty e^{-nsy} \frac1n \sum_{u_i \in U_A} (y + \rho_A-u_i)^n \d x, \\
        &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \mathcal L\{(y + \rho_A - U_A)^n\}(ns), \\
        &=  \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \binom nk  (\rho_A - u_i)^{n-k}\mathcal L\{y^k\}(ns), \\
        &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \frac{n!}{k!(n-k)!}  (\rho_A - u_i)^{n-k}\frac{k!}{(ns)^{k+1}}, \\
        &= \frac1n \sum_{u_i \in U_A} e^{-ns\rho_A} \sum_{k=0}^n \frac{(ns)^{n-k}(\rho_A - u_i)^{n-k}}{(n-k)!}, \\
        &\equiv \frac1n \sum_{u_i\in U_A} e^{-ns\rho_A} e^{ns(\rho_A - u_i)}, \quad \mod [s^{n+1}],\\
        &\equiv \frac1n \sum_{u_i\in U_A} e^{-nsu_i}, \quad \mod [s^{n+1}].
    \end{align*}
\end{proof}

\begin{corollary} \label{corollary:R_n_as_a_logarithm}
    Let $A$ be a an $n\times n$ symmetric matrix with real entries, with spectrum $\lambda(A)$ and $U_A$ be the $U$ transform of $\lambda(A)$, then 

    \begin{equation*}
        \mathcal R_{n}^{\mu_A}(s) \equiv - \frac1n \frac{\partial}{\partial s} \ln \left(\frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right) \qquad \mod [s^n].
    \end{equation*}
\end{corollary}

\begin{proof}
    Taking logarithm in both sides of Lemma \ref{lemma:quotient_norms} we get 

    \begin{equation*}
        \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \right) \equiv \ln\left( \frac1n\sum_{u_i \in U_A} e^{-ns u_i} \right) \qquad \mod [s^{n+1}].
    \end{equation*}

    The first $n+1$ coeficientes of the power series coincide, so the first $k$ coefficients of the derivatives also coincide. Now we use the definition of the $\mathcal R_n$ transform

    \begin{align*}
        \mathcal R_{n}^{\mu_A}(s) &= \mathcal K_{n}^{\mu_A}(s) - \mathcal K_n^{\mu_0}(s), \\
        &= - \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI -A) }_n + \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI) }_n,\\
        &= - \frac1n \frac{\partial}{\partial s} \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI -A)}_n^n}{ \norm{e^{-xs}\Delta^+(xI)}_n^n } \right), \\
        &\equiv -\frac1n \frac{\partial}{\partial s} \ln \left( \frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right), \qquad \mod[s^n].
    \end{align*}
\end{proof}

\begin{lemma}
    Let $A$ and $B$ be two $n\times n$ real symmetric matrices. The following are equivalents:

    \begin{enumerate}
        \item \[ \mathcal R_n^{\mu_A}(s) + \mathcal R_n^{\mu_B}(s) = \mathcal R_n^{\mu_{A+B}}(s) \qquad \mod[s^n]. \]
        \item \[ \det[xI - A] \boxplus_n \det[xI-B] = \det[xI - A - B]. \]
    \end{enumerate}
\end{lemma}

\begin{proof}
    Let us denote by $U_A, U_B$ and $U_{A+B}$ the $U$ transforms of the $\lambda(A), \lambda(B)$ and $\lambda(A+B)$, respectively. By Lemma \ref{lemma:Utrans_convolution}, the second statement is equivalent to 

    \begin{equation*}
        \E{ (x - U_A - U_B)^n } = \E{ (x - U_{A+B})^n }.
    \end{equation*}

    Expanding the power and equating terms, the last happens if and only if the first $m$ moments of $U_A + U_B$ and $U_{A+B}$ coincide. This is in turn equivalent to 

    \begin{equation*}
        \E{e^{-ms(U_A + U_B)}} = \E{e^{-ms(U_A + U_B)}} = \E{ e^{-msU_{A+B}} } \qquad \mod [s^{n+1}].
    \end{equation*}

    Define the function $f_A(s) \coloneqq - \frac1n \ln \E{ e^{-msU_A} }$ and similarly for $B$ and $A+B$. Using this function, the second statement is equivalent to 

    \begin{equation} \label{eq:f_matrix_module}
        f_A(s) + f_B(s) \equiv f_{A+B}(s) \qquad \mod [s^{n+1}].
    \end{equation}

    Using Corollary \ref{corollary:R_n_as_a_logarithm}, the first statement is equivalent to 

    \begin{equation} \label{eq:differential_matrix_module}
        \frac{\partial}{\partial_s} f_A(s) + \frac{\partial}{\partial_s} f_B(s) \equiv \frac{\partial}{\partial_s} f_{A+B}(s) \qquad \mod[s^n].
    \end{equation}

    So proving the equivalence of the two statements reduces to prove the equivalence between \eqref{eq:f_matrix_module} and \eqref{eq:differential_matrix_module}. Because the formal series in \eqref{eq:f_matrix_module} coincide up to the $n+1$th term, the derivatives coincide up to the $n$th term, so one implication is trivial. For the second implication, it suffices to prove that $f_A(0) + f_B(0) = f_{A+B}(0)$, but we have that $f_A(0)=f_B(0)=f_{A+B}(0)$, so the result follows.
\end{proof} \todo{Mencionar caso multiplicativo.}

\subsection{Finite free distributions}

\begin{theorem} \label{thm:if_and_only_if}
    Let $P(n^{-1}\partial_z)$ be a polynomial on $n^{-1}\partial_z$ such that the linear differential operator applied to $x^n$, $P(n^{-1}\partial_z)[z^n]$ is a monic polynomial. Then $P(n^{-1}\partial_z) = \det[zI - A]$ for a matrix $A$ if and only if 

    \begin{equation} \label{eq:cauchy_transform_R}
        \frac1n \frac{\partial_s P(s)}{P(s)} \equiv - \mathcal R_n^{\mu_A}(s) \qquad \mod [s^{n+1}].
    \end{equation}
\end{theorem}

\begin{proof}
    Recall that

    \begin{equation*}
        \mathcal R_{n}^{\mu_A}(s) \equiv - \frac1n \frac{\partial}{\partial s} \ln \left(\frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right) \qquad \mod [s^n].
    \end{equation*}

    Also 

    \begin{align*}
        \frac1n \frac{\partial_s P(s)}{P(s)} &= \frac1n \partial_s[\ln P(s)].
    \end{align*}

    Then \eqref{eq:cauchy_transform_R} is satisfied if and only if 

    \begin{equation*}
        P(s) \equiv \frac1n \sum_{u_i \in U_A} e^{-nsu_i} \qquad \mod [s^{n+1}].
    \end{equation*}

    The last relationship in turn is equivalent to 

    \begin{equation*}
        P(n^{-1}\partial_z)[z^n] = \left( \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z} \right)[z^n] = \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z}[z^n].
    \end{equation*}

    Expanding the last term and using the definition of $U_A$, we find

    \begin{align*}
        \frac1n \sum_{u_i \in U_A} e^{-u_i\partial_z}[z^n] &=  \frac1n \sum_{u_i \in U_A} \sum_{j=0}^n \frac{(-u_i)^j}{j!} \partial_z^j[z^n],\\
        &= \frac1n \sum_{u_i \in U_A} \sum_{j=0}^n (-u_i)^j \frac{n!}{j!(n-j)!}z^{n-j} = \frac1n \sum_{u_i \in U_A} (z - u_i)^n = \det[zI - A].
    \end{align*}\end{proof}


\subsubsection{Constant distribution}

A constant random variable has a first cumulant equal to $c \in \R$ and all the remaining cumulants equal to zero. Then its $\mathcal R_n^{\mu_A}$ transform is the constant formal series

\begin{equation*}
    \mathcal R_n^{\mu_A}(s) = c.
\end{equation*}

We can use Theorem \ref{thm:if_and_only_if} to find the polynomial $p$ with this $\mathcal R_n^{\mu_A}$ transform.

\begin{align*}
    -c &= \frac{1}{n}\partial_s[\ln P(s)],\\
    \Rightarrow -\int nc\d s &= \ln P(s),\\
    \Rightarrow \exp\left\{-ncs + c_0\right\} &= P(s),
\end{align*}

\noindent for some constant $c_0$. Then $p(z)$ is the monic scaling of the polynomial $P(n^{-1}\partial_z)[z^n] = e^{-c\partial_z}[z^n]$. Expanding we find

\begin{align*}
    p(z) &= e^{-c\partial_z}[z^n] = \sum_{k=0}^n \frac{(-c)^k}{k!}\partial_z^k[z^n],\\
    &= \sum_{k=0}^n \frac{n!}{k!(n-k)!}(-c)^k z^{n-k} = (z-c)^n.
\end{align*}

Thus, as we would expect, $p$ is the monic polynomial with all the roots equal to $c$ which is also the expected characteristic polynomial of the deterministic matrix $cI$.

\subsubsection{Gaussian distribution}

In free and classical probability, the central limit random variable is characterized for having the first two cumulants non-zero and the rest of the cumulants equal to zero. We already know what is the polynomial corresponding to a constant distribution in finite free probability. To find the general polynomial corresponding to a Gaussian, we can find the polynomial with the second cumulant non-zero and the rest equal to zero. If the second cumulant is equal to $\sigma^2$, then the formal series expansion of $\mathcal R_n^{\mu_A}$ is

\begin{equation*}
    \mathcal R_n^{\mu_A}(s) = s\sigma^2.
\end{equation*}

We can use Theorem \ref{thm:if_and_only_if} again to find

\begin{align*}
    \frac1n \partial_s[ \ln P(s) ] &= -s\sigma^2,\\ 
    \Rightarrow \ln P(s) &= -n\sigma^2 \int s \d s,\\
    \Rightarrow P(s) &= \exp\left\{ -n \sigma^2\frac {s^2}2 + c_0  \right\},
\end{align*}

\noindent where the constant $c_0$ accounts for making the polynomial monic. Now, evaluating $P$ in $n^{-1}\partial_z$ and applying to $z^n$, we have

\begin{align*}
    p(z) &= P(n^{-1}\partial_z)[z^n] = e^{-\sigma^2\frac{\partial_z^2}{2n}}[z^n] \eqcolon H_n(z,\sigma^2/n). 
\end{align*}

The last expression corresponds to the Hermite polynomial wit variance $\sigma^2/n$.

\subsubsection{Poisson distribution}

The Poisson distribution has all of its cumulants equal to a constant $\nu \in \R$, the formal series expansion of its $\mathcal R_n^{\mu_A}$ transform is 

\begin{align*}
    \mathcal R_n^{\mu_A}(s) &= \sum_{k=0}^\infty \nu s^n = \nu \sum_{k=0}^\infty s^n = \frac{\nu}{1-s}.
\end{align*}

Where the convergence is taken in $\abs{s} < 1$. Another use of Theorem \ref{thm:if_and_only_if} leads to

\begin{align*}
    \frac{\nu}{1-s} &= \mathcal R_n^{\mu_A}(s) = -\frac1n\partial_s[\ln P(s)], \\
    \Rightarrow -\int \frac{n\nu}{s-1} \d s &= \ln P(s),\\
    \Rightarrow \exp\left\{ n\nu \ln (1-s) \right\} &= P(s).
\end{align*}

Finally, we evaluate in $n^{-1}\partial_z$ and apply to $z^n$ and find the polynomial that has the given cumulants.

\begin{align} \label{eq:ff_poisson}
    P(n^{-1}\partial_z)[z^n] &= \left( 1 - \frac{\partial_z}{n} \right)^{n\nu}[z^n] \eqcolon L_n^{(\nu-1)n}(z).
\end{align}

So the finite free Poisson distribution corresponds to a generalized Laguerre polynomial. Notice that when $\nu=n=1$, we recover the original definition of the Laguerre polynoamials given in \todo{Citar definición (incluir Laguerre generalizados).}

\subsection{Finite Free Limit Theorems}

\begin{theorem}[Finite Free Law of Large Numbers]
    Let $(p_i(z))_{i=1}^n$ be a sequence of degree $n$ monic polynomials with real roots $r_{ij}$ such that

    \begin{equation*}
        p_i(z) = \prod_{j=1}^n (z - r_{ij}).
    \end{equation*}

    And for every fixed $i$,

    \begin{align*}
        \frac1n\sum_{j=1}^n r_{ij} &= m,\\
        \frac1n\sum_{j=1}^n r_{ij}^2 < c,
    \end{align*}

    \noindent for some positive constant $c$. Define $q_i(k,z) \coloneqq k^{-n}p_i(kz)$. Then

    \begin{equation*}
        \lim_{k\to \infty} [ q_1(k,z) \boxplus_n q_2(k,z) \boxplus_n \cdots \boxplus_n q_k(k,z) ] = (x-m)^n
    \end{equation*}
\end{theorem}

\begin{proof}
    Write for every $i$ the polynomial $p_i(z)$ as

    \begin{equation*}
        p_i(z) = z^n + a_{i1} z^{n-1} + \cdots 
    \end{equation*}

    If $P_i(\partial_z)$ is a linear operator that acts on $z^n$ to generate $p_i(z)$, then 

    \begin{equation*}
        P_i(\partial_z) = 1 + a_{i1} n \partial_z + \cdots 
    \end{equation*}

    Since the coefficient $a_{i1}$ is the sum of the roots, by hypothesis we have that $a_{i1} = m$ for every $i$. Now, the last equation implies that if $q_i$ is generated by a linear differential operator $Q_i(\partial_z)$, then

    \begin{equation*}
        Q_i(\partial_z) = 1 - \frac{m}{k}\partial_z + O(k^{-2}).
    \end{equation*}

    Using the multiplicative Theorem \ref{thm:multiplicative_operators} we find

    \begin{align*}
        \lim_{k\to \infty} [ q_1(k,z) \boxplus_n q_2(k,z) \boxplus_n \cdots \boxplus_n q_k(k,z) ] &= \lim_{k\to\infty}\prod_{j=1}^k Q_j(\partial_z)[z^n],\\
        &= \lim_{k\to\infty} \left( 1 - \frac{m}{k}\partial_z + O(k^{-2}) \right)^k[z^n],\\
        &= e^{-m\partial_z}[z^n] = (z-m)^n.
    \end{align*}
\end{proof}

\begin{theorem}[Finite Free Central Limit Theorem \cite{marcus2021polynomial}]
    Let $p_1, p_2, \dots$ be a sequence of degree $n$ real rooted polynomials with $p_i = \prod_j (x - r_{i,j})$ such that

    \begin{equation} \label{eq:hypotheses_ffclt}
        \sum_{j=1}^n r_{i,j} = 0, \qquad \frac1n \sum_{j=1}^n r^2_{i,j} = \sigma^2,
    \end{equation}

    \noindent for all $i$. Define $q_i(x) = k^{-n/2}p_i(\sqrt{k}x)$, then 

    % \begin{equation*}
    %     \lim_{n\to\infty} \left( q_1 \boxplus_d \cdots \boxplus_d q_n \right) = \left( \frac{d-1}{\sigma^2} \right)^{-d/2} H_d\left( x \sqrt{\frac{d-1}{\sigma^2}} \right),
    % \end{equation*}

    \begin{equation*}
        \lim_{k\to\infty} \left( q_1 \boxplus_n \cdots \boxplus_n q_k \right) = H_n(z,\sigma^2/(n-1)).
    \end{equation*}

    \noindent with $H_n(z,\sigma^2/(n-2))$ represents the $n$th Hermite polynomial with variance $\sigma^2/(n-1)$.
\end{theorem}


\begin{proof}
    Using the Vieta's formulas and the hypotheses \eqref{eq:hypotheses_ffclt}, we have that for every $i$ it is satisfied

    \begin{align*}
        a_1 &= \sum_{j=1}^n r_{ij} = 0,\\
        a_2 &= \sum_{1 \le j < m \le n} r_{ij}r_{im} = \frac12 \sum_{j=1}^n r_{ij}\sum_{m\neq j} r_{im} = \frac12 \sum_{j=1}^n r_{ij} \left(a_1 - r_{ij}\right),\\
        &= - \frac12 \sum_{j=1}^n r_{ij}^2 = - \frac12 n\sigma^2.
    \end{align*}

    So every $p_i$ has the form

    \begin{equation*}
        p_i(z) = z^n + (0)z^{n-1} - \frac{n\sigma^2}{2n(n-1)} z^{n-2} + \cdots
    \end{equation*}

    Multiplying by the factors to get $q$, we get

    \begin{align*}
        q_i(z) &= z^n + (0)z^{n-1} - \frac{n\sigma^2k^{-n/2}}{2n(n-1)} k^{\frac{n-2}{2}}z^{n-2} + \cdots ,\\
        &= z^n - \frac{\sigma^2}{2(n-1)k}z^{n-2}.
    \end{align*}

    This means that the linear differential operator $Q_i(\partial_z)$ that generates $q_i$ must have the form

    \begin{equation*}
        Q_i(\partial_z)[z^n] = \left( 1 - \frac{\sigma^2}{2(n-1)k}\partial_z^2 + O(k^{-3/2})\right)[z^n].
    \end{equation*}

    We recall the multiplicative Theorem \ref{thm:multiplicative_operators} to find,

    \begin{align*}
        q_1 \boxplus_n \cdots \boxplus_n q_k &= \prod_{j=1}^k Q_j(\partial_z)[z^n], \\
        &= \left( 1 - \frac{\sigma^2}{2(n-1)k}\partial_z^2 + O(k^{-3/2})\right)^k[z^n].
    \end{align*}

    Finally, letting $k\to\infty$, we can find

    \begin{equation*}
        \lim_{k\to\infty} \left(q_1 \boxplus_n \cdots \boxplus_n q_k\right) = e^{ - \frac{\sigma^2}{2(n-1)} }[z^n].
    \end{equation*}

    The last expression is the definition given previously for $H_n(z,\sigma^2/(n-1))$
\end{proof}


\begin{theorem}[Finite Free Poisson Limit Theorem]
    Let $p(z) = x^{n-1}(x-1)$. And for $\nu n \in \N$, the $\nu n$ times symmetric additive convolution of $p(z)$ with itself is a polynomial corresponding to the finite free Poisson distribution.

    \begin{equation*}
        \underbrace{p(z) \boxplus_n p(z) \boxplus_n \cdots \boxplus_n p(z)}_\text{$\nu n$ times} = (\nu n)!(-n)^{-\nu n}z^{n(1-\nu)}L_{\nu n}^{n(1-\nu)}(zn).
    \end{equation*}
\end{theorem}

\begin{proof}
    Notice that $x^{n-1}(x-1) = x^n - x^{n-1}$ can be written as generated by a linear differential operator in the following way

    \begin{equation*}
        \left(1 - \frac1n \partial_z\right)[z^n] = z^n - z^{n-1}.
    \end{equation*}

    Now, applying once again Theorem \ref*{thm:multiplicative_operators} we get

    \begin{align*}
        \underbrace{p(z) \boxplus_n p(z) \boxplus_n \cdots \boxplus_n p(z)}_\text{$\nu n$ times} &= \left(1-\frac1n \partial_z \right)^{\nu n}[z^n].
    \end{align*}

    Which is exactly the polynomial corresponding to the finite free Poisson distribution given in  \eqref{eq:ff_poisson}.
\end{proof}