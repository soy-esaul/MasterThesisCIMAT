%\section{Finite free probability and random matrices}


\section{Minor orthogonality}


\begin{definition}[Minor orthogonality]
    Let $R$ be an $m \times n$ random matrix. We say $R$ is minor orthogonal if for every $k,l \in \mathbb Z$ such that $k,l \le \max\{m.n\}$ and all sets $S,T,U,V$ with $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{V} = l$, it satisfies
    
    \begin{equation*}
        E_R\left[ [R]_{S,T} [R^*]_{U,V} \right] = \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{equation*}
\end{definition}

\begin{lemma} \label{lemma:orth_trans_is_minorth}
    If $R$ is minor orthogonal and $Q$ is a constant matrix such that $Q\hermit{Q} = I$, then $Q$ is minor orthogonal. If $\hermit{Q}Q = I$, then $RQ$ is minor orthogonal.
\end{lemma}

\begin{proof}
    Recall that by the Cauchy-Binet formula, for $|S|=|T| = k$ we have

    \begin{equation*}
        [QR]_{S,T} = \sum_{\abs{W} = k} [Q]_{S,W}[R]_{W,T},
    \end{equation*}

    \noindent so with $\abs{S} = \abs{T} = k, \abs{U}=\abs{V} = l$,

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= E_R \left[  \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[R]_{W,T} [\hermit{R}]_{U,Z}[\hermit{Q}]_{Z,V} \right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \left[ [R]_{W,T}[\hermit{R}]_{U,Z}\right],\\
        &= \sum_{\abs{W}=k} \sum_{\abs{Z} = l} [Q]_{S,W}[\hermit{Q}]_{Z,V} E_R \frac{\delta_{W,Z}\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= \sum_{\abs{W}=k} [Q]_{S,W}[\hermit{Q}]_{W,V}\frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}}
        = [Q\hermit{Q}]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},\\
        &= [I]_{S,V} \frac{\delta_{T,U}}{\binom{\max\{m,n\}}{k}},
        \intertext{Notice that $[I]_{S,V} = 1$ if and only if $S=V$, so we conclude that}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V}\delta_{T,U}}{\binom{\max\{m,n\}}{k}}.
    \end{align*}

    The case $\hermit{Q}Q =I$ is proven in the same way.
\end{proof}

\begin{definition}[Signed permutation matrix]
    A signed permutation matrix is a matrix that can be written $EP$ where $E$ is a diagonal matrix with entries $\pm 1$ and $P$ is a permutation matrix.
\end{definition}

\begin{lemma} \label{lemma:singed_per_is_minorth}
    A random matrix sampled uniformly from the set of signed permutation matrices is minor-orthogonal.
\end{lemma}

\begin{proof}
    Let $Q$ be a signed permutation matrix, we can write $Q = E P$, where $E$ is a diagonal random matrix with entries $\pm 1$ taken uniformly and $P$ is a matrix chosen uniformly from the permutation matrices, and both are independent. Then for $\abs{S} = \abs{T} = k$ and $\abs{U} = \abs{U} = l$,

    \begin{align*}
        E_Q\left[ [Q]_{S,T}[\hermit{Q}]_{U,V} \right] &= E_{E,P}\left[ [E P]_{S,T}[\hermit{P}E]_{U,V} \right],\\
        &= \sum_{\abs{W} = k} \sum_{\abs{Z} = l} E_{E,P} \left[ [E]_{S,W} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{Z,V} \right],
        \intertext{every $[E]_{S,W}$ is diagonal and the determinant would be zero if $S\neq W$, so}
        &= E_{E,P} \left[ [E]_{S,S} [P]_{W,T} [\hermit{P}]_{U,Z} [E]_{V,V} \right],\\
        \intertext{Let $\left\{\chi_i\right\}_{1\le i \le n}$ be the diagonal entries of $E$, then}
        &= E_{E}\left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right].
    \end{align*}

    Now we use that the variables $chi_i$ are independent and uniform in $\{-1,1\}$, so that $E[\chi_i] = 0$, but $E[\chi_i^2] = 1$ for all $i$, and this means 

    \begin{equation*}
        E_E \left[ \prod_{i\in S} \chi_i \prod_{j\in V} \chi_j \right] = \delta_{S,V}.
    \end{equation*}

    This last equality leads to

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,V} \right],\\
        &=  \delta_{S,V} E_{P}\left[  [P]_{S,T}[\hermit{P}]_{U,S} \right],\\ 
        &= \delta_{S,V} E_{P}\left[  [P]_{S,T}[P]_{S,U} \right].
    \end{align*}

    The submatrix $P_{S,T}$ can be transformed in a diagonal matrix by a permutation matrix because it has at most a non zero entry for each row and each column. If the diagonal matrix has a zero entry in the diagonal, then the determinant $[P]_{S,T}$ is zero, in other case, it is different that zero. The only case when all of the diagonal entries of the diagonal matrix are not zero is when $T = \pi(S)$ with $\pi$ the permutation function corresponding to $P$. This means that in order to have a non-zero determinant we need $T = \pi(S) = U$, and $[P]_{S,U} = \in \{-1,1\}$, so

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}[P]_{S,T} \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  [P]_{S,T}^2 \right],\\ 
        &= \delta_{S,V} \delta_{T,U} E_{P}\left[  \delta_{T=\pi(S)} \right], \\ 
        &= \delta_{S,V} \delta_{T,U} \P{T = \pi(S)}.
    \end{align*}

    We are supposing that we are sampling uniformly from the permutation matrices of size $n \times n$, so the probability that $T = \pi(S)$ when $\pi$ is a permutation of $n$ elements and $\abs{S} = \abs{T} = k$ is $1/\binom{n}{k}$. So, we can conclude

    \begin{align*}
        E_R \left[ [QR]_{S,T}[\hermit{R}\hermit{Q}]_{U,V} \right] &= \frac{\delta_{S,V} \delta_{T,U}}{\binom{n}{k}}.
    \end{align*}

    This is the definition of being minor-orthogonal. \todo{En el paper no lo prueban, pero el resultado se tiene también para una matriz de permutación con signos rectangular. En la siguiente prueba se usa esto, así que falta completar eso. La prueba es igual, simplemente tomando que $E$ es de tamaño $m\times m$ y $P$ de tamaño $m\times n$, todos los resultados se siguen.}

\end{proof}


\begin{corollary}
    An $m\times n$ random matrix sampled from the Haar measure on $\mathbb C_{n}^m$ is minor-orthogonal.
\end{corollary}

\begin{proof}
    Let $R$ be a Haar distributed random $m\times n$ matrix with $m \le n$ and $Q$ a random permutation matrix. Any random permutation matrix is unitary, so $RQ$ is Haar distributed for fixed $Q$, and by Lemmas \ref{lemma:orth_trans_is_minorth} and \ref{lemma:singed_per_is_minorth} we have that it is also minor-orthogonal. Then, if $Q$ is uniformly sampled from the signed permutation matrices,

    \begin{align*}
        E_R\left[ [R]_{S,T}[\hermit{R}]_{U,V} \right] = E_{R,Q}\left[ [RQ]_{S,T} [\hermit{(RQ)}]_{U,V}\right]. 
    \end{align*}

    Since $Q$ is minor orthogonal, $RQ$ is also minor orthogonal for fixed $R$ and 

    \begin{align*}
        E_R\left[ E_Q \left[ [RQ]_{S,T}[\hermit{(RQ)}]_{U,V} \right] \right] = E_R\left[ \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}}\right] = \frac{\delta_{S,V}\delta_{T=U}}{\binom{n}{k}},
    \end{align*}

    \noindent where $k = \abs{S} = \abs{T}$. 
\end{proof}


% Formulas


We denote by $\sigma_k(A)$ the coeficient of of $(-1)^{k}x^{d-k}$ in the characteristic polynomial of a $d$-dimensional matrix $A$. We will use the fact that \todo{Tal vez esto debería ir en preliminares}

\begin{equation*}
    \sigma_k(A) = \sum_{\abs{S} = k} [A]_{S,S}.
\end{equation*}

\begin{lemma} \label{lemma:conjugate_minorth}
    Let $m \le n$, $B$ an $n\times n$ random matrix and $R$ an $m\times n$ minor-orthogonal matrix independent from $B$. For all sets $S,T \subset \binom{[m]}{k}$ we have

    \begin{equation*}
        E_{B,R} \left[ [RB\hermit R]_{S,T} \right] = E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Using the Cauchy-Binet formula we have

    \begin{align*}
        E_{B,R} \left[ [RB\hermit{R}]_{S,T} \right] &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} E_R \left[ [R]_{S,X} [B]_{X,Y} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &= E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} E_R \left[ [R]_{S,X} [\hermit{R}]_{Y,T} \right] \right],\\ 
        &=  E_B \left[\sum_{X,Y \in \binom{[n]}{k}} [B]_{X,Y} \frac{\delta_{S,T} \delta_{X,Y}}{\binom{n}{k}} \right],\\ 
        &= E_B \left[\sum_{X \in \binom{[n]}{k}} [B]_{X,X} \frac{\delta_{S,T}}{\binom{n}{k}} \right],\\
        &= E_B \left[\delta_{S,T} \frac{\sigma_k(B)}{\binom{n}k}\right].
    \end{align*}
\end{proof}


\begin{lemma}
    Let $a > d$, $A$ an $a \times a$ random matrix and $Q$ a random $a \times d$ matrix sampled from the Haar measure on $\mathbb C_a^d$, then

    \begin{equation*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] = E_A \left[\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} \chi_x(Q) \right].
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $A$ be a fixed matrix and $Q$ a Haar unitary matrix on $\mathbb C_{a}^d$, the $k$th coefficient of the expected characterictic polynomial of $QA\hermit Q$ is 

    \begin{align*}
        E_Q \left[ \sigma_k(QA\hermit{Q}) \right] &= \sum_{\abs{S} = k} E_Q\left[ [QA\hermit Q]_{S,S} \right],\\ 
        &= \sum_{\abs{S} = k} \frac{\sigma_k(A)}{\binom ak},\\ 
        &= \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}}.
    \end{align*}

    Taking expectaton on the last expression we find

    \begin{align*}
        E_A \left[E_Q\left[ \chi_x \left( Q A \hermit{Q} \right) \right]\right] &= E_A \left[ \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}} \right] = \frac{\binom{d}{k} \sigma_k(A)}{\binom{a}{k}},
    \end{align*}

    \noindent which is the $k$th coefficient of $\frac{d!}{a!} \frac{\d^{(a-d)}}{\d x} E_A \left[\chi_x(A) \right]$.
\end{proof}


\begin{theorem} \label{thm:implies_symmad}
    Let $A, B$ be $d\times d$ random matrices and $R$ a $d\times d$ minor-orthogonal matrix, such that $A, B, R$ are jointly independent, then we have

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}
\end{theorem}

\begin{proof}
    We use 

    \begin{equation*}
        \sigma_k(A) = \sum_{\abs{S}=k} [A]_{S,S},
    \end{equation*}

    \noindent together with Theorem \ref{thm:marcus_binet} and Lemma \ref{lemma:conjugate_minorth} to get \todo{Hay que aclarar las normas de matrices.}

    \begin{align*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit{R}) \right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[ [A + RB\hermit{R}]_{S,S} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] E_{B,R}\left[ [RB\hermit R]_{\overline{U}(S),\overline{V}(S)} \right],\\ 
        &= \sum_{S\in\binom{[d]}{k}} \sum_{i=0}^k \sum_{U,V\in \binom{[k]}{i}} (-1)^{\norm{U}_1 + \norm{V}_1} E_A\left[ [A]_{U(S),V(S)} \right] \delta_{\overline{U}(S),\overline{V}(S)} \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}},\\ 
        \intertext{using that $U(S)=V(S)$ if and only if $\overline{U}(S) = \overline{V}(S)$,}
        &= \sum_{i=0}^k \frac{E_{B}\left[ \sigma_{k-i}(B)\right]}{\binom{d}{k-i}} \sum_{S\in\binom{[d]}{k}} \sum_{U,V\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right].
    \end{align*}

    To finish the proof we need to find

    \begin{equation} \label{eq:suma_rara} \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right]. \end{equation}

    Clearly, we are summing over all of the sets $V \in \binom{[d]}{i}$, but they appear more than once in the sum. To find the number of times every element $V\in \binom{[d]}{i}$ appears in the sum, we can count the total number of terms we are summing in \eqref{eq:suma_rara} and divide by the total number of elements in $\binom{[d]}{i}$. We have that $\abs{\binom{[d]}i} = \binom{d}{i}$ and the number of summands is $\binom{d}{k}\binom{k}{i}$, so

    \begin{align*}
        \frac{\binom{d}{k}\binom{k}{i}}{\binom{d}{i}} &= \frac{\frac{d!}{k!(d-k)!}\frac{k!}{i!(k-i)!}}{\frac{d!}{i!(d-i)!}} = \frac{(d-i)!}{(d-k)!(k-i)!} = \binom{d-i}{k-i}.
    \end{align*}

    So, we have

    \begin{equation*}
        \sum_{S\in\binom{[d]}{k}} \sum_{U\in \binom{[k]}{i}}  E_A\left[ [A]_{U(S),U(S)} \right] = \binom{d-i}{k-i} \sum_{V\in \binom{[d]}{i}}  E_A\left[ [A]_{V,V} \right] = \binom{d-i}{k-i}E_A\left[\sigma_{i}(A)\right].
    \end{equation*}

    Thus we can conclude

    \begin{equation*}
        E_{A,B,R}\left[ \sigma_k(A + RB\hermit R) \right] =  \sum_{i=0}^k \frac{\binom{d-i}{k-i}}{\binom{d}{k-i}}E_{A} \left[\sigma_i(A)\right] E_{A}\left[\sigma_{k-i}(B)\right].
    \end{equation*}

    \todo{Hay que aclarar qué signofica $U(S)$.}
\end{proof}


\begin{theorem} \label{thm:symmad}
    If $p(x)$ is the characteristic polynomial of $A$ and $q(x)$ is the characteristic polynomial of $B$, where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxplus_d q(x) = E_Q\left[\chi_x(A + Q B Q^*)\right],
    \end{equation*}

    \noindent where $\chi_x(\cdot)$ denotes the characteristic polynomial of $\cdot$ with $x$ as a variable and $E_Q$ denotes taking expectation over $Q$ where $Q$ is sampled from the Haar measure on the unitary complex $d\times d$ matrices.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symmad} and definition of the symmetric additive convolution.
\end{proof}



\begin{theorem} \label{thm:implies_symm_mult}
    Let $A$ and $B$ be $d\times d$ random matrices and $R$ a minor-orthogonal $d\times d$ matrix, such that $A,B,R$ are jointly independent, then

    \begin{equation*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] = \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{equation*}
\end{theorem}

\begin{proof}
    \begin{align*}
        E_{A,B,R} \left[\sigma_k(ARB\hermit{R})\right] &= \sum_{S \in \binom{[d]}{k}} E_{A,B,R} \left[[ARB\hermit{R}]_{S,S}\right],\\ 
        \intertext{By the Cauchy-Binet formula and independence}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[ [A]_{S,T} \right] E_{B,R} \left[ [RB\hermit{R}]_{T,S} \right],\\ 
        \intertext{By Lemma \ref{lemma:conjugate_minorth}}
        &= \sum_{S,T \in \binom{[d]}{k}} E_A\left[[A]_{S,T} \right] \delta_{T,S}\frac{E_B\left[ \sigma_k(B)\right]}{\binom{d}{k}},\\
        &= \frac{E_A[\sigma_k(A)]E_B[\sigma_k(B)]}{\binom{d}{k}}.
    \end{align*}
\end{proof}


\begin{theorem}
    Let $p(x)$ be the characteristic polynomial of $A$ and $q(x)$ be the characteristic polynomial of $B$ where $A$ and $B$ are $d\times d$ normal matrices with complex entries, then 

    \begin{equation*}
        p(x) \boxtimes_d q(x) = E_Q \left[ \chi_x (AQBQ^*) \right],
    \end{equation*}

    \noindent with $\chi_x$ and $E_Q$ as in Theorem \ref{thm:symmad}.
\end{theorem}

\begin{proof}
    It follows directly from Theorem \ref{thm:implies_symm_mult} and definition of the symmetric multiplicative convolution.
\end{proof}

\section{The $\mathcal R_n$ finite transform} 

\todo{Introducir la notación de $p(x) \equiv q(x) \mod x^n$.}

Given any polynomial $p(z)$ with order $p$ we can associate an empirical measure $\mu_p$ to its roots $z_i$ given by

\begin{equation*}
    \mu_p(\{x\}) = \frac1n\sum_{j=1}^p \delta_{x,z_j}.
\end{equation*}

This measure is similar to the spectral empirical measure of a random matrix. We can find its Cauchy transform in terms of the polynomial with the following lemma.

\begin{lemma} \label{lemma:cauchy_empirical_polynomial}
    Let $p$ be a monic polynomial of order $p$ with roots $\{z_i\}_{i=1}^n$, then the Cauchy transform of the empirical measure associated to the roots $z_i$ is given by 

    \begin{equation*}
        G_{\mu_p}(z) \coloneqq \frac1n \sum_{j=1}^n \frac1{z - z_j} = \frac{\partial_z p }{n p}(z) = \frac1n \partial_z \ln p(z).
    \end{equation*}

\end{lemma}

\begin{proof}
    $p(z)$ is a monic polynomial with roots $\left\{ z_j \right\}_{j \in [n]}$, then we can write,

    \begin{equation*}
        p(z) = \prod_{j=1}^n (z-z_j).
    \end{equation*}

    By the Leibnitz rule we find

    \begin{equation*}
        \partial_z p(z) = \sum_{j=1}^n \prod_{k\neq j} (z-z_k).
    \end{equation*}

    Using the last equation we have

    \begin{align*}
        \frac{\partial_z p}{n p}(z) &= \frac1n\sum_{j=1}^n \frac{\prod_{k\neq j} (z-z_k) }{ \prod_{l=1}^n (z-z_l) } = \frac1n\sum_{j=1}^n \frac{1}{z - z_j} \eqqcolon G_{\mu_p}(z).
    \end{align*}
\end{proof}

\begin{definition}[Legendre's transform]
    Let $f$ a convex function in a domain $D\subset \R$ and define

    \begin{equation} \label{eq:legendre_set}
        D^* \coloneqq \left\{ x^* \in \R \mathrel{:} \sup_{x \in D} \{xx^* - f(x)\} < \infty \right\}.
    \end{equation}

    We define $f^*$ the Legendre transform of $f$ as the function

    \begin{align*}
        f^* : D^* &\to \R.\\
        s \mapsto &\sup_{x \in D} \{ xs - f(x) \}.
    \end{align*}
\end{definition}

\begin{lemma} \label{lemma:strictly_convex}
    Let $f$ be a strictly convex function in a domain $D \subset \R$ and such that its derivative exists in a point $x_0 \in D$. Then $\left.\partial_x [f(x)]\right|_{x=x_0} \in D^*$ and 

    \begin{equation*}
        f^*(f'(x_0)) = x_0 f'(x_0) - f(x_0).
    \end{equation*}

    If additionally, $f$ has a second derivative, then the following two results are satisfied

    \begin{align*}
        (f')^{-1}(x_0) &= (f^*)'(x_0), \\
        f''((f^*)'(x_0)) &= \frac{1}{(f^*)''(x_0)}.
    \end{align*}
\end{lemma}


\begin{proof}
    Since $f$ is strictly convex and differentiable at $x_0$ we have for $x\in D, x\neq x_0$ that 
    
    \begin{align*}
        f(x) &> f(x_0) + (x-x_0)f'(x_0), \\
        \Rightarrow f(x) -xf'(x_0) &> f(x_0) - x_0 f'(x_0),\\
        \Rightarrow xf'(x_0) - f(x) &< x_0 f'(x_0) - f(x_0).
    \end{align*}

    This means that 

    \begin{equation*}
        \sup_{x\in D}\{ xf'(x_0) - f(x) \} = x_0f'(x_0) - f(x_0) < \infty.
    \end{equation*}

    Then, by the definition of $D^*$, we have that $f'(x_0) \in D^*$ and the Legendre transform in this point is $f^*(f'(x_0)) = x_0f'(x_0) - f(x_0)$.

    For the second part, if $f$ has a second derivative at $x_0$, differentiate the last equation to find

    \begin{equation*}
        (f^*)'(f'(x_0))f''(x_0) = x_0 f''(x_0) + f'(x_0) - f'(x_0) = x_0 f''(x_0). 
    \end{equation*}

    The second derivative $f''(x_0)$ can not be zero because $f$ is strictly convex, so $(f^*)'(f'(x_0)) = z$, which means $f^*$ and $f'$ are inverse under composition (in any point where $f$ is twice differentiable). For the second equation we use the fact that these functions are inverse and derive,

    \begin{align*}
        f'((f^*)'(z)) &= z,\\
        \Rightarrow f''((f^*)(x_0))(f^*)''(x_0) &= 1,\\
        \Rightarrow f''((f^*)(x_0)) &= \frac1{(f^*)''(x_0)}.
    \end{align*}
\end{proof}

\begin{lemma} \label{lemma:legendre_transform_norm}
    Let $D \subset \R$ and $\mu$ a measure that is absolutely continuous with respect to the Lebesgue measure, then for any continuous function $f: D \to \R$ we have that

    \begin{equation*}
        f^*(s) = \ln \norm{ e^{xs - f(x)} }_{\infty},
    \end{equation*}

    \noindent for all $s \in D^*$ where the Legendre transform and the norm are taken over $D$.
\end{lemma}

\begin{proof}
    We can write $f^*$ as $f^*(s) = \ln(\exp(f^*(s)))$, then

    \begin{align*}
        f^*(s) &= \ln(\exp(f^*(s))) = \ln \left(\exp\left\{ \sup_{x\in D} \{xs-f(x)\} \right\}\right), \\
        \intertext{using that $\exp(x)$ is monotone increasing,}
        &= \ln \left( \sup_{x \in D} \exp(xs - f(x))\right) = \ln \norm{ e^{xs-f(x)} }_{\infty}.
    \end{align*}
\end{proof}

\begin{definition}[The $\mathcal K_n$ transform \cite{anaya2016cumulantes}]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal K_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal K_n^{\mu_A} (s) \coloneqq - \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+ [x I - A]^{\frac1n} }_n,
    \end{equation*}

    \noindent where $\Delta^+$ represents the normalized determinant
    
    \begin{equation*}
        \Delta^+[A] = \left\{ \begin{array}{cc}
            \det[A]^{\frac1n} & \text{if $A$ is positive definite,}\\
            0 & \text{otherwise.}
        \end{array} \right.
    \end{equation*}
    
    \noindent and the integration domain for the norm is $(\rho_A, \infty)$ with $\rho_A$ the spectral radius of $A$.
\end{definition}


\begin{definition}[The $\mathcal R_n$ transform]
    Let $A$ be an $n\times n$ symmetric matrix with real entries. We define the $\mathcal R_n$ transform of its empirical spectral measure $\mu_A$ as 

    \begin{equation*}
        \mathcal R_n^{\mu_A} (s) = \mathcal K_n^{\mu_A} - \left( 1 + \frac1n \right) \frac1s.
    \end{equation*}
\end{definition}

\begin{theorem} Let $A$ be a self-adjoint $n \times n$ matrix with empirical spectral distribution $\mu_A$, then

    \begin{equation*}
        \lim_{n\to\infty} \mathcal K_n^{\mu_A} (s) = G_{\mu_A}^{-1}(s),
    \end{equation*}
\noindent with $s \in (\rho_A, \infty)$ and where $G_{\mu_A}^{-1}(s)$ is the inverse under composition of $G_{\mu_A}(s)$.
\end{theorem}

\begin{proof}
    
    We begin defining the function $g(x) \coloneqq -\ln \Delta^+[xI - A]$ and let $\lambda_1, \dots, \lambda_n$ be the ordered eigenvalues of $A$. When we differentiate $g$ with respect to $x$, we find 

    \begin{align*}
        \partial_x g(x) &= \partial_x \left[-\ln \Delta^+[xI - A]\right] = -\partial_x \ln\left[ \left(\prod_{j=1}^n (x - \lambda_j) \right)^{\frac1n}\right], \\
        &= -\partial_x \ln \left[ \prod_{j=1}^n (x-\lambda_j)^{\frac1n} \right] = - \partial_x \sum_{j=1}^n \frac1n \ln (x-\lambda_j) = -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} = - G_{\mu_A}(x).
    \end{align*}

    For the second derivative we have

    \begin{align*}
        \partial_{xx} g(x) = \partial_x \left[ -\frac1n \sum_{j=1}^n  \frac1{x-\lambda_j} \right] = \frac1n \sum_{j=1}^n \frac{1}{(x-\lambda_j)^2} > 0.
    \end{align*}

    So $g(x)$ is strictly convex and has a second derivative. Using Lemma \ref{lemma:strictly_convex} we find that 

    \begin{equation} \label{eq:inverse_cauchy}
        (g^*)'(x) = (g')^{-1}(x) = (-G_{\mu_A})^{-1}(x) = G_{\mu_A}^{-1}(-x).
    \end{equation}

    Now we find $(g^*)'(x)$ with the aide of Lemma \ref{lemma:legendre_transform_norm}.

    \begin{align*}
        (g^*)'(s) &= \frac{\partial}{\partial s} \ln \norm{ e^{xs} - g(x) }_\infty = \frac{\partial}{\partial s} \ln \norm{ e^{xs} \Delta^+(xI - A) }_\infty.
    \end{align*}

    Substituting the last in \eqref{eq:inverse_cauchy} we get 

    \begin{equation*}
        G_{\mu_A}^{-1}(s) = (g^*)'(-s) = -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    On the other hand, the limit of $\mathcal K_n^{\mu_A}$ is 

    \begin{equation*}
        \lim_{n\to\infty}  \mathcal K_n^{\mu_A}(s) = - \lim_{n\to\infty} \frac{\partial}{\partial s} \ln \norm{ e^{xs}\Delta^+(xI - A) }_n =  -\frac{\partial}{\partial s} \ln \norm{ e^{-xs} \Delta^+(xI - A) }_\infty.
    \end{equation*}

    This gives the desired result.
\end{proof}


We state without proof the following result. The proof  can be found in \cite{anaya2016cumulantes}.

\begin{lemma}
    Let $S$ be a multiset of complex numbers and denote by $\abs S=n$ its number of elements with multiplicity. Then exists an unique multiset of complex numbers $T$ such that $\abs{T}=n$ and 

    \begin{equation*}
        \prod_{s_i \in S} (x-s_i) = \frac 1n \sum_{t_i \in T} (x-t_i)^n.
    \end{equation*}

    We call such multiset the $U$ transform of $S$.
\end{lemma}

% \begin{proof}
%     Expanding the term $(x-t_i)^n$ we get,

%     \begin{equation*}
%         \frac 1m \sum_{t_i \in T} (x-t_i)^n = \frac 1m \sum_{t_i \in T} \sum_{k=0}^n \binom nk x^{n-k} (-1)^k t_i^k = \sum_{k=0}^n \binom nk x^{n-k} (-1)^k \E{T^k}. 
%     \end{equation*}

%     Where we use $\E{f(T)}$ to denote the expectation with respect to the empirical distribution associated to the multiset $T$, $\E{f(T)} = \frac1{\abs T} \sum_{t_i \in T} f(t_i)$.
% \end{proof}

\begin{lemma} \ref{lemma:Utrans_convolution}
    Let $p,q$ be polynomials with degree $n$ and $U_p,U_q$ be the $U$ transforms of its sets of roots, then

    \begin{equation*}
        [p \boxplus_n q](x) = \frac1n \sum_{u_j \in U_p, u_k \in U_q} (x - u_j -u_k)^n.
    \end{equation*}
\end{lemma}

\begin{proof}
    Let $p(x) = \sum_{j=0}^n x^{n-j}(-1)^j p_j$. Let $x_i$ be its roots, by definition of the $U$ transform we have,

    \begin{equation*}
        p(x) = \prod_{j=0}^n (x - x_j) = \frac1n \sum_{u_j \in U_p} (x-u_j)^n = \sum_{j=0}^n x^{n-j} (-1)^j \binom n j \E{U_p^j}.
    \end{equation*}

    Equating coefficients we find that $p_j = \binom nj\E{U_p^j}$ and the analogous happens for $q_k$. Finally, we find the convolution

    \begin{align*}
        [p\boxplus_n q](x) &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{(n-i)!(n-j)!}{(n-k)!n!} \binom{n}{i}\binom{n}{j} \E{U_p^i} \E{U_q^j},\\
        &= \sum_{k=0}^n x^{n-k}(-1)^k \sum_{i+j=k} \frac{n!}{i!j!(n-k)!} \E{U_p^i U_q^j}, \\
        &= \sum_{k=0}^n \binom nk x^{n-k}(-1)^k \sum_{j=0}^k \binom{k}{j}\E{U_p^{k-j} U_q^j}, \\
        &= \sum_{k=0}^n \binom nk x^{n-k}(-1)^k \E{(U_p + U_q)^k},\\
        &= \E{ (x - U_p - U_q)^n }. 
    \end{align*}
\end{proof}

\begin{lemma}  \label{lemma:quotient_norms}
    Let $A$ be an $n\times n$ real symmetric matrix. Denote by $\lambda(A)$ its spectrum and by $U_A$ the $U$ transform of $\lambda(A)$, then 
\todo{Encontrar $R_0^n$}
    \begin{equation*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \equiv \frac1n\sum_{u_i \in U_A} e^{-ns u_i} \qquad \mod [s^{n+1}]. 
    \end{equation*} 
\end{lemma}

\begin{proof}
    By definition of the $U$ transform, for any $n\times n$ symmetric matrix $A$ we have

    \begin{equation*}
        \Delta^+(xI - A)^n = \det[xI - A] = \prod_{j=1}^n (x - \lambda_j) = \frac1n \sum_{u_i \in U_A} (x-u_i)^n.
    \end{equation*}

    Using this and the previously found result for $\norm{e^{-xs}\Delta^+(xI)}_n^n$ we find

    \begin{align*}
        \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} &= \frac{(ns)^{n+1}}{n!}\norm{e^{-xs}\Delta^+(xI - A)}_n^n,\\
        &= \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \Delta^+(xI - A)^n \d x,\\
        &= \frac{(ns)^{n+1}}{n!}\int_{\rho_A}^\infty e^{-nxs} \frac1n \sum_{u_i \in U_A} (x-u_i)^n \d x,\\
        &= \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A}\int_{0}^\infty e^{-nsy} \frac1n \sum_{u_i \in U_A} (y + \rho_A-u_i)^n \d x, \\
        &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \mathcal L\{(y + \rho_A - U_A)^n\}(ns), \\
        &=  \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \binom nk  (\rho_A - u_i)^{n-k}\mathcal L\{y^k\}(ns), \\
        &= \frac1n \sum_{u_i\in U_A} \frac{(ns)^{n+1}}{n!}e^{-ns\rho_A} \sum_{k=0}^n \frac{n!}{k!(n-k)!}  (\rho_A - u_i)^{n-k}\frac{k!}{(ns)^{k+1}}, \\
        &= \frac1n \sum_{u_i \in U_A} e^{-ns\rho_A} \sum_{k=0}^n \frac{(ns)^{n-k}(\rho_A - u_i)^{n-k}}{(n-k)!}, \\
        &\equiv \frac1n \sum_{u_i\in U_A} e^{-ns\rho_A} e^{ns(\rho_A - u_i)}, \quad \mod [s^{n+1}],\\
        &\equiv \frac1n \sum_{u_i\in U_A} e^{-nsu_i}, \quad \mod [s^{n+1}].
    \end{align*}
\end{proof}

\begin{corollary} \label{corollary:R_n_as_a_logarithm}
    Let $A$ be a an $n\times n$ symmetric matrix with real entries, with spectrum $\lambda(A)$ and $U_A$ be the $U$ transform of $\lambda(A)$, then 

    \begin{equation*}
        \mathcal R_{n}^{\mu_A}(s) \equiv - \frac1n \frac{\partial}{\partial s} \ln \left(\frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right) \qquad \mod [s^n].
    \end{equation*}
\end{corollary}

\begin{proof}
    Taking logarithm in both sides of Lemma \ref{lemma:quotient_norms} we get 

    \begin{equation*}
        \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI - A)}_n^n}{\norm{e^{-xs}\Delta^+(xI)}_n^n} \right) \equiv \ln\left( \frac1n\sum_{u_i \in U_A} e^{-ns u_i} \right) \qquad \mod [s^{n+1}].
    \end{equation*}

    The first $n+1$ coeficientes of the power series coincide, so the first $k$ coefficients of the derivatives also coincide. Now we use the definition of the $\mathcal R_n$ transform

    \begin{align*}
        \mathcal R_{n}^{\mu_A}(s) &= \mathcal K_{n}^{\mu_A}(s) - \mathcal K_n^{\mu_0}(s), \\
        &= - \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI -A) }_n + \frac{\partial}{\partial s} \ln \norm{ e^{-xs}\Delta^+(xI) }_n,\\
        &= - \frac1n \frac{\partial}{\partial s} \ln \left( \frac{\norm{e^{-xs}\Delta^+(xI -A)}_n^n}{ \norm{e^{-xs}\Delta^+(xI)}_n^n } \right), \\
        &\equiv -\frac1n \frac{\partial}{\partial s} \ln \left( \frac1n \sum_{u_i \in U_A} e^{-nsu_i} \right), \qquad \mod[s^n].
    \end{align*}
\end{proof}

\begin{lemma}
    Let $A$ and $B$ be two $n\times n$ real symmetric matrices. The following are equivalents:

    \begin{enumerate}
        \item \[ \mathcal R_n^{\mu_A}(s) + \mathcal R_n^{\mu_B}(s) = \mathcal R_n^{\mu_{A+B}}(s) \qquad \mod[s^n]. \]
        \item \[ \det[xI - A] \boxplus_n \det[xI-B] = \det[xI - A - B]. \]
    \end{enumerate}
\end{lemma}

\begin{proof}
    Let us denote by $U_A, U_B$ and $U_{A+B}$ the $U$ transforms of the $\lambda(A), \lambda(B)$ and $\lambda(A+B)$, respectively. By Lemma \ref{lemma:Utrans_convolution}, the second statement is equivalent to 

    \begin{equation*}
        \E{ (x - U_A - U_B)^n } = \E{ (x - U_{A+B})^n }.
    \end{equation*}

    Expanding the power and equating terms, the last happens if and only if the first $m$ moments of $U_A + U_B$ and $U_{A+B}$ coincide. This is in turn equivalent to 

    \begin{equation*}
        \E{e^{-ms(U_A + U_B)}} = \E{e^{-ms(U_A + U_B)}} = \E{ e^{-msU_{A+B}} } \qquad \mod [s^{n+1}].
    \end{equation*}

    Define the function $f_A(s) \coloneqq - \frac1n \ln \E{ e^{-msU_A} }$ and similarly for $B$ and $A+B$. Using this function, the second statement is equivalent to 

    \begin{equation} \label{eq:f_matrix_module}
        f_A(s) + f_B(s) \equiv f_{A+B}(s) \qquad \mod [s^{n+1}].
    \end{equation}

    Using Corollary \ref{corollary:R_n_as_a_logarithm}, the first statement is equivalent to 

    \begin{equation} \label{eq:differential_matrix_module}
        \frac{\partial}{\partial_s} f_A(s) + \frac{\partial}{\partial_s} f_B(s) \equiv \frac{\partial}{\partial_s} f_{A+B}(s) \qquad \mod[s^n].
    \end{equation}

    So proving the equivalence of the two statements reduces to prove the equivalence between \eqref{eq:f_matrix_module} and \eqref{eq:differential_matrix_module}. Because the formal series in \eqref{eq:f_matrix_module} coincide up to the $n+1$th term, the derivatives coincide up to the $n$th term, so one implication is trivial. For the second implication, it suffices to prove that $f_A(0) + f_B(0) = f_{A+B}(0)$, but we have that $f_A(0)=f_B(0)=f_{A+B}(0)$, so the result follows.
\end{proof} \todo{Mencionar caso multiplicativo.}








% \begin{theorem}[Finite central Limit Theorem \cite{marcus2021polynomial}]
%     Let $p_1, p_2, \dots$ be a sequence of degree $d$ real rooted polynomials with $p_i = \prod_j (x - r_{i,j})$ such that

%     \begin{equation*}
%         \sum_{j} r_{i,j} = 0, \qquad \frac1d \sum_j r^2_{i,j} = \sigma^2,
%     \end{equation*}

%     \noindent for all $i$. Define $q_i(x) = n^{-m/2}p_i(\sqrt{n}x)$, then 

%     \begin{equation*}
%         \lim_{n\to\infty} \left( q_1 \boxplus_d \cdots \boxplus_d q_n \right) = \left( \frac{d-1}{\sigma^2} \right)^{-d/2} H_d\left( x \sqrt{\frac{d-1}{\sigma^2}} \right),
%     \end{equation*}

%     \noindent with $H_d$ represents the $d$th Hermite polynomial and the constants work to make it monic.
% \end{theorem}
